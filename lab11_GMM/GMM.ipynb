{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af30a4f",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00dd7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from logpdf_loglikelihood_GAU import logpdf_GAU_ND #for single density of a gaussian component\n",
    "from scipy.special import logsumexp #for marginalizing the joints to retrieve the GMM log density\n",
    "from mean_covariance import vrow, vcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef4e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logpdf_GMM(X, gmm):\n",
    "    \"\"\"\n",
    "    Compute log density of data points X under a Gaussian Mixture Model (GMM).\n",
    "    Parameters\n",
    "    -X: matrix of size (D, N) where D is the number of features and N is the number of data points.\n",
    "    -gmm: list of gaussian components. Each one is a tuple of (weight, mean, covariance).\n",
    "           weight: scalar\n",
    "           mean: vector of size (D,)\n",
    "           covariance: matrix of size (D, D)\n",
    "        Example: gmm = [(w1, mu1, C1), (w2, mu2, C2), ...]\n",
    "    Returns\n",
    "    -logpdf: vector of size (N,) containing the log density of each data point under the GMM.\n",
    "    \"\"\"\n",
    "\n",
    "    #1. create matrix S of shape (K, N), where N = number of samples and K = number of components\n",
    "    K = len(gmm)  # number of components\n",
    "    N = X.shape[1]\n",
    "    S = np.zeros((K, N))\n",
    "\n",
    "    #iterate over components, for each componente take mean, covariance and compute log density of the Gaussian\n",
    "    for k in range(K):\n",
    "        weight, mean, covariance = gmm[k]\n",
    "        S[k, :] = logpdf_GAU_ND(X, mean, covariance) + np.log(weight)\n",
    "    \n",
    "    #these are the log joints, then GMM can be casted as a latent variable model, i.e. we can find the GMM log density marginalizing the joints over the latent variable\n",
    "    #latent variable if the component/cluster\n",
    "    logdens = logsumexp(S, axis=0)  #marginalize joints over the components\n",
    "\n",
    "    return logdens  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99100d17",
   "metadata": {},
   "source": [
    "we now test the function to compute the GMM log density over a reference dataset using reference GMM parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a992046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GMM_load import load_gmm, save_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9422b04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of example_X: (4, 1000)\n",
      "GMM log likelihood shape: (1000,)\n",
      "The computed log likelihoods match the solution.\n"
     ]
    }
   ],
   "source": [
    "#load first GMM\n",
    "gmm = load_gmm('./GMM_models/GMM_4D_3G_init.json')\n",
    "example_X = np.load('./Data/GMM_data_4D.npy')\n",
    "\n",
    "print(f'Shape of example_X: {example_X.shape}')\n",
    "\n",
    "GMM_ll = logpdf_GMM(example_X, gmm)\n",
    "print(f'GMM log likelihood shape: {GMM_ll.shape}')\n",
    "\n",
    "#compare\n",
    "GMM_ll_solution = np.load('./GMM_models/ll/GMM_4D_3G_init_ll.npy')\n",
    "\n",
    "#check if the log likelihoods are equal\n",
    "if np.allclose(GMM_ll, GMM_ll_solution):\n",
    "    print(\"The computed log likelihoods match the solution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a9a3251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of example_X_1D: (1, 4000)\n",
      "GMM log likelihood shape for 1D data: (4000,)\n",
      "The computed log likelihoods for 1D data match the solution.\n"
     ]
    }
   ],
   "source": [
    "#chek with 1-D Data\n",
    "gmm_1D = load_gmm('./GMM_models/GMM_1D_3G_init.json')\n",
    "example_X_1D = np.load('./Data/GMM_data_1D.npy')\n",
    "print(f'Shape of example_X_1D: {example_X_1D.shape}')\n",
    "GMM_ll_1D = logpdf_GMM(example_X_1D, gmm_1D)\n",
    "print(f'GMM log likelihood shape for 1D data: {GMM_ll_1D.shape}')\n",
    "\n",
    "#compare with 1D solution\n",
    "GMM_ll_1D_solution = np.load('./GMM_models/ll/GMM_1D_3G_init_ll.npy')\n",
    "#check if the log likelihoods are equal\n",
    "if np.allclose(GMM_ll_1D, GMM_ll_1D_solution):\n",
    "    print(\"The computed log likelihoods for 1D data match the solution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d950fc6",
   "metadata": {},
   "source": [
    "# GMM Estimation: E-M Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52950521",
   "metadata": {},
   "source": [
    "1) **E-step**: here we have to minimize the KL divergence, so the ELBO gets higher and equals to objective function of GMM. In practice, in order to make the KL divergence become equal to zero, its minimum, we are required to compute the responsibilities, so the cluster posteriors. So, we compute the **auxiliary function**:\n",
    "    $$\n",
    "    \\text{AUX}(\\theta, \\theta_t) = \\mathbb{E}_{C_1, ..., C_M \\mid \\mathbf{X}_1 = \\mathbf{x}_1, ...., \\mathbf{X}_N = \\mathbf{x}_N, \\theta_t} \\left[ \\log f_{\\mathbf{X}_1, ..., \\mathbf{X}_N, C_1, ..., C_N}(\\mathbf{x}_1, ..., \\mathbf{x}_N, c_1, ..., c_N \\mid \\theta_t)\\right]\n",
    "    $$\n",
    "    Exploiting the *i.i.d* assumption for our samples, and the linearity property of the expectation operator (i.e. an expectation over a sum is equal to the sum of all the single expectations):\n",
    "    $$\n",
    "    \\text{AUX}(\\theta, \\theta_t) = \\sum_{i = 1}^N \\mathbb{E}_{C_1, ..., C_M \\mid \\mathbf{X}_1 = \\mathbf{x}_1, ...., \\mathbf{X}_N = \\mathbf{x}_N, \\theta_t} \\left[ \\log f_{\\mathbf{X}_i, C_i}(\\mathbf{x}_i, c_i \\mid \\theta_t)\\right]\n",
    "    $$\n",
    "    Then we can observe that, since in the expectations we just have the log-joint for the i-th samples, these expectations just depends on the posteriors of the i-th samples:\n",
    "    $$\n",
    "    \\text{AUX}(\\theta, \\theta_t) = \\sum_{i = 1}^N \\mathbb{E}_{C_i \\mid \\mathbf{X}_i = \\mathbf{x}_i, \\theta_t} \\left[ \\log f_{\\mathbf{X}_i, C_i}(\\mathbf{x}_i, c_i \\mid \\theta_t)\\right]\n",
    "    $$\n",
    "    Then, computing the responsibilities, we can write the values of these expectations as:\n",
    "    $$\n",
    "    \\text{AUX}(\\theta, \\theta_t) = \\sum_{i = 1}^N \\sum_{c = 1}^{K} \\gamma_{c, i} \\left[ \\log \\mathcal{N}(\\mathbf{x}_i \\mid \\mu_c, \\mathbf{\\Sigma}_c) + \\log w_c\\right]\n",
    "    $$\n",
    "    So, $\\text{AUX}(\\theta, \\theta_t)$ depends on the cluster posteriors, which we compute in this step like this:\n",
    "    $$\n",
    "    \\gamma_{c, i} = f_{C \\mid \\mathbf{X}}(c \\mid \\mathbf{x}_i, \\mathbf{\\theta}_t) = \\frac{\\mathcal{N}(\\mathbf{x}_i \\mid \\mu_c, \\mathbf{\\Sigma}_c) wc}{\\sum_{c' = 1}^K \\mathcal{N}(\\mathbf{x}_i \\mid \\mu_c', \\mathbf{\\Sigma}_c') wc'}\n",
    "    $$\n",
    "\n",
    "2) **M-step**: in this step we use the cluster labels inferred in the first step, to estimate a new set of model parameters. In practice, we optimize the auxiliary function $\\text{AUX}(\\theta, \\theta_t)$ wrt $\\theta = \\left[ \\mathbf{M}, \\mathbf{S}, \\mathbf{w}\\right]$ and $\\sum_{i = 1}^K w_c = 1$, following a ML approach, and we find:\n",
    "    $$\n",
    "    \\mu_c^{t+1} = \\frac{\\sum_{i = 1}^N \\gamma_{c, i} \\mathbf{x}_i}{\\sum_{i = 1}^N \\gamma_{c, i}}\n",
    "    $$\n",
    "    $$\n",
    "    \\mathbf{\\Sigma}_c^{t+1} = \\frac{\\sum_{i = 1}^N \\gamma_{c, i} \\left( \\mathbf{x}_i - \\mu_c\\right) \\left( \\mathbf{x}_i - \\mu_c\\right)^T}{\\sum_{i = 1}^N \\gamma_{c, i}}\n",
    "    $$\n",
    "    $$\n",
    "    w_c^{t+1} = \\frac{\\sum_{i = 1}^N \\gamma_{c, i}}{\\sum_{c' = 1}^K \\sum_{i = 1}^N \\gamma_{c', i}}\n",
    "    $$\n",
    "    Here we can call $N_c = \\sum_{i = 1}^N \\gamma_{c, i}$ and $N = \\sum_{c' = 1}^K \\sum_{i = 1}^N \\gamma_{c', i}$. Note that when camputing these parameters for cluser $c$ we **always sum all the fractions of the points in the dataset**, thus respecting the **soft margin assignments** approach. <br>\n",
    "    When programming the algorithm, we can build and reuse for all the three estimates these zero, first and second order statistics:\n",
    "    $$\n",
    "    Z_c = \\sum_{i = 1}^N \\gamma_{c, i} \\qquad\n",
    "    \n",
    "    \\mathbf{F}_c = \\sum_{i = 1}^N \\gamma_{c, i} \\mathbf{x}_i \\qquad\n",
    "    \n",
    "    \\mathbf{S}_c = \\sum_{i = 1}^N \\gamma_{c, i} \\mathbf{x}_i \\mathbf{x}_i^T\n",
    "    $$\n",
    "    and rewrite the new three estimates for cluster $c$ this way:\n",
    "    $$\n",
    "    \\mu_c^{t+1} = \\frac{\\mathbf{F}_c}{Z_c}\n",
    "    $$\n",
    "    $$\n",
    "    \\mathbf{\\Sigma}_c^{t+1} = \\frac{\\mathbf{S}_c}{Z_c} - \\mu_c^{(t+1)} {\\mu_c^{(t+1)}}^T\n",
    "    $$\n",
    "    $$\n",
    "    w_c^{t+1} = \\frac{Z_c}{N}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4c7a0",
   "metadata": {},
   "source": [
    "E-M in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "311ec08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_EM_iteration(X, gmm_start):\n",
    "   \"\"\"\n",
    "      One single iteration of the EM algorithm for Gaussian Mixture Models (GMM).\n",
    "      Parameters\n",
    "      -X: matrix of size (D, N) where D is the number of features and N is the number of data points.\n",
    "      -gmm_start: list of starter GMM components. Can be obtained with either K-Means or LGB Algorithm.\n",
    "               Each one is a tuple of (weight, mean, covariance).\n",
    "               weight: scalar\n",
    "               mean: vector of size (D,)\n",
    "               covariance: matrix of size (D, D)\n",
    "      -threshold_stop: threshold for stopping the EM algorithm. If the change in log likelihood is less than this value, stop.\n",
    "      Returns\n",
    "      -gmm: list of gaussian components. Each one is a tuple of (weight, mean, covariance).\n",
    "            weight: scalar\n",
    "            mean: vector of size (D,)\n",
    "            covariance: matrix of size (D, D)\n",
    "    \"\"\"\n",
    "    \n",
    "   #1. E-STEP: compute responsibilities\n",
    "   #create matrix S of shape (K, N), where N = number of samples and K = number of components\n",
    "   K = len(gmm_start)  # number of components\n",
    "   N = X.shape[1]\n",
    "   S = np.zeros((K, N))\n",
    "\n",
    "   #iterate over components, for each componente take mean, covariance and compute log density of the Gaussian\n",
    "   for k in range(K):\n",
    "      weight, mean, covariance = gmm_start[k]\n",
    "      S[k, :] = logpdf_GAU_ND(X, mean, covariance) + np.log(weight)\n",
    "\n",
    "\n",
    "   #for each sample, marginalize the log-joints over the components to get the log-marginal\n",
    "   logdens = logsumexp(S, axis=0)  #vector of size (N,)\n",
    "\n",
    "   #compute log-posteriors by removing log-marginal from the log-joints\n",
    "   log_posteriors = S - logdens   #(K, N) - (N,) -> broadcasting -> (K, N) - (N,N) = (K, N) thanks to broadcasting\n",
    "\n",
    "   #compute responsibilities, so cluster posteriors, by exponentiating the log-posteriors\n",
    "   responsibilities = np.exp(log_posteriors) #(K, N)\n",
    "\n",
    "\n",
    "   #2. M-STEP: estimate new GMM parameters\n",
    "   gmm = []\n",
    "   #compute zero, first, and second order statistics from the responsibilities of each cluster\n",
    "   #for each cluster k, do:\n",
    "   for k in range(K):\n",
    "      gamma = responsibilities[k, :]\n",
    "      Z_gamma = np.sum(gamma) #zero order\n",
    "      F_gamma = vcol(np.sum(vrow(gamma) * X, axis = 1)) #first order\n",
    "      S_gamma = (vrow(gamma) * X) @ X.T #second order, (D, N) @ (N, D) = (D, D)\n",
    "\n",
    "      #ESTIMATE NEW PARAMS for the cluster k\n",
    "      mu_k_new = F_gamma / Z_gamma  #col vector (D, 1)\n",
    "      cov_k_new = S_gamma / Z_gamma - vcol(mu_k_new) @ vrow(mu_k_new)  #covariance matrix, (D, D)\n",
    "      weight_k_new = Z_gamma / N #n = sum of reponsibilities for each sample for each cluster k = total number of samples, since responsibilities of each sample sum to 1 being fractions\n",
    "      gmm.append((weight_k_new, mu_k_new, cov_k_new))  #append new params for the cluster k\n",
    "\n",
    "   return gmm  #return the new GMM parameters after one EM iteration\n",
    "\n",
    "\n",
    "\n",
    "def GMM_EM(X, gmm_start, threshold_stop=1e-6, max_iter=100):\n",
    "   \"\"\"\n",
    "   EM algorithm for Gaussian Mixture Models (GMM). \n",
    "   Parameters\n",
    "   -X: matrix of size (D, N) where D is the number of features and N is the number of data points.\n",
    "   -gmm_start: list of starter GMM components. Can be obtained with either K-Means or LGB Algorithm.\n",
    "            Each one is a tuple of (weight, mean, covariance).\n",
    "            weight: scalar\n",
    "            mean: vector of size (D,)\n",
    "            covariance: matrix of size (D, D)\n",
    "   -threshold_stop: threshold for stopping the EM algorithm. If the change in log likelihood is less than this value, stop.\n",
    "   -max_iter: maximum number of iterations for the EM algorithm.\n",
    "   Returns\n",
    "   -gmm: list of gaussian components. Each one is a tuple of (weight, mean, covariance).\n",
    "            weight: scalar\n",
    "            mean: vector of size (D,)\n",
    "            covariance: matrix of size (D, D)\n",
    "   \"\"\"\n",
    "\n",
    "   gmm_old = gmm_start.copy()  \n",
    "   num_iters = 0\n",
    "   while True:\n",
    "      #compute the log likelihood with old GMM params\n",
    "      GMM_ll_old = logpdf_GMM(X, gmm_old).mean()  \n",
    "\n",
    "      #run 1 iter of EM\n",
    "      gmm_new = GMM_EM_iteration(X, gmm_old)\n",
    "\n",
    "      #compute new log likelihood\n",
    "      GMM_ll_new = logpdf_GMM(X, gmm_new).mean()\n",
    "\n",
    "      #for sure GMM_ll_new >= GMM_ll_old\n",
    "      #stop if GMM_ll_new - GMM_ll_old < threshold_stop\n",
    "      if GMM_ll_old > GMM_ll_new:\n",
    "         print(\"Warning: mean GMM log likelihood decreased. This is unexpected.\")\n",
    "         print(f\"GMM_ll_old (mean): {GMM_ll_old}, GMM_ll_new (mean): {GMM_ll_new}\")\n",
    "         \n",
    "      if GMM_ll_new - GMM_ll_old < threshold_stop:\n",
    "         break\n",
    "\n",
    "      num_iters += 1\n",
    "      if num_iters >= max_iter:\n",
    "         print(f\"Reached maximum number of iterations: {max_iter}. Stopping EM.\")\n",
    "         break\n",
    "\n",
    "      #update old GMM params\n",
    "      gmm_old = gmm_new.copy()\n",
    "\n",
    "\n",
    "   return gmm_new  #return the last GMM parameters after EM iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105f784",
   "metadata": {},
   "source": [
    "Let's apply the E-M Algorithm to the example data, using example initial params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76be7aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computed GMM parameters match the solution.\n"
     ]
    }
   ],
   "source": [
    "example_X = np.load('./Data/GMM_data_4D.npy')\n",
    "gmm_start = load_gmm('./GMM_models/GMM_4D_3G_init.json')\n",
    "threshold_stop = 1e-6\n",
    "gmm_final = GMM_EM(example_X, gmm_start, threshold_stop=threshold_stop)\n",
    "\n",
    "\n",
    "#compare with the solution\n",
    "gmm_EM_solution = load_gmm('./GMM_models/GMM_4D_3G_EM.json')\n",
    "\n",
    "#check if the GMM parameters are equal\n",
    "if all(np.isclose(gmm_final[k][0], gmm_EM_solution[k][0]) and np.allclose(gmm_final[k][1], gmm_EM_solution[k][1]) and np.allclose(gmm_final[k][2], gmm_EM_solution[k][2]) for k in range(len(gmm_final))):\n",
    "    print(\"The computed GMM parameters match the solution.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
