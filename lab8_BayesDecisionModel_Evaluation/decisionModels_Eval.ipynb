{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8efcb619",
   "metadata": {},
   "source": [
    "# Models Evaluation\n",
    "\n",
    "## Confusion Matrix\n",
    "It's a matrix thta summarizes all the possible outcomes of the classification. On the columns we have the actual (so, *real*) classes, whereas on the rows we have the *predicted* classes for the sample. <br>\n",
    "For example, let's consider a binary classification problem where we want to distinguish between two classes: *False* and *True**. The resulting confusion matrix might look like this:\n",
    "\n",
    "|  | Hf (Actual) | Ht (Actual) |\n",
    "|---|---|---|\n",
    "| **Hf (Predicted)** | 150 | 25 |\n",
    "| **Ht (Predicted)** | 10 | 215 |\n",
    "\n",
    "In this matrix:\n",
    "\n",
    "* **150** is the number of samples that were actually False and were correctly predicted as False (**TN** for the Hf class).\n",
    "* **25** is the number of samples that were actually True but were incorrectly predicted as False (**FN** for the Hf class).\n",
    "* **10** is the number of samples that were actually False but were incorrectly predicted as True (**FN** for the Hf class).\n",
    "* **215** is the number of samples that were actually True and were correctly predicted as True (**TP** for the Ht class).\n",
    "\n",
    "This confusion matrix provides a clear view of how many samples were classified correctly and what types of errors the model made. <br>\n",
    "\n",
    "Now, let's consider the **Iris** dataset, which has 3 classes. Let's import the *train/validation* split used before and fit the three Gaussian Generative Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82ccb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR shape: (4, 100)\n",
      "DVAL shape: (4, 50)\n",
      "LTR shape: (100,)\n",
      "LVAL shape: (50,)\n"
     ]
    }
   ],
   "source": [
    "#Import the train validation split from \"./split/iris_split.npz\"\n",
    "import numpy as np\n",
    "\n",
    "savedSplit = np.load('./split/iris_split.npz')\n",
    "\n",
    "DTR = savedSplit['DTR']\n",
    "DVAL = savedSplit['DVAL']\n",
    "LTR = savedSplit['LTR']\n",
    "LVAL = savedSplit['LVAL']\n",
    "\n",
    "print(f\"DTR shape: {DTR.shape}\")\n",
    "print(f\"DVAL shape: {DVAL.shape}\")\n",
    "print(f\"LTR shape: {LTR.shape}\")\n",
    "print(f\"LVAL shape: {LVAL.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67f3a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "MVG_path = './models_finished/MVG'\n",
    "MVGTC_path = './models_finished/MVG_TiedCov'\n",
    "MVGNB_path = './models_finished/Naive_Bayes'\n",
    "if not MVG_path in sys.path:\n",
    "    sys.path.append(MVG_path)\n",
    "if not MVGTC_path in sys.path:\n",
    "    sys.path.append(MVGTC_path)\n",
    "if not MVGNB_path in sys.path:\n",
    "    sys.path.append(MVGNB_path)\n",
    "\n",
    "import MVG\n",
    "import MVG_TiedCov as MVGTC\n",
    "import Naive_Bayes as MVGNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf285e",
   "metadata": {},
   "source": [
    "### Iris Dataset, MVG Classifier Confusion Matrix Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45e4dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MVG Pipeline\n",
    "\n",
    "def MVG_Pipeline(DTR, LTR, useLog=True):\n",
    "\n",
    "    ML_params_MVG = MVG.computeParams_ML(DTR, LTR)\n",
    "\n",
    "\n",
    "    S_LogLikelihoods_MVG = MVG.scoreMatrix_Pdf_GAU(DVAL, ML_params_MVG, useLog=useLog)\n",
    "    print(f\"S_LogLikelihoods_MVG shape, computed from the Validation Set: {S_LogLikelihoods_MVG.shape}\")\n",
    "\n",
    "    SJoint_MVG = MVG.computeSJoint(S_LogLikelihoods_MVG, np.ones((3, )) / 3., useLog=useLog) #compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    print(f\"Joint densities shape: {SJoint_MVG.shape}\")\n",
    "\n",
    "    SPost_MVG = MVG.computePosteriors(SJoint_MVG, useLog=useLog) #compute the posteriors by normalizing the joint densities\n",
    "    print(f\"Posteriors shape: {SPost_MVG.shape}\")\n",
    "\n",
    "    PVAL_MVG = np.argmax(SPost_MVG, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "    print(f\"Predictions shape: {PVAL_MVG.shape}\")\n",
    "    print(f\"Predictions: {PVAL_MVG}\")\n",
    "\n",
    "    return PVAL_MVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "61b73bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_LogLikelihoods_MVG shape, computed from the Validation Set: (3, 50)\n",
      "Joint densities shape: (3, 50)\n",
      "Posteriors shape: (3, 50)\n",
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 2 1 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "PVAL_MVG = MVG_Pipeline(DTR, LTR, useLog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b685f550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[19  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  2 14]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Compute confusion matrix\n",
    "#classes: 0, 1, 2\n",
    "\n",
    "Pred0_Actual0 = np.sum((PVAL_MVG == 0) & (LVAL == 0))    #True Positives for class 0\n",
    "Pred0_Actual1 = np.sum((PVAL_MVG == 0) & (LVAL == 1))    #False Positives for class 0 from class 1\n",
    "Pred0_Actual2 = np.sum((PVAL_MVG == 0) & (LVAL == 2))    #False Positives for class 0 from class 2\n",
    "\n",
    "Pred1_Actual0 = np.sum((PVAL_MVG == 1) & (LVAL == 0))    #False Positives for class 0 from class 1\n",
    "Pred1_Actual1 = np.sum((PVAL_MVG == 1) & (LVAL == 1))    #True Positives for class 1\n",
    "Pred1_Actual2 = np.sum((PVAL_MVG == 1) & (LVAL == 2))    #False Positives for class 1 from class 2\n",
    "\n",
    "Pred2_Actual0 = np.sum((PVAL_MVG == 2) & (LVAL == 0))    #False Positives for class 0 from class 2\n",
    "Pred2_Actual1 = np.sum((PVAL_MVG == 2) & (LVAL == 1))    #False Positives for class 1 from class 2\n",
    "Pred2_Actual2 = np.sum((PVAL_MVG == 2) & (LVAL == 2))    #True Positives for class 2\n",
    "\n",
    "#confMatrix is populated manually since I have compute all the values in the confusion matrix\n",
    "ConfMatrix_MVG_manual = np.array([[Pred0_Actual0, Pred0_Actual1, Pred0_Actual2],\n",
    "                       [Pred1_Actual0, Pred1_Actual1, Pred1_Actual2],\n",
    "                       [Pred2_Actual0, Pred2_Actual1, Pred2_Actual2]])\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{ConfMatrix_MVG_manual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "97e81342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeConfMatrix(PVAL, LVAL):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix for the predicted labels and the actual labels.\n",
    "    Args:\n",
    "    - PVAL: Predicted labels\n",
    "    - LVAL: Actual labels\n",
    "    Returns:\n",
    "    - Confusion matrix\n",
    "    \"\"\"\n",
    "    numClasses = np.unique(LVAL).shape[0] #number of classes\n",
    "    ConfMatrix = np.zeros((numClasses, numClasses)) #initialize the confusion matrix with zeros\n",
    "\n",
    "    for classPredicted in range(numClasses):\n",
    "        #for each class find the tre positives and ALL the false negatives\n",
    "\n",
    "        classRow = np.array([]) #initialize the classRow with an empty array\n",
    "\n",
    "        for classActual in range(numClasses):\n",
    "            if classActual == classPredicted: \n",
    "                TP = np.sum((PVAL == classPredicted) & (LVAL == classPredicted))\n",
    "                classRow = np.append(classRow, TP)\n",
    "                continue\n",
    "\n",
    "            #compute each FP for each wrongly assigned label\n",
    "            FPi = np.sum((PVAL == classPredicted) & (LVAL == classActual))\n",
    "\n",
    "            #add FPi to the classCol\n",
    "            classRow = np.append(classRow, FPi)\n",
    "\n",
    "        \n",
    "        #add classCol to the confusion matrix\n",
    "        ConfMatrix[classPredicted, :] = classRow\n",
    "\n",
    "\n",
    "    return ConfMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fbeba67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix, MVG Classifier:\n",
      "[[19.  0.  0.]\n",
      " [ 0. 15.  0.]\n",
      " [ 0.  2. 14.]]\n"
     ]
    }
   ],
   "source": [
    "confMatrix_MVG = computeConfMatrix(PVAL_MVG, LVAL)\n",
    "print(f\"Confusion Matrix, MVG Classifier:\\n{confMatrix_MVG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c38c22",
   "metadata": {},
   "source": [
    "### Iris Dataset, Tied Covariance MVG Classifier Confusion Matrix Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "29f464d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MVGTC Pipeline\n",
    "\n",
    "def MVTC_Pipeline(DTR, LTR, useLog=True):\n",
    "    ML_params_MVGTC = MVGTC.computeParams_ML_TiedCov(DTR, LTR, useLDAForTiedCov=True)\n",
    "\n",
    "    S_LogLikelihoods_MVGTC = MVGTC.scoreMatrix_Pdf_GAU(DVAL, ML_params_MVGTC, useLog=useLog)\n",
    "    print(f\"S_LogLikelihoods_MVGTC shape, computed from the Validation Set: {S_LogLikelihoods_MVGTC.shape}\")\n",
    "\n",
    "    SJoint_MVGTC = MVGTC.computeSJoint(S_LogLikelihoods_MVGTC, np.ones((3, )) / 3., useLog=useLog) #compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    print(f\"Joint densities shape: {SJoint_MVGTC.shape}\")\n",
    "\n",
    "    SPost_MVGTC = MVGTC.computePosteriors(SJoint_MVGTC, useLog=useLog) #compute the posteriors by normalizing the joint densities\n",
    "    print(f\"Posteriors shape: {SPost_MVGTC.shape}\")\n",
    "\n",
    "    PVAL_MVGTC = np.argmax(SPost_MVGTC, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "    print(f\"Predictions shape: {PVAL_MVGTC.shape}\")\n",
    "    print(f\"Predictions: {PVAL_MVGTC}\")\n",
    "\n",
    "    return PVAL_MVGTC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e0f859ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_LogLikelihoods_MVGTC shape, computed from the Validation Set: (3, 50)\n",
      "Joint densities shape: (3, 50)\n",
      "Posteriors shape: (3, 50)\n",
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 1 1 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n",
      "\n",
      "Confusion Matrix, Tied Cov MVG Classifier:\n",
      "[[19.  0.  0.]\n",
      " [ 0. 16.  0.]\n",
      " [ 0.  1. 14.]]\n"
     ]
    }
   ],
   "source": [
    "confMatrix_MVGTC = computeConfMatrix(MVTC_Pipeline(DTR, LTR), LVAL)\n",
    "print(f\"\\nConfusion Matrix, Tied Cov MVG Classifier:\\n{confMatrix_MVGTC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a6582",
   "metadata": {},
   "source": [
    "### Iris Dataset, Naive Bayes MVG Classifier Confusion Matrix Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dfdd9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Pipeline\n",
    "\n",
    "def MVGNB_Pipeline(DTR, LTR, useLog=True):\n",
    "\n",
    "    ML_params_MVGNB = MVGNB.computeParams_ML_NaiveBayesAssumption(DTR, LTR)\n",
    "\n",
    "    S_LogLikelihoods_MVGNB = MVGNB.scoreMatrix_Pdf_GAU(DVAL, ML_params_MVGNB, useLog=True)\n",
    "    print(f\"S_LogLikelihoods_MVGNB shape, computed from the Validation Set: {S_LogLikelihoods_MVGNB.shape}\")\n",
    "\n",
    "    SJoint_MVGNB = MVGNB.computeSJoint(S_LogLikelihoods_MVGNB, np.ones((3, )) / 3., useLog=True) #compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    print(f\"Joint densities shape: {SJoint_MVGNB.shape}\")\n",
    "\n",
    "    SPost_MVGNB = MVGNB.computePosteriors(SJoint_MVGNB, useLog=True) #compute the posteriors by normalizing the joint densities\n",
    "    print(f\"Posteriors shape: {SPost_MVGNB.shape}\")\n",
    "\n",
    "    PVAL_MVGNB = np.argmax(SPost_MVGNB, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "    print(f\"Predictions shape: {PVAL_MVGNB.shape}\")\n",
    "    print(f\"Predictions: {PVAL_MVGNB}\")\n",
    "\n",
    "    return PVAL_MVGNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b88baf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_LogLikelihoods_MVGNB shape, computed from the Validation Set: (3, 50)\n",
      "Joint densities shape: (3, 50)\n",
      "Posteriors shape: (3, 50)\n",
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 1 2 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n",
      "\n",
      "Confusion Matrix, Naive Bayes MVG Classifier:\n",
      "[[19.  0.  0.]\n",
      " [ 0. 15.  0.]\n",
      " [ 0.  2. 14.]]\n"
     ]
    }
   ],
   "source": [
    "confMatrix_MVGNB = computeConfMatrix(MVGNB_Pipeline(DTR, LTR), LVAL)\n",
    "print(f\"\\nConfusion Matrix, Naive Bayes MVG Classifier:\\n{confMatrix_MVGNB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319bad2",
   "metadata": {},
   "source": [
    "Given the limited number of errors, a detailed analysis of the IRIS dataset is not very interesting. We\n",
    "thus turn our attention to a larger evaluation dataset. <br>\n",
    "We can use the dataset used in Lab7, storing the tercets samples of the *Divina Commedia*. Each tercet is associated to a label that denotes the cantica from where tercet is extracted ($0$: *Inferno*, $1$ = *Purgatorio*, $2$ = *Paradiso*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b3dd6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "commedia_ll = np.load(\"./data/commedia_ll.npy\")\n",
    "commedia_labels = np.load(\"./data/commedia_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3747bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commedia_ll shape: (3, 1204)\n",
      "commedia_labels shape: (1204,)\n",
      "First 10 logLikelihoods of Inferno: [-122.72443339 -133.30648701 -134.36987251 -170.65723182 -163.97348133\n",
      " -139.39515141 -166.71004347 -174.57737603 -147.62396153 -123.47570192]\n",
      "First 10 labels: [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"commedia_ll shape: {commedia_ll.shape}\")\n",
    "print(f\"commedia_labels shape: {commedia_labels.shape}\")\n",
    "print(f\"First 10 logLikelihoods of Inferno: {commedia_ll[0, :10]}\")\n",
    "print(f\"First 10 labels: {commedia_labels[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e6579",
   "metadata": {},
   "source": [
    "Let's create a new function that computes the confusionMatrix given the log-likelihoods and the Priors. The classification rule used is always the maximum Posterior class probability Decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d28ee959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#always import MVG before using this function!\n",
    "\n",
    "def computeConfMatrixFromLL(LVAL, logLikelihoods, Priors, useLog=True):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix for the predicted labels and the actual labels.\n",
    "    Args:\n",
    "    - logLikelihoods: matriix of log likelihoods for each class\n",
    "    - Priors: array of priors for each class, priors are application dependent\n",
    "    - useLog: if True, use log likelihoods, else use normal likelihoods\n",
    "\n",
    "    Returns:\n",
    "    - Confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    SJoint = MVG.computeSJoint(logLikelihoods, Priors, useLog=useLog) #compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    SPost = MVG.computePosteriors(SJoint, useLog=True)  #compute the posteriors by normalizing the joint densities\n",
    "    PVAL = np.argmax(SPost, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "\n",
    "    #call the computeConfMatrix function to compute the confusion matrix\n",
    "    return computeConfMatrix(PVAL, LVAL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072842f8",
   "metadata": {},
   "source": [
    "For computing the Confusion Matrix for the *Commedia* dataset, we assume uniform Priors for each cantica: $P(l\\_lInf) = P(l\\_lPur) = P(l\\_lPar) = \\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b05108ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix, Commedia Classifier:\n",
      "[[210. 113.  61.]\n",
      " [137. 191. 111.]\n",
      " [ 53.  98. 230.]]\n"
     ]
    }
   ],
   "source": [
    "#Compute the confusion matrix for the log likelihoods\n",
    "#Assume uniform priors for each class\n",
    "\n",
    "confMatrix_Commedia = computeConfMatrixFromLL(commedia_labels, commedia_ll, np.ones((3, )) / 3., useLog=True)\n",
    "print(f\"\\nConfusion Matrix, Commedia Classifier:\\n{confMatrix_Commedia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a1371",
   "metadata": {},
   "source": [
    "## Optimal Bayes decision\n",
    "The goal of a classifier is to allow us to choose an action $a$ to perform among\n",
    "a set of actions $\\mathcal{A}$. In the context of classification, an action can be simply \"Classify sample $x_t$ with label $k$\", although we can have also more complex types of actions. <br> \n",
    "We can associate to each action a **cost** $C(a \\mid k)$ that we have to pay when we choose action $a$ and the sample belongs to class $k$. This can be seen as a missclassification cost, which depends both on the actual and predicted class. <br>\n",
    "Unluckily, at evaluation time we don't know the actual classes of the samples (what whould the point of classification be, otherwise?), but we have access to the Priors and we can calculate the Posterios. These are useful to compute the costs. <br>\n",
    "For a $K$-class problem (where classes are numbered from $0$ up to $K-1$), let's denote the Priors as:\n",
    "$$\n",
    "\\pi = \\begin{bmatrix} \\pi_0 \\\\ \\vdots \\\\ \\pi_{K-1} \\end{bmatrix}\n",
    "$$\n",
    "We can compute the class Posteriors, conditioned on the **Recognizer** $\\mathcal{R}$ that was used to compute them (the Recognizer is our Classifier) by applying Bayes' theorem. This involves expressing the Joint probability in terms of the Likelihood of the data given the class and the Prior probability of the class. The Posterior probability is then obtained by normalizing these terms by the sum over all possible classes:\n",
    "$$\n",
    "P(C = c | x, \\mathcal{R}) = \\frac{f_{X|C,\\mathcal{R}}(x|c)\\pi_c}{\\sum_{k=0}^{K-1} f_{X|C,\\mathcal{R}}(x|k)\\pi_k}\n",
    "$$\n",
    "These class Posteriors are conditioned on the Recognizer because they represent the **believes** that the Recognizer has about each sample belonging to a class $k$. We can compute the **Expected Cost** that we'll pay, according to these believes (which are the best of our knowledge at evaluation time):\n",
    "$$\n",
    "C_{X, \\mathcal{R}}(a) = \\mathop{\\mathbb{E}} \\left[ C(a \\mid k) \\mid x, \\mathcal{R}\\right] = \\sum_{k=0}^{K-1} P(C = k \\mid x, \\mathcal{R}) C(a \\mid k)\n",
    "$$\n",
    "where:\n",
    "- $a$ is the action\n",
    "- $k$ is the class\n",
    "- $x$ is the test sample\n",
    "- $\\mathcal{R}$ is the Recognizer, so the Classifier\n",
    "\n",
    "So in practice we calculate the Posteriors according to the Recognizer, and then we multiply them by the Cost of taking action $a$ given class $k$. The Expected Cost is obatined by summing this quantity all over all classes, from $0$ to $K-1$. <br>\n",
    "Regarding the costs $ C(a \\mid k)$, we can define the **cost matrix** as: \n",
    "$$\n",
    "\\mathbf{C} = \\begin{bmatrix}\n",
    "0 & C_{0,1} & \\cdots & C_{0,K-1} \\\\\n",
    "C_{1,0} & 0 & \\cdots & C_{1,K-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "C_{K-1,0} & C_{K-1,1} & \\cdots & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $C_{i,j}$ represents the cost of predicting class $i$ when the actual class is $j$. <br>\n",
    "\n",
    "-------------------------\n",
    "## Example: Rain vs. Clear Classification\n",
    "\n",
    "For example, let's consider a binary classification problem where the possible classes are \"Rain\" and \"Clear\".\n",
    "\n",
    "**Confusion Matrix**\n",
    "\n",
    "The confusion matrix is a table that summarizes the performance of a classification model. For our two classes, it would look like this:\n",
    "\n",
    "$$\n",
    "\\text{Confusion Matrix} = \\begin{bmatrix}\n",
    "\\text{True Positive (TP)} & \\text{False Positive (FP)} \\\\\n",
    "\\text{False Negative (FN)} & \\text{True Negative (TN)}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\text{Predicted Rain | Actual Rain} & \\text{Predicted Rain | Actual Clear} \\\\\n",
    "\\text{Predicted Clear | Actual Rain} & \\text{Predicted Clear | Actual Clear}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* **TP (True Positive):** The number of times the model correctly predicted \"Rain\" when it was actually raining.\n",
    "* **FP (False Positive):** The number of times the model incorrectly predicted \"Rain\" when it was actually \"Clear\" (also known as a Type I error).\n",
    "* **FN (False Negative):** The number of times the model incorrectly predicted \"Clear\" when it was actually raining (also known as a Type II error).\n",
    "* **TN (True Negative):** The number of times the model correctly predicted \"Clear\" when it was actually clear.\n",
    "\n",
    "**Cost Matrix**\n",
    "\n",
    "The cost matrix defines the cost associated with each type of prediction outcome. Let's assume the following costs:\n",
    "\n",
    "* Correctly predicting \"Rain\" (TP) has no cost: 0\n",
    "* Incorrectly predicting \"Rain\" when it's \"Clear\" (FP) has a cost of 1 (e.g., inconvenience of carrying an umbrella unnecessarily).\n",
    "* Incorrectly predicting \"Clear\" when it's raining (FN) has a higher cost of 5 (e.g., getting caught in the rain without an umbrella).\n",
    "* Correctly predicting \"Clear\" (TN) has no cost: 0\n",
    "\n",
    "Based on these costs, the cost matrix would be:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\begin{bmatrix}\n",
    "C_{\\text{Predicted Rain, Actual Rain}} & C_{\\text{Predicted Rain, Actual Clear}} \\\\\n",
    "C_{\\text{Predicted Clear, Actual Rain}} & C_{\\text{Predicted Clear, Actual Clear}}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "5 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $C_{0,0} = 0$: Cost of predicting \"Rain\" (index 0) when the actual class is \"Rain\" (index 0).\n",
    "* $C_{0,1} = 1$: Cost of predicting \"Rain\" (index 0) when the actual class is \"Clear\" (index 1).\n",
    "* $C_{1,0} = 5$: Cost of predicting \"Clear\" (index 1) when the actual class is \"Rain\" (index 0).\n",
    "* $C_{1,1} = 0$: Cost of predicting \"Clear\" (index 1) when the actual class is \"Clear\" (index 1).\n",
    "\n",
    "-------------------------\n",
    "\n",
    "\n",
    "The optimal Bayes decision consists in predicting the class $c^*$ which has minimum expected Bayes cost:\n",
    "$$\n",
    "c^* = argmin_c \\{ C_{X, \\mathcal{R}}(a) \\}\n",
    "$$\n",
    "\n",
    "## Binary task: optimal decisions\n",
    "Now let's consider **binary tasks** in which we have two classes, that can always be summarized as class *True* - $H_T$ - and class *False* -$H_F$. IN this case the cost matrix is always:\n",
    "$$\n",
    "\\mathbf{C} = \\begin{bmatrix}\n",
    "C_{\\text{Predicted } H_F \\text{, Actual } H_F} & C_{\\text{Predicted } H_F \\text{, Actual } H_T} \\\\\n",
    "C_{\\text{Predicted } H_T   \\text{, Actual } H_F} & C_{\\text{Predicted } H_T \\text{, Actual } H_T}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "C(H_F \\mid H_F) & C(H_F \\mid H_T) \\\\\n",
    "C(H_T \\mid H_F) & C(H_T \\mid H_T)\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "C_{tn} & C_{fn} \\\\\n",
    "C_{fp} & C_{tp}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The cost for predicting the right class is of course zero so we can rewrite the matrix as:\n",
    "$$\n",
    "\\mathbf{C} = \\begin{bmatrix}\n",
    "0 & C_{fn} \\\\\n",
    "C_{fp} & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "So, applying the formula, the expected Bayes costs for predicting either of the two\n",
    "classes are:\n",
    "$$\n",
    "C_{x,\\mathcal{R}}(H_F) = C (H_F \\mid H_F) P(C = H_F | x, \\mathcal{R}) + C (H_F \\mid H_T) P(C = H_T | x, \\mathcal{R}) = C_{fn}P(C = H_T | x, \\mathcal{R}) \\\\\n",
    "C_{x,\\mathcal{R}}(H_T) = C (H_T \\mid H_F) P(C = H_F | x, \\mathcal{R}) + C (H_T \\mid H_T) P(C = H_T | x, \\mathcal{R}) = C_{fp}P(C = H_F | x, \\mathcal{R})\n",
    "$$\n",
    "We predict the class $c^*$ that has minimum cost: $c^* = argmin_c \\{ C_{X, \\mathcal{R}}(c) \\}$ <br>\n",
    "For the binary task, we can also\n",
    "express $c^*$ taking into account the logarithm of the two costs:\n",
    "$$\n",
    "c^* =\n",
    "\\begin{cases}\n",
    "    H_T & \\text{if } \\log \\frac{C_{fn} P(C = H_T | x, \\mathcal{R})}{C_{fp} P(C = H_F | x, \\mathcal{R})} > 0 \\\\ \\\\\n",
    "    H_F & \\text{if } \\log \\frac{C_{fn} P(C = H_T | x, \\mathcal{R})}{C_{fp} P(C = H_F | x, \\mathcal{R})} \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "If $\\mathcal{R}$ is a Generative Model, we can factorize the class Posteriors into Likelihoods and Priors:\n",
    "\n",
    "$$\n",
    "c^* =\n",
    "\\begin{cases}\n",
    "    H_T & \\text{if } \\log \\frac{\\pi_{H_T} C_{fn} f_{X|C,\\mathcal{R}}(x|H_T)}{(1-\\pi_{H_T}) C_{fp} f_{X|C,\\mathcal{R}}(x|H_F)} > 0 \\\\ \\\\\n",
    "    H_F & \\text{if } \\log \\frac{\\pi_{H_T} C_{fn} f_{X|C,\\mathcal{R}}(x|H_T)}{(1-\\pi_{H_T}) C_{fp} f_{X|C,\\mathcal{R}}(x|H_F)} \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Where $\\pi_{H_T} = P(C = H_T)$ and $1 - \\pi_{H_T} = P(C = H_F)$ are the two Priors. <br>\n",
    "By recalling that the ratio between the two likelihoods is the **log-likelihood ratio** - $\\text{llr}(x)$ and the ratio between the Priors is the **Prior log odds**, we can compare the $\\text{llr}(x)$, acting a a **Score**, to the other quantities, that act as a **Threshold**:\n",
    "\n",
    "$$\n",
    "c^* =\n",
    "\\begin{cases}\n",
    "    H_T & \\text{if } \\text{llr}(x) = \\log \\frac{f_{X|C,\\mathcal{R}}(x|H_T)}{f_{X|C,\\mathcal{R}}(x|H_F)} > - \\log \\frac{(1-\\pi_{H_T}) C_{H_T}}{\\pi_{H_T} C_{fn}} \\\\ \\\\\n",
    "    H_F & \\text{if } \\text{llr}(x) = \\log \\frac{f_{X|C,\\mathcal{R}}(x|H_T)}{f_{X|C,\\mathcal{R}}(x|H_F)} \\le - \\log \\frac{(1-\\pi_{H_T}) C_{fp}}{\\pi_{H_T} C_{fn}}\n",
    "\\end{cases}\n",
    "$$\n",
    "Where here the Threshold takes into account both Priors (defined by $\\pi_{H_T}$ and $1-\\pi_{H_T}$) and Costs of Errors (which are $C_{fp}$, $C_{fn}$). <br> <br>\n",
    "The *triplet* $\\left( \\pi_{H_T}, C_{fp}, C_{fn}\\right)$ denotes the **application**: since these three values are specific to the problem we're trying to solve, they define the specific application or context of the decision-making process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9f651",
   "metadata": {},
   "source": [
    "In practice, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
