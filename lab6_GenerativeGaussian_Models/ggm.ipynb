{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Gaussian Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use again the *iris* dataset, and solve the iris classification prolem using Gaussian classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from load_dataset import loadDataSet                                #for loading the dataset\n",
    "from train_validation_split import splitTrainingValidation          #for splitting the dataset into training and validation sets\n",
    "from mean_covariance import vcol, vrow, compute_mu_C                #for computing the empirical mean and the empirical covariance of the dataset \n",
    "from logpdf_loglikelihood_GAU import logpdf_GAU_ND                  #for computing the log-likelihood of the Gaussian distribution\n",
    "from sklearn.metrics import classification_report                   #for generating the classification report of the models\n",
    "from scipy.special import logsumexp                                 #for scipy.special.logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (4, 150)\n",
      "Labels shape:  (150,)\n"
     ]
    }
   ],
   "source": [
    "numFeatures = 4\n",
    "\n",
    "#load the iris dataset\n",
    "D, L = loadDataSet('iris.csv', numFeatures)\n",
    "print(\"Data shape: \", D.shape)\n",
    "print(\"Labels shape: \", L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (4, 100)\n",
      "Training labels shape:  (100,)\n",
      "Evaluation data shape:  (4, 50)\n",
      "Evaluation labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset into training and validation sets\n",
    "#DTR and LTR are training data and labels, DTE and LTE are evaluation (or more precisely validation) data and labels\n",
    "(DTR, LTR), (DVAL, LVAL) = splitTrainingValidation(2/3, D, L)\n",
    "print(\"Training data shape: \", DTR.shape)\n",
    "print(\"Training labels shape: \", LTR.shape)\n",
    "print(\"Evaluation data shape: \", DVAL.shape)\n",
    "print(\"Evaluation labels shape: \", LVAL.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 100 samples for training and 50 samples for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian Classifier\n",
    "The optimal Bayes decision is to select for each test point the class with highest **posterior probability**: having class $c$ and $x_{t}$ as test point, we can thus write:\n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) \\rightarrow We \\space assign \\space x_{t} \\space to \\space the \\space class \\space having \\space the \\space highest \\space Posterior \\space probability\n",
    "$$ \n",
    "We will assume that the samples are independent and identically distributed (*i.i.d.*) according to $(\\mathbf{X}_t, C_{t}) ∼ (\\mathbf{X}, C)$. <br>\n",
    "Let $f_{X,C}$ be the joint density of $X, C$: we can\n",
    "compute the joint likelihood for the hypothesized class $c$ for the observed test\n",
    "sample $x_{t}$ as $f_{X,C}(x_{t}, c)$ and then use **Bayes rule** to compute the class posterior\n",
    "probability:\n",
    "$$\n",
    "P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')}\n",
    "$$\n",
    "We can factorize the joint density as:\n",
    "$$\n",
    "f_{\\mathbf{X}_t, C_t}(\\mathbf{x}_t, c) = f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c) P(c)\n",
    "$$\n",
    "Where:\n",
    "- $f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c)$ is the class conditional distribution \n",
    "- $P(c)$ is called *Prior probabilty*: it's application-dependent and describes the probability of the class being $c$ **before** we observe $x_{t}$ \n",
    "\n",
    "In this specific case, we assume that our data, given the class, can be described by a **Gaussian distribution**:\n",
    "$$\n",
    "(\\mathbf{X}_t \\mid C_{t} = c) ∼ (\\mathbf{X} \\mid C = c) ∼ \\mathcal{N}(\\mathbf{µ_{c}}, \\mathbf{Σ_{c}})\n",
    "$$\n",
    "If we knew $\\mathbf{µ_{c}}$, $\\mathbf{Σ_{c}}$ then we could compute the conditional this way;\n",
    "$$\n",
    "f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c) = \\mathcal{N}(\\mathbf{µ_{c}}, \\mathbf{Σ_{c}})\n",
    "$$\n",
    "The problem is that we don't have **these parameters** $\\theta = [(\\mathbf{µ_{1}}, \\mathbf{Σ_{1}}), . . . ,(\\mathbf{µ_{k}}, \\mathbf{Σ_{k}})] $, where $k$ is the number of different classes. <br>\n",
    "However, since we have at our disposal a *labeled Dataset*, we can assume:\n",
    "- Gaussian distribution for $\\mathbf{X} \\mid C$\n",
    "- That, given the model parameters $\\theta$, all the samples observations are *i.i.d* \n",
    "\n",
    "After (and only after) making these assumptions, we can plug in the **Maximum Likelihood Estimators** (*MLE*), which, for a **MVG** distribution, are the empirical mean and covariance matrix of each class. So, for each class $c$ we can compute the two estimators:\n",
    "$$\n",
    "\\mu^{MLE}_{c} = \\frac{1}{N_c} \\sum_{i} x_{c,i}, \\quad \n",
    "\\Sigma^{MLE}_{c} = \\frac{1}{N_c} \\sum_{i} (x_{c,i} - \\mu^*_c)(x_{c,i} - \\mu^*_c)^T\n",
    "$$\n",
    "Where $x_{c,i}$ is the $i$-th sample of class $c$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_0:\n",
      "[[4.96129032]\n",
      " [3.42903226]\n",
      " [1.46451613]\n",
      " [0.2483871 ]]\n",
      "Shape: (4, 1)\n",
      "mu_1:\n",
      "[[5.91212121]\n",
      " [2.78484848]\n",
      " [4.27272727]\n",
      " [1.33939394]]\n",
      "Shape: (4, 1)\n",
      "mu_2:\n",
      "[[6.45555556]\n",
      " [2.92777778]\n",
      " [5.41944444]\n",
      " [1.98888889]]\n",
      "Shape: (4, 1)\n",
      "C_0:\n",
      "[[0.13140479 0.11370447 0.02862643 0.01187305]\n",
      " [0.11370447 0.16270552 0.01844953 0.01117586]\n",
      " [0.02862643 0.01844953 0.03583767 0.00526535]\n",
      " [0.01187305 0.01117586 0.00526535 0.0108845 ]]\n",
      "Shape: (4, 4)\n",
      "C_1:\n",
      "[[0.26470156 0.09169881 0.18366391 0.05134068]\n",
      " [0.09169881 0.10613407 0.08898072 0.04211203]\n",
      " [0.18366391 0.08898072 0.21955923 0.06289256]\n",
      " [0.05134068 0.04211203 0.06289256 0.03208448]]\n",
      "Shape: (4, 4)\n",
      "C_2:\n",
      "[[0.30080247 0.08262346 0.18614198 0.04311728]\n",
      " [0.08262346 0.08533951 0.06279321 0.05114198]\n",
      " [0.18614198 0.06279321 0.18434414 0.04188272]\n",
      " [0.04311728 0.05114198 0.04188272 0.0804321 ]]\n",
      "Shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "#Compute the MLE estimators of a MVG distribtion, which are the empirical mean and covariance of the training data\n",
    "mu_0, C_0, = compute_mu_C(DTR[:, LTR == 0])\n",
    "mu_1, C_1, = compute_mu_C(DTR[:, LTR == 1])\n",
    "mu_2, C_2, = compute_mu_C(DTR[:, LTR == 2])\n",
    "\n",
    "print(f\"mu_0:\\n{mu_0}\\nShape: {mu_0.shape}\")\n",
    "print(f\"mu_1:\\n{mu_1}\\nShape: {mu_1.shape}\")\n",
    "print(f\"mu_2:\\n{mu_2}\\nShape: {mu_2.shape}\")\n",
    "print(f\"C_0:\\n{C_0}\\nShape: {C_0.shape}\")\n",
    "print(f\"C_1:\\n{C_1}\\nShape: {C_1.shape}\")\n",
    "print(f\"C_2:\\n{C_2}\\nShape: {C_2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the estimated model, we now turn our attention towards inference for a test sample $x$. As we\n",
    "have seen, the final goal is to compute class posterior probabilities $P(c \\mid \\mathbf{x})$. We split the process in three\n",
    "stages:\n",
    "\n",
    "*Stage 1*: For each sample we compute the likelihoods, so the class conditional probabilities as:\n",
    "$$\n",
    "f_{X|C} (x_t | c) = \\mathcal{N} (x_t | \\mu^{MLE}_c, \\Sigma^{MLE}_c)\n",
    "$$\n",
    "\n",
    "**Beware**: model params were estimated using the *training samples*, whereas densities are computed using *estimation samples*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logpdf_0 Shape: (50,)\n",
      "logpdf_1 Shape: (50,)\n",
      "logpdf_2 Shape: (50,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#For each class Compute the log-pdf of the training data given the MLE parameters of the MVG distribution\n",
    "#It's better to compute the log-pdf and not the pdf, because the pdf can be very small and can cause numerical problems (underflow)\n",
    "#Then the logpdf gets exponentiated and the numerical problems are avoided\n",
    "logpdf_0 = logpdf_GAU_ND(DVAL, mu_0, C_0)\n",
    "logpdf_1 = logpdf_GAU_ND(DVAL, mu_1, C_1)\n",
    "logpdf_2 = logpdf_GAU_ND(DVAL, mu_2, C_2)\n",
    "\n",
    "print(f\"logpdf_0 Shape: {logpdf_0.shape}\")\n",
    "print(f\"logpdf_1 Shape: {logpdf_1.shape}\")\n",
    "print(f\"logpdf_2 Shape: {logpdf_2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now in order to compute the pds I exponentiate the log-likelihoods\n",
    "pds_0 = np.exp(logpdf_0)\n",
    "pds_1 = np.exp(logpdf_1)\n",
    "pds_2 = np.exp(logpdf_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can automate the process and compute a *Score Matrix* having for each row i the conditional of class i and so $S[i, j]$ is the pdf of the j-th sample given the i-th class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreMatrix_Pdf_GAU(D, params):\n",
    "    \"\"\"\n",
    "    Compute the Pdf of the data given the parameters of a Gaussian distribution\n",
    "    and populate the score matrix S with the log-pdf of each class\n",
    "    #The score matrix is filled with the pdfs of the training data given the MLE parameters of the MVG distribution\n",
    "    #S[i, j] is the pdf of the j-th sample given the i-th class\n",
    "\n",
    "    Parameters:\n",
    "    - D: the data matrix of shape (numFeatures, numSamples)\n",
    "    - params: the model parameters, so  list of tuples (mu, C) where mu is the mean vector fo class c and C is the covariance matrix of class c\n",
    "\n",
    "    Returned Values:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "\n",
    "    \"\"\"\n",
    "    numClasses = len(params) #number of classes, since for each class we have a tuple (mu, C)\n",
    "\n",
    "    \n",
    "    S = np.zeros((numClasses, D.shape[1]))\n",
    "    for label in range(numClasses):\n",
    "        S[label, :] = np.exp(logpdf_GAU_ND(D, params[label][0], params[label][1]))\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#Compute score matrix S of log likelihoods for each sample and class\n",
    "S_Likelihoods = scoreMatrix_Pdf_GAU(DVAL, [(mu_0, C_0), (mu_1, C_1), (mu_2, C_2)])\n",
    "print(f\"Score matrix shape: {S_Likelihoods.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 2*: We multiply the class conditional probabilities, computed before, with the class *Prior* probabilities. In\n",
    "the following we assume that the three classes have the same Prior probability $P(c) = 1/3$. We can thus\n",
    "compute the joint distribution for samples and classes as:\n",
    "$$\n",
    "f_{X,C}(x_t, c) = f_{X|C}(x_t | c) P_C(c)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSJoint(S, Priors):\n",
    "    \"\"\"\n",
    "    Compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    #The joint densities are the product of the score matrix S with the Priors\n",
    "\n",
    "    Parameters:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "    - Priors: the priors of the classes, so a list of length numClasses\n",
    "\n",
    "    Returned Values:\n",
    "    - SJoint: the joint densities of shape (numClasses, numSamples) where each row is the joint density of the class given the sample\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #Old implementation of computeSJoint:\n",
    "\n",
    "\n",
    "    numClasses = len(Priors) #number of classes, since we have 1 prior for each class\n",
    "    newS = np.zeros((numClasses, S.shape[1])) #initialize newS with zeros\n",
    "\n",
    "    for classIndex in range(numClasses):\n",
    "        #multiply each row of S (where 1 row corresponds to a class) with the prior of the class\n",
    "        newS[classIndex, :] = S[classIndex, :] * Priors[classIndex]\n",
    "\n",
    "\n",
    "    return newS\n",
    "    \"\"\"\n",
    "\n",
    "    #S has shape: (numClasses, numSamples)\n",
    "    #Priors has shape: (numClasses, ) -> it's a row vector\n",
    "    #To correctly perform the multiplication, we need to transpose Priors to make it a column vector\n",
    "    return S * vcol(Priors) #multiply each row of S (where 1 row corresponds to a class) with the prior of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint densities shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "SJoint_MVG = computeSJoint(S_Likelihoods, np.ones((3, )) / 3.) #compute the joint densities by multiplying the score matrix S with the Priors\n",
    "print(f\"Joint densities shape: {SJoint_MVG.shape}\")\n",
    "\n",
    "SJoint_MVG_Sol = np.load(\"./solutions/SJoint_MVG.npy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if the joint densities are equal to the solution\n",
    "#Beware: the joint densities are not equal to the solution, but they are very close to the solution due to numerical problems\n",
    "np.allclose(SJoint_MVG, SJoint_MVG_Sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem stemming from this technique is that these calculations originate many numeric problems! That's why the expressions like: \n",
    "```python\n",
    "SJoint_MVG==SJoint_MVG_Sol\n",
    "```\n",
    "return False whereas expressions like:\n",
    "```python\n",
    "np.allclose(SJoint_MVG, SJoint_MVG_Sol)\n",
    "```\n",
    "return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 3*: Finally, we can compute the class Posteriors probabilities as:\n",
    "$$\n",
    "P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')}\n",
    "$$\n",
    "At the denominator we sum the joint probability over all classes to compute the marginal densities for each sample wich are $f_{\\mathbf{X}}(\\mathbf{x}_t)$ and have shape ```(1, DVAL.shape[1])```. The *axis_0* has shape equal to 1 since we sum over all the rows, corresponding to the joints for all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vrow(SJoint_MVG_Sol.sum(0)).shape #check if the first column of the joint densities are equal to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePosteriors(SJoint):\n",
    "    \"\"\"\n",
    "    Compute the posteriors by normalizing the joint densities\n",
    "    The posteriors are the joint densities divided by the sum of the joint densities which are the marginals\n",
    "\n",
    "    Parameters:\n",
    "    - SJoint: the joint densities of shape (numClasses, numSamples) where each row is the joint density of the class \n",
    "\n",
    "    Returned Values:\n",
    "    - SPost: the posteriors of shape (numClasses, numSamples) where each row is the posterior of the class given the sample\n",
    "    \"\"\"\n",
    "    #1. Compute marginals\n",
    "    SMarginal = vrow(SJoint.sum(0)) #sum over the rows (axis=0) to get the marginal of each sample\n",
    "\n",
    "    #2. Compute posteriors by dividing the joint densities by the marginals\n",
    "    SPost = SJoint / SMarginal #element wise division\n",
    "\n",
    "    return SPost\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "SPost_MVG = computePosteriors(SJoint_MVG) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"Posteriors shape: {SPost_MVG.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Rule**: As said before, the optimal Bayes decision is to select for each test sample the class with highest **posterior probability**: \n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 2 1 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "#Select for each sample the class with the highest posterior probability\n",
    "PVAL_MVG = np.argmax(SPost_MVG, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "print(f\"Predictions shape: {PVAL_MVG.shape}\")\n",
    "print(f\"Predictions: {PVAL_MVG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error calculation for the MVG ggm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong predictions: 2\n",
      "Error Rate: 4.00%\n"
     ]
    }
   ],
   "source": [
    "error_count_MVG = np.count_nonzero(PVAL_MVG != LVAL)\n",
    "print(f\"Number of wrong predictions: {error_count_MVG}\")\n",
    "error_rate_MVG = np.mean(PVAL_MVG != LVAL)\n",
    "print(f\"Error Rate: {error_rate_MVG:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Precision, Recall, F-1 Score for the MVG gmm model: <br>\n",
    "With 3 classes, accuracy is compute as:\n",
    "$$\n",
    "acc = \\frac{Correct \\space predictions}{Tot \\space samples} = \\frac{T0+T1+T2}{T0+T1+T2+F0+F1+F2} = 1 - Error \\space Rate\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        19\n",
      "           1      1.000     0.882     0.938        17\n",
      "           2      0.875     1.000     0.933        14\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.958     0.961     0.957        50\n",
      "weighted avg      0.965     0.960     0.960        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(LVAL, PVAL_MVG, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already discussed, working directly with densities is often problematic, due to numerical\n",
    "issues. It’s useful to implement the whole procedure directly in terms of log-densities (if we need, we can\n",
    "recover posterior probabilities at the end). <br>\n",
    "Working in the *log-domain*, the three stages for computing the class posterior probabilities $P(c \\mid \\mathbf{x})$ are: <br>\n",
    "*Stage 1*: For each sample we compute the log-likelihoods, so the class conditional log-probabilities as:\n",
    "$$\n",
    "\\log f_{X|C} (x_t | c) = \\log \\mathcal{N} (x_t | \\mu^{MLE}_c, \\Sigma^{MLE}_c)\n",
    "$$\n",
    "\n",
    "**Beware**: model params were estimated using the *training samples*, whereas densities are computed using *estimation samples*! <br>\n",
    "We can thus rewrite and extend the function `scoreMatrix_Pdf_GAU` written before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreMatrix_Pdf_GAU(D, params, useLog=False):  \n",
    "    #Compute the (log?)-Pdf of the data given the parameters of a Gaussian distribution and populate the score matrix S with the (log?)-pdf of each class\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - D: the data matrix of shape (numFeatures, numSamples)\n",
    "    - params: the model parameters, so  list of tuples (mu, C) where mu is the mean vector fo class c and C is the covariance matrix of class c\n",
    "    - useLog: if True, compute the log-pdf, else compute the pdf\n",
    "\n",
    "    Returned Values:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #The score matrix is filled with the pdfs of the training data given the MLE parameters of the MVG distribution\n",
    "    #S[i, j] is the pdf of the j-th sample given the i-th class\n",
    "    \n",
    "    numClasses = len(params) #number of classes, since for each class we have a tuple (mu, C)\n",
    "    S = np.zeros((numClasses, D.shape[1]))\n",
    "    for label in range(numClasses):\n",
    "        if useLog:\n",
    "            #if useLog is True, then compute the log-pdf\n",
    "            S[label, :] = logpdf_GAU_ND(D, params[label][0], params[label][1])\n",
    "        else:\n",
    "            #if useLog is False, then compute the pdf\n",
    "            S[label, :] = np.exp(logpdf_GAU_ND(D, params[label][0], params[label][1]))\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#Compute score matrix S of log likelihoods for each sample and class\n",
    "S_logLikelihoods = scoreMatrix_Pdf_GAU(DVAL, [(mu_0, C_0), (mu_1, C_1), (mu_2, C_2)], useLog=True)\n",
    "print(f\"log Score matrix shape: {S_logLikelihoods.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.exp(S_logLikelihoods), S_Likelihoods) #check if the log score matrix, upon exponentiation, is equal to the score matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 2*: We add the log class conditional probabilities, computed before, to the log of the class *Prior* probabilities. In\n",
    "the following we assume that the three classes have the same Prior probability $P(c) = 1/3$. We can thus\n",
    "compute the joint distribution for samples and classes in the *log-domain* as:\n",
    "$$\n",
    "l_{c} = \\log f_{X,C}(x_t, c) = \\log \\left( f_{X|C}(x_t | c) P_C(c) \\right) = \\log f_{X|C}(x_t | c) + \\log P_C(c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSJoint(S, Priors, useLog=False):\n",
    "    # Compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "    - Priors: the priors of the classes, so a list of length numClasses\n",
    "    - useLog: if True, compute the log-joint densities, else compute the joint densities\n",
    "\n",
    "    Returned Values:\n",
    "    - SJoint: the (log?)joint densities of shape (numClasses, numSamples) where each row is the joint density of the class given the sample\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if (useLog):\n",
    "        #S needs to be already in log scale, so we just need to add the log of the priors\n",
    "        return S + vcol(np.log(Priors)) #multiply each row of S (where 1 row corresponds to a class) with the prior of the class\n",
    "    else:\n",
    "        return S * vcol(Priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_MVG = computeSJoint(S_logLikelihoods, np.ones((3, )) / 3., useLog=True) #compute the joint densities by multiplying the score matrix S with the Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_MVG_Sol = np.load(\"./solutions/logSJoint_MVG.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.exp(SJoint_log_MVG), SJoint_MVG) #check if the log joint densities, upon exponentiation, are equal to the joint densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 3*: Finally, we can compute the log class Posteriors probabilities as:\n",
    "$$\n",
    "\\log P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')} \\right) = \n",
    "\\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{f_{\\mathbf{X}}(\\mathbf{x}_t)} \\right) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log  f_{\\mathbf{X}}(\\mathbf{x}_t) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log \\sum_{c} e^{l_{c}}\n",
    "$$ \n",
    "where $l_{c}$ are all the log-joints. <br>\n",
    "However, we need to take care that computing the exponential terms may result again in numerical\n",
    "errors. A robust method to comute $\\log \\sum_{c} e^{l_{c}}$ is to rewrite it as:\n",
    "$$\n",
    "\\log \\sum_{c} e^{l_{c}} = l + \\log \\sum_{c} e^{l_{c} - l}\n",
    "$$\n",
    "where $l$ is the highest of the log-joints: $l = max_{c} {l_{c}}$\n",
    "This is known as the *log-sum-exp* trick, and is already implemented in *scipy* as `scipy.special.logsumexp`. We can thus use `scipy.special.logsumexp(s)`,\n",
    "where `s` is the array that contains the joint log-probabilities for a given sample, to compute the log-marginals $\\log f_X(x_{t})$. <br>\n",
    "`scipy.special.logsumexp` also allows specifying an axis, thus we can directly compute the array of\n",
    "marginals for all samples directly from the matrix of joint log-densities as we did before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePosteriors(SJoint, useLog=False):\n",
    "    \"\"\"\n",
    "    Compute the posteriors by normalizing the joint densities\n",
    "    The posteriors are the joint densities divided by the sum of the joint densities which are the marginals\n",
    "\n",
    "    Parameters:\n",
    "    - SJoint: the joint densities of shape (numClasses, numSamples) where each row is the joint density of the class \n",
    "\n",
    "    Returned Values:\n",
    "    - SPost: the posteriors of shape (numClasses, numSamples) where each row is the posterior of the class given the sample\n",
    "    \"\"\"\n",
    "    if useLog:\n",
    "        #1. Compute marginals usign the logsumexp trick to minimize numerical problems\n",
    "        #logsumexp is a function that computes the log of the sum of exponentials of input elements\n",
    "        #It is more numerically stable than computing the sum of exponentials directly\n",
    "        #It computes log(exp(a) + exp(b)) in a numerically stable way\n",
    "\n",
    "        #sum over the rows (axis=0) to get the marginal of each sample\n",
    "        SMarginal = logsumexp(SJoint, axis=0)\n",
    "        #SMarginal has now shape = (numSamples, ) -> it's a row vector\n",
    "        #I need to make it of shape (1, numSamples) \n",
    "        SPost = SJoint - vrow(SMarginal) #element wise division in log scale, so I just need to subtract the marginals from the joint densities\n",
    "        \n",
    "\n",
    "    else:\n",
    "        \n",
    "        #1. Compute marginals\n",
    "        SMarginal = vrow(SJoint.sum(0)) #sum over the rows (axis=0) to get the marginal of each sample\n",
    "\n",
    "        #2. Compute posteriors by dividing the joint densities by the marginals\n",
    "        SPost = SJoint / SMarginal #element wise division\n",
    "\n",
    "    return SPost\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#calculate log S post\n",
    "SPost_log_MVG = computePosteriors(SJoint_log_MVG, useLog=True) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"log Posteriors shape: {SPost_log_MVG.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check that these posterios probabilities, unpon exponentiation, are the same as the one previosly computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPost_log_MVG_Sol = np.load(\"./solutions/logPosterior_MVG.npy\")\n",
    "#Check if the log posteriors are equal to the solution\n",
    "np.allclose(SPost_log_MVG, SPost_log_MVG_Sol) #check if the log posteriors are equal to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.exp(SPost_log_MVG), SPost_MVG) #check if the posteriors, upon exponentiation, are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Gaussian Classifier\n",
    "We now consider the Naive Bayes version of the classifier. As we have seen, the Naive Bayes version of\n",
    "the MVG is simply a Gaussian classifier where the **covariance matrices are diagonal**. The MLE solution\n",
    "for the mean parameters is the same. For the covariance matrices we diagonalize the MLE solution:\n",
    "$$\n",
    "\\Sigma_c^{MLE, \\space Naive Bayes} = \\text{diag}(\\Sigma_c^{MLE}) = \\text{diag} \\left[ \\frac{1}{N_c} \\sum_i (x_{c,i} - \\mu_c^{MLE})(x_{c,i} - \\mu_c^{MLE})^T \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu_0, mu_1, mu_2 are good\n",
    "# compute C_0, C_1, C_2 for Naive Bayes\n",
    "\n",
    "\"\"\"\n",
    "np.diag(np.array([1, 2, 3])) gives as result:\n",
    "array([[1, 0, 0],\n",
    "       [0, 2, 0],\n",
    "       [0, 0, 3]])\n",
    "You have to repeat twice np.diag since theinner np.diag extracts the diagonal, the outer np.diag creates a diagonal matrix from the vector\n",
    "\"\"\"\n",
    "C_0_NB = np.diag(np.diag(C_0))\n",
    "C_1_NB = np.diag(np.diag(C_1))\n",
    "C_2_NB = np.diag(np.diag(C_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the procedure is exactly the **same** as with the MVG model: <br>\n",
    "*It's always better to work in the log-domain*\n",
    "- We compute the log class Posteriors in three stages (here they're represented in the equation from the last to the first):\n",
    "$$\n",
    "\\log P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')} \\right) = \n",
    "\\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{f_{\\mathbf{X}}(\\mathbf{x}_t)} \\right) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log  f_{\\mathbf{X}}(\\mathbf{x}_t) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log \\sum_{c} e^{l_{c}}\n",
    "$$ \n",
    "- We apply the classification rule (i.e. we assign text sample $x_{t}$ to the class $c$ having the highest log Posterior):\n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} \\log P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = argmax_{c} P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 1*: compute log Class Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "S_logLikelihoods_NB = scoreMatrix_Pdf_GAU(DVAL, [(mu_0, C_0_NB), (mu_1, C_1_NB), (mu_2, C_2_NB)], useLog=True)\n",
    "print(f\"log Score matrix shape: {S_logLikelihoods_NB.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 2*: compute log Joint Score Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_NB = computeSJoint(S_logLikelihoods_NB, np.ones((3, )) / 3., useLog=True) #compute the joint densities by multiplying the score matrix S with the Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SJoint_log_NB_Sol = np.load(\"./solutions/logSJoint_NaiveBayes.npy\")\n",
    "np.allclose(SJoint_log_NB, SJoint_log_NB_Sol) #check if the log joint densities are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 3*: compute log Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#calculate log S post\n",
    "SPost_log_NB = computePosteriors(SJoint_log_NB, useLog=True) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"log Posteriors shape: {SPost_log_MVG.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPost_log_NB_Sol = np.load(\"./solutions/logPosterior_NaiveBayes.npy\")\n",
    "np.allclose(SPost_log_NB, SPost_log_NB_Sol) #check if the log posteriors are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Classification rule*:\n",
    "$\n",
    "c_{t}^{*} = argmax_{c} \\log P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 1 2 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "#Select for each sample the class with the highest posterior probability\n",
    "PVAL_NB = np.argmax(SPost_log_NB_Sol, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "print(f\"Predictions shape: {PVAL_NB.shape}\")\n",
    "print(f\"Predictions: {PVAL_NB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error calculation for the Naive Bayes ggm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong predictions: 2\n",
      "Error Rate: 4.00%\n"
     ]
    }
   ],
   "source": [
    "error_count_NB = np.count_nonzero(PVAL_NB != LVAL)\n",
    "print(f\"Number of wrong predictions: {error_count_NB}\")\n",
    "error_rate_NB = np.mean(PVAL_NB != LVAL)\n",
    "print(f\"Error Rate: {error_rate_NB:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Precision, Recall, F-1 Score for the MVG gmm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        19\n",
      "           1      1.000     0.882     0.938        17\n",
      "           2      0.875     1.000     0.933        14\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.958     0.961     0.957        50\n",
      "weighted avg      0.965     0.960     0.960        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(LVAL, PVAL_MVG, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
