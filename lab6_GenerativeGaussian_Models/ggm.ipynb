{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Gaussian Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use again the *iris* dataset, and solve the iris classification prolem using Gaussian classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from load_dataset import loadDataSet                                #for loading the dataset\n",
    "from train_validation_split import splitTrainingValidation          #for splitting the dataset into training and validation sets\n",
    "from mean_covariance import vcol, vrow, compute_mu_C                #for computing the empirical mean and the empirical covariance of the dataset \n",
    "from logpdf_loglikelihood_GAU import logpdf_GAU_ND                  #for computing the log-likelihood of the Gaussian distribution\n",
    "from sklearn.metrics import classification_report                   #for generating the classification report of the models\n",
    "from scipy.special import logsumexp                                 #for scipy.special.logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (4, 150)\n",
      "Labels shape:  (150,)\n"
     ]
    }
   ],
   "source": [
    "numFeatures = 4\n",
    "\n",
    "#load the iris dataset\n",
    "D, L = loadDataSet('iris.csv', numFeatures)\n",
    "print(\"Data shape: \", D.shape)\n",
    "print(\"Labels shape: \", L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (4, 100)\n",
      "Training labels shape:  (100,)\n",
      "Evaluation data shape:  (4, 50)\n",
      "Evaluation labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset into training and validation sets\n",
    "#DTR and LTR are training data and labels, DTE and LTE are evaluation (or more precisely validation) data and labels\n",
    "(DTR, LTR), (DVAL, LVAL) = splitTrainingValidation(2/3, D, L)\n",
    "print(\"Training data shape: \", DTR.shape)\n",
    "print(\"Training labels shape: \", LTR.shape)\n",
    "print(\"Evaluation data shape: \", DVAL.shape)\n",
    "print(\"Evaluation labels shape: \", LVAL.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 100 samples for training and 50 samples for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian Classifier\n",
    "The optimal Bayes decision is to select for each test point the class with highest **posterior probability**: having class $c$ and $x_{t}$ as test point, we can thus write:\n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) \\rightarrow We \\space assign \\space x_{t} \\space to \\space the \\space class \\space having \\space the \\space highest \\space Posterior \\space probability\n",
    "$$ \n",
    "We will assume that the samples are independent and identically distributed (*i.i.d.*) according to $(\\mathbf{X}_t, C_{t}) ∼ (\\mathbf{X}, C)$. <br>\n",
    "Let $f_{X,C}$ be the joint density of $X, C$: we can\n",
    "compute the joint likelihood for the hypothesized class $c$ for the observed test\n",
    "sample $x_{t}$ as $f_{X,C}(x_{t}, c)$ and then use **Bayes rule** to compute the class posterior\n",
    "probability:\n",
    "$$\n",
    "P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')}\n",
    "$$\n",
    "We can factorize the joint density as:\n",
    "$$\n",
    "f_{\\mathbf{X}_t, C_t}(\\mathbf{x}_t, c) = f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c) P(c)\n",
    "$$\n",
    "Where:\n",
    "- $f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c)$ is the class conditional distribution \n",
    "- $P(c)$ is called *Prior probabilty*: it's application-dependent and describes the probability of the class being $c$ **before** we observe $x_{t}$ \n",
    "\n",
    "In this specific case, we assume that our data, given the class, can be described by a **Gaussian distribution**:\n",
    "$$\n",
    "(\\mathbf{X}_t \\mid C_{t} = c) ∼ (\\mathbf{X} \\mid C = c) ∼ \\mathcal{N}(\\mathbf{µ_{c}}, \\mathbf{Σ_{c}})\n",
    "$$\n",
    "If we knew $\\mathbf{µ_{c}}$, $\\mathbf{Σ_{c}}$ then we could compute the conditional this way;\n",
    "$$\n",
    "f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c) = \\mathcal{N}(\\mathbf{µ_{c}}, \\mathbf{Σ_{c}})\n",
    "$$\n",
    "The problem is that we don't have **these parameters** $\\theta = [(\\mathbf{µ_{1}}, \\mathbf{Σ_{1}}), . . . ,(\\mathbf{µ_{k}}, \\mathbf{Σ_{k}})] $, where $k$ is the number of different classes. <br>\n",
    "However, since we have at our disposal a *labeled Dataset*, we can assume:\n",
    "- Gaussian distribution for $\\mathbf{X} \\mid C$\n",
    "- That, given the model parameters $\\theta$, all the samples observations are *i.i.d* \n",
    "\n",
    "After (and only after) making these assumptions, we can plug in the **Maximum Likelihood Estimators** (*MLE*), which, for a **MVG** distribution, are the empirical mean and covariance matrix of each class. So, for each class $c$ we can compute the two estimators:\n",
    "$$\n",
    "\\mu^{MLE}_{c} = \\frac{1}{N_c} \\sum_{i} x_{c,i}, \\quad \n",
    "\\Sigma^{MLE}_{c} = \\frac{1}{N_c} \\sum_{i} (x_{c,i} - \\mu^*_c)(x_{c,i} - \\mu^*_c)^T\n",
    "$$\n",
    "Where $x_{c,i}$ is the $i$-th sample of class $c$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_0:\n",
      "[[4.96129032]\n",
      " [3.42903226]\n",
      " [1.46451613]\n",
      " [0.2483871 ]]\n",
      "Shape: (4, 1)\n",
      "mu_1:\n",
      "[[5.91212121]\n",
      " [2.78484848]\n",
      " [4.27272727]\n",
      " [1.33939394]]\n",
      "Shape: (4, 1)\n",
      "mu_2:\n",
      "[[6.45555556]\n",
      " [2.92777778]\n",
      " [5.41944444]\n",
      " [1.98888889]]\n",
      "Shape: (4, 1)\n",
      "C_0:\n",
      "[[0.13140479 0.11370447 0.02862643 0.01187305]\n",
      " [0.11370447 0.16270552 0.01844953 0.01117586]\n",
      " [0.02862643 0.01844953 0.03583767 0.00526535]\n",
      " [0.01187305 0.01117586 0.00526535 0.0108845 ]]\n",
      "Shape: (4, 4)\n",
      "C_1:\n",
      "[[0.26470156 0.09169881 0.18366391 0.05134068]\n",
      " [0.09169881 0.10613407 0.08898072 0.04211203]\n",
      " [0.18366391 0.08898072 0.21955923 0.06289256]\n",
      " [0.05134068 0.04211203 0.06289256 0.03208448]]\n",
      "Shape: (4, 4)\n",
      "C_2:\n",
      "[[0.30080247 0.08262346 0.18614198 0.04311728]\n",
      " [0.08262346 0.08533951 0.06279321 0.05114198]\n",
      " [0.18614198 0.06279321 0.18434414 0.04188272]\n",
      " [0.04311728 0.05114198 0.04188272 0.0804321 ]]\n",
      "Shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "#Compute the MLE estimators of a MVG distribtion, which are the empirical mean and covariance of the training data\n",
    "mu_0, C_0, = compute_mu_C(DTR[:, LTR == 0])\n",
    "mu_1, C_1, = compute_mu_C(DTR[:, LTR == 1])\n",
    "mu_2, C_2, = compute_mu_C(DTR[:, LTR == 2])\n",
    "\n",
    "print(f\"mu_0:\\n{mu_0}\\nShape: {mu_0.shape}\")\n",
    "print(f\"mu_1:\\n{mu_1}\\nShape: {mu_1.shape}\")\n",
    "print(f\"mu_2:\\n{mu_2}\\nShape: {mu_2.shape}\")\n",
    "print(f\"C_0:\\n{C_0}\\nShape: {C_0.shape}\")\n",
    "print(f\"C_1:\\n{C_1}\\nShape: {C_1.shape}\")\n",
    "print(f\"C_2:\\n{C_2}\\nShape: {C_2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the estimated model, we now turn our attention towards inference for a test sample $x$. As we\n",
    "have seen, the final goal is to compute class posterior probabilities $P(c \\mid \\mathbf{x})$. We split the process in three\n",
    "stages:\n",
    "\n",
    "*Stage 1*: For each sample we compute the likelihoods, so the class conditional probabilities as:\n",
    "$$\n",
    "f_{X|C} (x_t | c) = \\mathcal{N} (x_t | \\mu^{MLE}_c, \\Sigma^{MLE}_c)\n",
    "$$\n",
    "\n",
    "**Beware**: model params were estimated using the *training samples*, whereas densities are computed using *estimation samples*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logpdf_0 Shape: (50,)\n",
      "logpdf_1 Shape: (50,)\n",
      "logpdf_2 Shape: (50,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#For each class Compute the log-pdf of the training data given the MLE parameters of the MVG distribution\n",
    "#It's better to compute the log-pdf and not the pdf, because the pdf can be very small and can cause numerical problems (underflow)\n",
    "#Then the logpdf gets exponentiated and the numerical problems are avoided\n",
    "logpdf_0 = logpdf_GAU_ND(DVAL, mu_0, C_0)\n",
    "logpdf_1 = logpdf_GAU_ND(DVAL, mu_1, C_1)\n",
    "logpdf_2 = logpdf_GAU_ND(DVAL, mu_2, C_2)\n",
    "\n",
    "print(f\"logpdf_0 Shape: {logpdf_0.shape}\")\n",
    "print(f\"logpdf_1 Shape: {logpdf_1.shape}\")\n",
    "print(f\"logpdf_2 Shape: {logpdf_2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now in order to compute the pds I exponentiate the log-likelihoods\n",
    "pds_0 = np.exp(logpdf_0)\n",
    "pds_1 = np.exp(logpdf_1)\n",
    "pds_2 = np.exp(logpdf_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can automate the process and compute a *Score Matrix* having for each row i the conditional of class i and so $S[i, j]$ is the pdf of the j-th sample given the i-th class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreMatrix_Pdf_GAU(D, params):\n",
    "    \"\"\"\n",
    "    Compute the Pdf of the data given the parameters of a Gaussian distribution\n",
    "    and populate the score matrix S with the log-pdf of each class\n",
    "    #The score matrix is filled with the pdfs of the training data given the MLE parameters of the MVG distribution\n",
    "    #S[i, j] is the pdf of the j-th sample given the i-th class\n",
    "\n",
    "    Parameters:\n",
    "    - D: the data matrix of shape (numFeatures, numSamples)\n",
    "    - params: the model parameters, so  list of tuples (mu, C) where mu is the mean vector fo class c and C is the covariance matrix of class c\n",
    "\n",
    "    Returned Values:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "\n",
    "    \"\"\"\n",
    "    numClasses = len(params) #number of classes, since for each class we have a tuple (mu, C)\n",
    "\n",
    "    \n",
    "    S = np.zeros((numClasses, D.shape[1]))\n",
    "    for label in range(numClasses):\n",
    "        S[label, :] = np.exp(logpdf_GAU_ND(D, params[label][0], params[label][1]))\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#Compute score matrix S of log likelihoods for each sample and class\n",
    "S_Likelihoods = scoreMatrix_Pdf_GAU(DVAL, [(mu_0, C_0), (mu_1, C_1), (mu_2, C_2)])\n",
    "print(f\"Score matrix shape: {S_Likelihoods.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 2*: We multiply the class conditional probabilities, computed before, with the class *Prior* probabilities. In\n",
    "the following we assume that the three classes have the same Prior probability $P(c) = 1/3$. We can thus\n",
    "compute the joint distribution for samples and classes as:\n",
    "$$\n",
    "f_{X,C}(x_t, c) = f_{X|C}(x_t | c) P_C(c)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSJoint(S, Priors):\n",
    "    \"\"\"\n",
    "    Compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    #The joint densities are the product of the score matrix S with the Priors\n",
    "\n",
    "    Parameters:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "    - Priors: the priors of the classes, so a list of length numClasses\n",
    "\n",
    "    Returned Values:\n",
    "    - SJoint: the joint densities of shape (numClasses, numSamples) where each row is the joint density of the class given the sample\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #Old implementation of computeSJoint:\n",
    "\n",
    "\n",
    "    numClasses = len(Priors) #number of classes, since we have 1 prior for each class\n",
    "    newS = np.zeros((numClasses, S.shape[1])) #initialize newS with zeros\n",
    "\n",
    "    for classIndex in range(numClasses):\n",
    "        #multiply each row of S (where 1 row corresponds to a class) with the prior of the class\n",
    "        newS[classIndex, :] = S[classIndex, :] * Priors[classIndex]\n",
    "\n",
    "\n",
    "    return newS\n",
    "    \"\"\"\n",
    "\n",
    "    #S has shape: (numClasses, numSamples)\n",
    "    #Priors has shape: (numClasses, ) -> it's a row vector\n",
    "    #To correctly perform the multiplication, we need to transpose Priors to make it a column vector\n",
    "    return S * vcol(Priors) #multiply each row of S (where 1 row corresponds to a class) with the prior of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint densities shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "SJoint_MVG = computeSJoint(S_Likelihoods, np.ones((3, )) / 3.) #compute the joint densities by multiplying the score matrix S with the Priors\n",
    "print(f\"Joint densities shape: {SJoint_MVG.shape}\")\n",
    "\n",
    "SJoint_MVG_Sol = np.load(\"./solutions/SJoint_MVG.npy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if the joint densities are equal to the solution\n",
    "#Beware: the joint densities are not equal to the solution, but they are very close to the solution due to numerical problems\n",
    "np.allclose(SJoint_MVG, SJoint_MVG_Sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem stemming from this technique is that these calculations originate many numeric problems! That's why the expressions like: \n",
    "```python\n",
    "SJoint_MVG==SJoint_MVG_Sol\n",
    "```\n",
    "return False whereas expressions like:\n",
    "```python\n",
    "np.allclose(SJoint_MVG, SJoint_MVG_Sol)\n",
    "```\n",
    "return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 3*: Finally, we can compute the class Posteriors probabilities as:\n",
    "$$\n",
    "P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')}\n",
    "$$\n",
    "At the denominator we sum the joint probability over all classes to compute the marginal densities for each sample wich are $f_{\\mathbf{X}}(\\mathbf{x}_t)$ and have shape ```(1, DVAL.shape[1])```. The *axis_0* has shape equal to 1 since we sum over all the rows, corresponding to the joints for all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vrow(SJoint_MVG_Sol.sum(0)).shape #check if the first column of the joint densities are equal to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePosteriors(SJoint):\n",
    "    \"\"\"\n",
    "    Compute the posteriors by normalizing the joint densities\n",
    "    The posteriors are the joint densities divided by the sum of the joint densities which are the marginals\n",
    "\n",
    "    Parameters:\n",
    "    - SJoint: the joint densities of shape (numClasses, numSamples) where each row is the joint density of the class \n",
    "\n",
    "    Returned Values:\n",
    "    - SPost: the posteriors of shape (numClasses, numSamples) where each row is the posterior of the class given the sample\n",
    "    \"\"\"\n",
    "    #1. Compute marginals\n",
    "    SMarginal = vrow(SJoint.sum(0)) #sum over the rows (axis=0) to get the marginal of each sample\n",
    "\n",
    "    #2. Compute posteriors by dividing the joint densities by the marginals\n",
    "    SPost = SJoint / SMarginal #element wise division\n",
    "\n",
    "    return SPost\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "SPost_MVG = computePosteriors(SJoint_MVG) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"Posteriors shape: {SPost_MVG.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Rule**: As said before, the optimal Bayes decision is to select for each test sample the class with highest **posterior probability**: \n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 2 1 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "#Select for each sample the class with the highest posterior probability\n",
    "PVAL_MVG = np.argmax(SPost_MVG, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "print(f\"Predictions shape: {PVAL_MVG.shape}\")\n",
    "print(f\"Predictions: {PVAL_MVG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error calculation for the MVG ggm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong predictions: 2\n",
      "Error Rate: 4.00%\n"
     ]
    }
   ],
   "source": [
    "error_count_MVG = np.count_nonzero(PVAL_MVG != LVAL)\n",
    "print(f\"Number of wrong predictions: {error_count_MVG}\")\n",
    "error_rate_MVG = np.mean(PVAL_MVG != LVAL)\n",
    "print(f\"Error Rate: {error_rate_MVG:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Precision, Recall, F-1 Score for the MVG gmm model: <br>\n",
    "With 3 classes, accuracy is compute as:\n",
    "$$\n",
    "acc = \\frac{Correct \\space predictions}{Tot \\space samples} = \\frac{T0+T1+T2}{T0+T1+T2+F0+F1+F2} = 1 - Error \\space Rate\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        19\n",
      "           1      1.000     0.882     0.938        17\n",
      "           2      0.875     1.000     0.933        14\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.958     0.961     0.957        50\n",
      "weighted avg      0.965     0.960     0.960        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(LVAL, PVAL_MVG, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already discussed, working directly with densities is often problematic, due to numerical\n",
    "issues. It’s useful to implement the whole procedure directly in terms of log-densities (if we need, we can\n",
    "recover posterior probabilities at the end). <br>\n",
    "Working in the *log-domain*, the three stages for computing the class posterior probabilities $P(c \\mid \\mathbf{x})$ are: <br>\n",
    "*Stage 1*: For each sample we compute the log-likelihoods, so the class conditional log-probabilities as:\n",
    "$$\n",
    "\\log f_{X|C} (x_t | c) = \\log \\mathcal{N} (x_t | \\mu^{MLE}_c, \\Sigma^{MLE}_c)\n",
    "$$\n",
    "\n",
    "**Beware**: model params were estimated using the *training samples*, whereas densities are computed using *estimation samples*! <br>\n",
    "We can thus rewrite and extend the function `scoreMatrix_Pdf_GAU` written before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreMatrix_Pdf_GAU(D, params, useLog=False):  \n",
    "    #Compute the (log?)-Pdf of the data given the parameters of a Gaussian distribution and populate the score matrix S with the (log?)-pdf of each class\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - D: the data matrix of shape (numFeatures, numSamples)\n",
    "    - params: the model parameters, so  list of tuples (mu, C) where mu is the mean vector fo class c and C is the covariance matrix of class c\n",
    "    - useLog: if True, compute the log-pdf, else compute the pdf\n",
    "\n",
    "    Returned Values:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #The score matrix is filled with the pdfs of the training data given the MLE parameters of the MVG distribution\n",
    "    #S[i, j] is the pdf of the j-th sample given the i-th class\n",
    "    \n",
    "    numClasses = len(params) #number of classes, since for each class we have a tuple (mu, C)\n",
    "    S = np.zeros((numClasses, D.shape[1]))\n",
    "    for label in range(numClasses):\n",
    "        if useLog:\n",
    "            #if useLog is True, then compute the log-pdf\n",
    "            S[label, :] = logpdf_GAU_ND(D, params[label][0], params[label][1])\n",
    "        else:\n",
    "            #if useLog is False, then compute the pdf\n",
    "            S[label, :] = np.exp(logpdf_GAU_ND(D, params[label][0], params[label][1]))\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#Compute score matrix S of log likelihoods for each sample and class\n",
    "S_logLikelihoods = scoreMatrix_Pdf_GAU(DVAL, [(mu_0, C_0), (mu_1, C_1), (mu_2, C_2)], useLog=True)\n",
    "print(f\"log Score matrix shape: {S_logLikelihoods.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.exp(S_logLikelihoods), S_Likelihoods) #check if the log score matrix, upon exponentiation, is equal to the score matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 2*: We add the log class conditional probabilities, computed before, to the log of the class *Prior* probabilities. In\n",
    "the following we assume that the three classes have the same Prior probability $P(c) = 1/3$. We can thus\n",
    "compute the joint distribution for samples and classes in the *log-domain* as:\n",
    "$$\n",
    "l_{c} = \\log f_{X,C}(x_t, c) = \\log \\left( f_{X|C}(x_t | c) P_C(c) \\right) = \\log f_{X|C}(x_t | c) + \\log P_C(c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSJoint(S, Priors, useLog=False):\n",
    "    # Compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "    - Priors: the priors of the classes, so a list of length numClasses\n",
    "    - useLog: if True, compute the log-joint densities, else compute the joint densities\n",
    "\n",
    "    Returned Values:\n",
    "    - SJoint: the (log?)joint densities of shape (numClasses, numSamples) where each row is the joint density of the class given the sample\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if (useLog):\n",
    "        #S needs to be already in log scale, so we just need to add the log of the priors\n",
    "        return S + vcol(np.log(Priors)) #multiply each row of S (where 1 row corresponds to a class) with the prior of the class\n",
    "    else:\n",
    "        return S * vcol(Priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_MVG = computeSJoint(S_logLikelihoods, np.ones((3, )) / 3., useLog=True) #compute the joint densities by multiplying the score matrix S with the Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_MVG_Sol = np.load(\"./solutions/logSJoint_MVG.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.exp(SJoint_log_MVG), SJoint_MVG) #check if the log joint densities, upon exponentiation, are equal to the joint densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 3*: Finally, we can compute the log class Posteriors probabilities as:\n",
    "$$\n",
    "\\log P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')} \\right) = \n",
    "\\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{f_{\\mathbf{X}}(\\mathbf{x}_t)} \\right) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log  f_{\\mathbf{X}}(\\mathbf{x}_t) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log \\sum_{c} e^{l_{c}}\n",
    "$$ \n",
    "where $l_{c}$ are all the log-joints. <br>\n",
    "However, we need to take care that computing the exponential terms may result again in numerical\n",
    "errors. A robust method to comute $\\log \\sum_{c} e^{l_{c}}$ is to rewrite it as:\n",
    "$$\n",
    "\\log \\sum_{c} e^{l_{c}} = l + \\log \\sum_{c} e^{l_{c} - l}\n",
    "$$\n",
    "where $l$ is the highest of the log-joints: $l = max_{c} {l_{c}}$\n",
    "This is known as the *log-sum-exp* trick, and is already implemented in *scipy* as `scipy.special.logsumexp`. We can thus use `scipy.special.logsumexp(s)`,\n",
    "where `s` is the array that contains the joint log-probabilities for a given sample, to compute the log-marginals $\\log f_X(x_{t})$. <br>\n",
    "`scipy.special.logsumexp` also allows specifying an axis, thus we can directly compute the array of\n",
    "marginals for all samples directly from the matrix of joint log-densities as we did before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePosteriors(SJoint, useLog=False):\n",
    "    \"\"\"\n",
    "    Compute the posteriors by normalizing the joint densities\n",
    "    The posteriors are the joint densities divided by the sum of the joint densities which are the marginals\n",
    "\n",
    "    Parameters:\n",
    "    - SJoint: the joint densities of shape (numClasses, numSamples) where each row is the joint density of the class \n",
    "\n",
    "    Returned Values:\n",
    "    - SPost: the posteriors of shape (numClasses, numSamples) where each row is the posterior of the class given the sample\n",
    "    \"\"\"\n",
    "    if useLog:\n",
    "        #1. Compute marginals usign the logsumexp trick to minimize numerical problems\n",
    "        #logsumexp is a function that computes the log of the sum of exponentials of input elements\n",
    "        #It is more numerically stable than computing the sum of exponentials directly\n",
    "        #It computes log(exp(a) + exp(b)) in a numerically stable way\n",
    "\n",
    "        #sum over the rows (axis=0) to get the marginal of each sample\n",
    "        SMarginal = logsumexp(SJoint, axis=0)\n",
    "        #SMarginal has now shape = (numSamples, ) -> it's a row vector\n",
    "        #I need to make it of shape (1, numSamples) \n",
    "        SPost = SJoint - vrow(SMarginal) #element wise division in log scale, so I just need to subtract the marginals from the joint densities\n",
    "        \n",
    "\n",
    "    else:\n",
    "        \n",
    "        #1. Compute marginals\n",
    "        SMarginal = vrow(SJoint.sum(0)) #sum over the rows (axis=0) to get the marginal of each sample\n",
    "\n",
    "        #2. Compute posteriors by dividing the joint densities by the marginals\n",
    "        SPost = SJoint / SMarginal #element wise division\n",
    "\n",
    "    return SPost\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#calculate log S post\n",
    "SPost_log_MVG = computePosteriors(SJoint_log_MVG, useLog=True) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"log Posteriors shape: {SPost_log_MVG.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check that these posterios probabilities, unpon exponentiation, are the same as the one previosly computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPost_log_MVG_Sol = np.load(\"./solutions/logPosterior_MVG.npy\")\n",
    "#Check if the log posteriors are equal to the solution\n",
    "np.allclose(SPost_log_MVG, SPost_log_MVG_Sol) #check if the log posteriors are equal to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.exp(SPost_log_MVG), SPost_MVG) #check if the posteriors, upon exponentiation, are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Gaussian Classifier\n",
    "We now consider the Naive Bayes version of the classifier. As we have seen, the Naive Bayes version of\n",
    "the MVG is simply a Gaussian classifier where the **covariance matrices are diagonal**. The MLE solution\n",
    "for the mean parameters is the same. For the covariance matrices we diagonalize the MLE solution:\n",
    "$$\n",
    "\\Sigma_c^{MLE, \\space Naive Bayes} = \\text{diag}(\\Sigma_c^{MLE}) = \\text{diag} \\left[ \\frac{1}{N_c} \\sum_i (x_{c,i} - \\mu_c^{MLE})(x_{c,i} - \\mu_c^{MLE})^T \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu_0, mu_1, mu_2 are good\n",
    "# compute C_0, C_1, C_2 for Naive Bayes\n",
    "\n",
    "\"\"\"\n",
    "np.diag(np.array([1, 2, 3])) gives as result:\n",
    "array([[1, 0, 0],\n",
    "       [0, 2, 0],\n",
    "       [0, 0, 3]])\n",
    "You have to repeat twice np.diag since theinner np.diag extracts the diagonal, the outer np.diag creates a diagonal matrix from the vector\n",
    "\"\"\"\n",
    "C_0_NB = np.diag(np.diag(C_0))\n",
    "C_1_NB = np.diag(np.diag(C_1))\n",
    "C_2_NB = np.diag(np.diag(C_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the procedure is exactly the **same** as with the MVG model: <br>\n",
    "*It's always better to work in the log-domain*\n",
    "- We compute the log class Posteriors in three stages (here they're represented in the equation from the last to the first):\n",
    "$$\n",
    "\\log P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')} \\right) = \n",
    "\\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{f_{\\mathbf{X}}(\\mathbf{x}_t)} \\right) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log  f_{\\mathbf{X}}(\\mathbf{x}_t) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log \\sum_{c} e^{l_{c}}\n",
    "$$ \n",
    "- We apply the classification rule (i.e. we assign text sample $x_{t}$ to the class $c$ having the highest log Posterior):\n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} \\log P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = argmax_{c} P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 1*: compute log Class Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "S_logLikelihoods_NB = scoreMatrix_Pdf_GAU(DVAL, [(mu_0, C_0_NB), (mu_1, C_1_NB), (mu_2, C_2_NB)], useLog=True)\n",
    "print(f\"log Score matrix shape: {S_logLikelihoods_NB.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 2*: compute log Joint Score Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_NB = computeSJoint(S_logLikelihoods_NB, np.ones((3, )) / 3., useLog=True) #compute the joint densities by multiplying the score matrix S with the Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SJoint_log_NB_Sol = np.load(\"./solutions/logSJoint_NaiveBayes.npy\")\n",
    "np.allclose(SJoint_log_NB, SJoint_log_NB_Sol) #check if the log joint densities are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 3*: compute log Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#calculate log S post\n",
    "SPost_log_NB = computePosteriors(SJoint_log_NB, useLog=True) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"log Posteriors shape: {SPost_log_MVG.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPost_log_NB_Sol = np.load(\"./solutions/logPosterior_NaiveBayes.npy\")\n",
    "np.allclose(SPost_log_NB, SPost_log_NB_Sol) #check if the log posteriors are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Classification rule*:\n",
    "$\n",
    "c_{t}^{*} = argmax_{c} \\log P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 1 2 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "#Select for each sample the class with the highest posterior probability\n",
    "PVAL_NB = np.argmax(SPost_log_NB_Sol, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "print(f\"Predictions shape: {PVAL_NB.shape}\")\n",
    "print(f\"Predictions: {PVAL_NB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error calculation for the Naive Bayes ggm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong predictions: 2\n",
      "Error Rate: 4.00%\n"
     ]
    }
   ],
   "source": [
    "error_count_NB = np.count_nonzero(PVAL_NB != LVAL)\n",
    "print(f\"Number of wrong predictions: {error_count_NB}\")\n",
    "error_rate_NB = np.mean(PVAL_NB != LVAL)\n",
    "print(f\"Error Rate: {error_rate_NB:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Precision, Recall, F-1 Score for the Naive Bayes gmm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        19\n",
      "           1      1.000     0.882     0.938        17\n",
      "           2      0.875     1.000     0.933        14\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.958     0.961     0.957        50\n",
      "weighted avg      0.965     0.960     0.960        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(LVAL, PVAL_MVG, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tied Covariance Gaussian Classifier\n",
    "We now consider the Tied covariance version of the classifier, which assumes that the **covariance matrices of the different classes are tied**. This means that the noise is class-independent, so the\n",
    "distribution of class samples around the class mean is the same. <br>\n",
    "We have seen that the ML solution for the class means is again the same. The\n",
    "ML solution for the covariance matrix is given by the empirical within-class covariance matrix ($S_{w}$):\n",
    "$$\n",
    "\\Sigma^* = S_{w} = \\frac{1}{N} \\sum_c \\sum_i (x_{c,i} - \\mu_c^*) (x_{c,i} - \\mu_c^*)^T\n",
    "$$\n",
    "It's important, again, to underline te fact that $\\Sigma^* = S_{w}$ does **not** depend on the class $c$.\n",
    "Remember that we have already computed within-class covariance matrices when we implemented LDA, but beware: this **happens just using the Maximum Likelihood estimators**, with other techniques this is not the same. <br>\n",
    "This function below is taken from the LAB3 Code and modified to compute just $S_{w}$ and not $S_{b}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSw(D, L):\n",
    "    '''\n",
    "    Params:\n",
    "    - D: Dataset features matrix, not ceCntered\n",
    "    - L: Labels of the samples\n",
    "\n",
    "    Returned Values:\n",
    "    - Sw: Within-class scatter matrix\n",
    "    '''\n",
    "\n",
    "    #find the unique labels for each class\n",
    "    uniqueLabels = np.unique(L)\n",
    "\n",
    "    #nc in the formula is computed as the number of samples of class c\n",
    "    #separate data into classes\n",
    "    DC = [D[:, L == label] for label in uniqueLabels]  #DC[0] -> samples of class 0, DC[1] -> samples of class 1 etc...\n",
    "\n",
    "    #compute nc for each class\n",
    "    #each element in DC has a shape which is (4, DC_i.shape[1]) (assuming samples are not equally distributed among all the classes which is true in 99% of cases...)\n",
    "    #So for nc I just have to take DC_i.shape[1] for each i in DC\n",
    "    nc = [DC_i.shape[1] for DC_i in DC]\n",
    "\n",
    "    #Compute the mean as done before with PCA\n",
    "    mu = D.mean(axis=1)\n",
    "    mu = mu.reshape((mu.shape[0], 1))\n",
    "\n",
    "    #Now compute the mean for each class\n",
    "    muC = [DC[label].mean(axis=1) for label, labelName in enumerate(uniqueLabels)]\n",
    "    muC = [mc.reshape((mc.shape[0], 1)) for mc in muC]\n",
    "\n",
    "    Sw = 0  #within  matrix initialization\n",
    "\n",
    "    #iterate over all the classes to execute the summations to calculate the Sw matrix\n",
    "    for label, labelName in enumerate(uniqueLabels):\n",
    "\n",
    "        #add up to the Sw (within) matrix\n",
    "        #for diff1 subtract the the class mean from the samples of each class, i.e center center the samples for each class \n",
    "        diff1 = DC[label] - muC[label]  #x_{c, i} - muC done by rows\n",
    "\n",
    "        #SHORTCUT: compute the Sw matrix as a weighted sum of the covariance matrices of each class\n",
    "        #so for each class:\n",
    "        #Compute the Covariance Matrix C using DC = D - mu\n",
    "        C_i = (diff1 @ diff1.T) / float(diff1.shape[1])  #Covariance matrix for class i\n",
    "\n",
    "        #weighted sum of all the C_i\n",
    "        Sw += nc[label] * C_i\n",
    "\n",
    "    \n",
    "    #at the end of the summations, just multiply by 1/N (N is the number of samples)\n",
    "    Sw = Sw / D.shape[1]\n",
    "\n",
    "    #return both matrices\n",
    "    return Sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tied covariance matrix shape: (4, 4)\n",
      "Tied covariance matrix:\n",
      "[[0.23637589 0.09525344 0.1364944  0.03614529]\n",
      " [0.09525344 0.11618517 0.05768855 0.0357726 ]\n",
      " [0.1364944  0.05768855 0.14992811 0.03746458]\n",
      " [0.03614529 0.0357726  0.03746458 0.04291763]]\n"
     ]
    }
   ],
   "source": [
    "#Compute the Sw matrix which is the single tied covariance matrix for all classes\n",
    "C_Tied = computeSw(DTR, LTR) #compute the Sw matrix which is the single tied covariance matrix for all classes\n",
    "print(f\"Tied covariance matrix shape: {C_Tied.shape}\")\n",
    "print(f\"Tied covariance matrix:\\n{C_Tied}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can observe that, given $\\Sigma^*_c$ which is the ML solution for class $c$ for the MVG Model, we can compute the tied Covariance Matrix as:\n",
    "$$\n",
    "\\Sigma^* = \\frac{1}{N} \\sum_c N_c \\Sigma^*_c\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeParams_ML_TiedCov(D, labels, useLDAForTiedCov=False):\n",
    "    #Compute the ML (Maximum Likelihood) parameters of the MVG Tied Covariance model\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - D: the data matrix of shape (numFeatures, numSamples)\n",
    "    - labels: the labels of the data, so a list of length numSamples\n",
    "    - useLDAForTiedCov: if True, compute the covariance matrix using the LDA method, else compute the covariance matrix summing all the Covariance of each class * Nc and dividing by N\n",
    "\n",
    "    Returned Values:\n",
    "    params:\n",
    "    - CTied: the tied covariance matrix of shape (numFeatures, numFeatures) which is the same for all classes\n",
    "    - mu: the mean vectors of shape (numFeatures, numClasses) where each column is the mean vector of the class c\n",
    "    \"\"\"\n",
    "\n",
    "    params = []\n",
    "    numClasses = np.unique(labels).shape[0] #number of classes\n",
    "\n",
    "    if (useLDAForTiedCov):\n",
    "        #compute the covariance matrix using the LDA method\n",
    "        Sw = computeSw(D, labels)\n",
    "        for label in range(numClasses):\n",
    "            #compute MLE meanst of each class i\n",
    "            mu, _ = compute_mu_C(D[:, labels == label])\n",
    "            params.append((mu, Sw))\n",
    "\n",
    "        return params\n",
    "\n",
    "    else:\n",
    "        CTied = 0                               #initialize the tied covariance matrix\n",
    "        muVect = []                        #initialize the mean vectors list\n",
    "        for label in range(numClasses):\n",
    "            #compute MLE estimates of mean and covariance matrix for each class i\n",
    "            D_c = D[:, labels == label]\n",
    "            Nc = D_c.shape[1]                   #Nc is the number of samples of class c\n",
    "            mu, C = compute_mu_C(D_c)\n",
    "            muVect.append(mu)\n",
    "            CTied += Nc * C\n",
    "\n",
    "        #at the end do: CTied / N\n",
    "        CTied = CTied / D.shape[1]              #N = D.shape[1] is the number of samples\n",
    "\n",
    "        #put everything in the params list\n",
    "        for label in range(numClasses):\n",
    "            params.append((muVect[label], CTied))\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tied covariance matrix shape: (4, 4)\n",
      "Tied covariance matrix:\n",
      "[[0.23637589 0.09525344 0.1364944  0.03614529]\n",
      " [0.09525344 0.11618517 0.05768855 0.0357726 ]\n",
      " [0.1364944  0.05768855 0.14992811 0.03746458]\n",
      " [0.03614529 0.0357726  0.03746458 0.04291763]]\n"
     ]
    }
   ],
   "source": [
    "#Compute CTied and muVect for the training data usign MLE\n",
    "TiedCov_Params = computeParams_ML_TiedCov(DTR, LTR) #compute the CTied and muVect for the training data usign MLE\n",
    "CTied = TiedCov_Params[0][1]                        #get the tied covariance matrix f\n",
    "print(f\"Tied covariance matrix shape: {C_Tied.shape}\")\n",
    "print(f\"Tied covariance matrix:\\n{C_Tied}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeParams_ML_TiedCov(DTR, LTR, useLDAForTiedCov=True)[0][1] == computeParams_ML_TiedCov(DTR, LTR)[0][1] #check if the covariance matrix is the same for both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "S_logLikelihoods_TiedCov = scoreMatrix_Pdf_GAU(DVAL, TiedCov_Params, useLog=True) #compute the log-likelihoods of the data given the MLE parameters of the MVG distribution with tied covariance matrix\n",
    "print(f\"log Score matrix shape: {S_logLikelihoods_NB.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_TiedCov = computeSJoint(S_logLikelihoods_TiedCov, np.ones((3, )) / 3., useLog=True) #compute the joint densities by multiplying the score matrix S with the Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SJoint_log_TiedCov_Sol = np.load(\"./solutions/logSJoint_TiedMVG.npy\")\n",
    "np.allclose(SJoint_log_TiedCov, SJoint_log_TiedCov_Sol) #check if the log joint densities are equal to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#calculate log S post\n",
    "SPost_log_TiedCov = computePosteriors(SJoint_log_TiedCov, useLog=True) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"log Posteriors shape: {SPost_log_TiedCov.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPost_log_TiedCov_Sol = np.load(\"./solutions/logPosterior_TiedMVG.npy\")\n",
    "np.allclose(SPost_log_TiedCov, SPost_log_TiedCov_Sol) #check if the log posteriors are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Classification rule*:\n",
    "$\n",
    "c_{t}^{*} = argmax_{c} \\log P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 1 1 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "#Select for each sample the class with the highest posterior probability\n",
    "PVAL_TiedCov = np.argmax(SPost_log_TiedCov_Sol, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "print(f\"Predictions shape: {PVAL_TiedCov.shape}\")\n",
    "print(f\"Predictions: {PVAL_TiedCov}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error calculation for the Tied Cov. MVG ggm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong predictions: 1\n",
      "Error Rate: 2.00%\n"
     ]
    }
   ],
   "source": [
    "error_count_TiedCov = np.count_nonzero(PVAL_TiedCov != LVAL)\n",
    "print(f\"Number of wrong predictions: {error_count_TiedCov}\")\n",
    "error_rate_TiedCov = np.mean(PVAL_TiedCov != LVAL)\n",
    "print(f\"Error Rate: {error_rate_TiedCov:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the **lowest** Error Rate among all the three models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Precision, Recall, F-1 Score for the Tied Cov. MVG gmm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        19\n",
      "           1      1.000     0.941     0.970        17\n",
      "           2      0.933     1.000     0.966        14\n",
      "\n",
      "    accuracy                          0.980        50\n",
      "   macro avg      0.978     0.980     0.978        50\n",
      "weighted avg      0.981     0.980     0.980        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(LVAL, PVAL_TiedCov, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate MVG: 4.00%\n",
      "Error Rate Naive Bayes: 4.00%\n",
      "Error Rate Tied Covariance MVG: 2.00%\n"
     ]
    }
   ],
   "source": [
    "#print all the 3 error rates\n",
    "print(f\"Error Rate MVG: {error_rate_MVG:.2%}\")\n",
    "print(f\"Error Rate Naive Bayes: {error_rate_NB:.2%}\")\n",
    "print(f\"Error Rate Tied Covariance MVG: {error_rate_TiedCov:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_rates = [\n",
    "    (\"MVG\", error_rate_MVG), \n",
    "    (\"Naive Bayes\", error_rate_NB),\n",
    "    (\"Tied Covariance MVG\", error_rate_TiedCov)\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 2))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = [[name, f\"{error:.2%}\"] for name, error in error_rates]\n",
    "table = ax.table(cellText=table_data, colLabels=[\"Classifier\", \"Error Rate\"], loc=\"center\")\n",
    "\n",
    "#center text in the cells\n",
    "for (i, j), cell in table.get_celld().items():\n",
    "    if i == 0:\n",
    "        cell.set_text_props(weight='bold')\n",
    "    else:\n",
    "        cell.set_text_props(ha='center', va='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(14)\n",
    "table.auto_set_column_width([0, 1])\n",
    "table.scale(1.5, 1.5)  # Adjust the scale for better visibility\n",
    "plt.title(\"Error Rates of Generative Gaussian Classifiers on Iris Dataset\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary tasks: log-likelihood ratios and MVG\n",
    "We now focus on the same binary task we employed for LDA (see Laboratory 3), which requries classifying only two kinds of flowers, iris versicolor and iris virginica.\n",
    "So, the first thing to to is to extract just the samples of classes iris-versicolor and iris-virginica from the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (4, 66)\n",
      "Training labels shape:  (66,)\n",
      "Evaluation data shape:  (4, 34)\n",
      "Evaluation labels shape:  (34,)\n"
     ]
    }
   ],
   "source": [
    "D, L = loadDataSet('iris.csv', numFeatures)\n",
    "D_not0 = D[:, L != 0]                                                   #remove the samples of class 0\n",
    "L_not0 = L[L != 0]                                                      #remove the labels of class 0\n",
    "(DTR, LTR), (DVAL, LVAL) = splitTrainingValidation(2/3, D_not0, L_not0) #split the dataset into training and validation sets\n",
    "\n",
    "print(\"Training data shape: \", DTR.shape)\n",
    "print(\"Training labels shape: \", LTR.shape)\n",
    "print(\"Evaluation data shape: \", DVAL.shape)\n",
    "print(\"Evaluation labels shape: \", LVAL.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign to the test sample $x_{t}$ the class having the highest Posterior probability, comparing $P (C_{t} = 2 \\mid \\mathbf{X}_t = \\mathbf{x}_t)$ and $P (C_{t} = 1 \\mid \\mathbf{X}_t = \\mathbf{x}_t)$, assuming class $2$ is the virginica class and the *true* class, whereas class $1$ is the versicolor class and the *false* class (i.e. we are testing the hypothesis\n",
    "that a flower is from the virginica class). <br>\n",
    "We can jump into the *log-domain* and express the comparison in\n",
    "terms of **class posterior log-ratio**, which is the log of the ratio between the two Posteriors:\n",
    "$$\n",
    "\\log \\text{r}(x_{t}) = \\log \\frac{P (C_{t} = 2 \\mid \\mathbf{X}_t = \\mathbf{x}_t)}{P (C_{t} = 1 \\mid \\mathbf{X}_t = \\mathbf{x}_t)}\n",
    "$$\n",
    "In the log domain this is the sum of two terms:\n",
    "$$\n",
    "\\log \\text{r}(x_{t}) = \\log \\frac{f_{X|C}(x_t | 2)}{f_{X|C}(x_t | 1)} +\n",
    "\\log \\frac{P(C= 2)}{P(C=1)}\n",
    "$$\n",
    "So, we can see that $\\log \\text{r}(x_{t})$ depends on the log of the ratio between the likelihoods of the two classes, plus the log of the priors of the two classes. <br>\n",
    "Since the priors do not depend on the dataset, our system should just focus on providing the first term\n",
    "which is the log of the ratio of the likelihoods, so the conditionals, of the two classes, also called **log-likelihood ratio**:\n",
    "$$\n",
    "s(x_t) = \\text{llr}(x_t) = \\log \\frac{f_{X|C}(x_t | 2)}{f_{X|C}(x_t | 1)} = \\log f_{X|C}(x_t | 2) - \\log f_{X|C}(x_t | 1) = \\log \\mathcal{N}(x_t | \\mu_2, \\Sigma_2) - \\log \\mathcal{N}(x_t | \\mu_1, \\Sigma_1)\n",
    "$$\n",
    "Then the classification rule is simple: we compare the $\\text{llr}(x_{t})$ to the **prior log-odds** $ \\left( \\log \\frac{P(C= 2)}{P(C=1)} \\right)$, which can be considered a selected *threshold* depending on the application. This generates a decision surfaces used to separate and classify our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, given the labeled dataset, the parameters of the Gaussians are estimated using the Maximum Likelihood (ML) approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_1:\n",
      "[[5.95882353]\n",
      " [2.78529412]\n",
      " [4.32647059]\n",
      " [1.33823529]]\n",
      "Shape: (4, 1)\n",
      "mu_2:\n",
      "[[6.5875  ]\n",
      " [2.95625 ]\n",
      " [5.571875]\n",
      " [2.053125]]\n",
      "Shape: (4, 1)\n",
      "C_1:\n",
      "[[0.30948097 0.10851211 0.21991349 0.07186851]\n",
      " [0.10851211 0.10831315 0.09627163 0.04173875]\n",
      " [0.21991349 0.09627163 0.25018166 0.0807526 ]\n",
      " [0.07186851 0.04173875 0.0807526  0.04000865]]\n",
      "Shape: (4, 4)\n",
      "C_2:\n",
      "[[0.41296875 0.06226562 0.29058594 0.04691406]\n",
      " [0.06226562 0.06308594 0.04564453 0.03201172]\n",
      " [0.29058594 0.04564453 0.28764648 0.04868164]\n",
      " [0.04691406 0.03201172 0.04868164 0.05874023]]\n",
      "Shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "#Compute the MLE estimators for class 1 and class 2\n",
    "mu_1, C_1, = compute_mu_C(DTR[:, LTR == 1])\n",
    "mu_2, C_2, = compute_mu_C(DTR[:, LTR == 2])\n",
    "print(f\"mu_1:\\n{mu_1}\\nShape: {mu_1.shape}\")\n",
    "print(f\"mu_2:\\n{mu_2}\\nShape: {mu_2.shape}\")\n",
    "print(f\"C_1:\\n{C_1}\\nShape: {C_1.shape}\")\n",
    "print(f\"C_2:\\n{C_2}\\nShape: {C_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loglikelihood_1 Shape: (34,)\n",
      "loglikelihood_2 Shape: (34,)\n"
     ]
    }
   ],
   "source": [
    "#Then we can compute the log-likelihoods for class 1 and class 2 given the MLE parameters of the MVG distribution\n",
    "#Beware: we use the VALIDATION data to compute the log-likelihoods, not the training data!\n",
    "loglikelihood_1 = logpdf_GAU_ND(DVAL, mu_1, C_1)\n",
    "loglikelihood_2 = logpdf_GAU_ND(DVAL, mu_2, C_2)\n",
    "print(f\"loglikelihood_1 Shape: {loglikelihood_1.shape}\")\n",
    "print(f\"loglikelihood_2 Shape: {loglikelihood_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llr Shape: (34,)\n"
     ]
    }
   ],
   "source": [
    "#Then we compute the 1-D array of the log-likelihood ratios\n",
    "#In the log-domain, we just need to subtract the log-likelihoods of the two classes\n",
    "llr_MVG = loglikelihood_2 - loglikelihood_1\n",
    "print(f\"llr Shape: {llr_MVG.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llr_MVG_Sol = np.load(\"./solutions/llr_MVG.npy\")\n",
    "#Check if the log-likelihood ratios are equal to the solution\n",
    "np.allclose(llr_MVG, llr_MVG_Sol) #check if the log-likelihood ratios are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the threshold, given by the **prior log-odds** $ \\left( \\log \\frac{P(C= 2)}{P(C=1)} \\right)$, since we are treating a binary problem, we can rewrite it as follows:\n",
    "$$\n",
    "\\text{Treshold} = \\log \\frac{P(C= 2)}{P(C=1)} = \\log \\frac{\\pi}{1- \\pi}\n",
    "$$\n",
    "where $\\pi$ is the prior, application dependent, probability of observing class $2$. <br>\n",
    "In our case we can assume **uniform** priors so: $\\pi = P(C=2) = 1- \\pi = P(C=1) = \\frac{1}{2}$. <br>\n",
    "Given this assumption, we can rewrite the threshold as:\n",
    "$$\n",
    "\\text{Treshold} = \\log \\frac{\\pi}{1- \\pi} = \\log \\frac{1/2}{1-1/2} = 0\n",
    "$$\n",
    "To summarize, the classification rule is: \n",
    "$$\n",
    "\\text{llr}(x_{t}) = \\log \\mathcal{N}(x_t | \\mu_2, \\Sigma_2) - \\log \\mathcal{N}(x_t | \\mu_1, \\Sigma_1) \\gtrless 0\n",
    "$$\n",
    "The predictions are thus obtained by assigning label $2$ to samples whose log-likelihood ratio is greater or equal to 0,\n",
    "and label $1$ to the other samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (34,)\n",
      "Predictions: [1 2 1 2 1 1 1 2 2 2 2 2 2 2 1 1 1 1 1 2 2 1 2 2 2 1 1 1 1 2 2 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Classification rule: assign samples whose llr_MVG >= 0 to class 2 and samples whose llr_MVG < 0 to class 1\n",
    "PVAL_bin_MVG = np.where(llr_MVG >= 0, 2, 1) #assign samples whose llr_MVG >= 0 to class 2 and samples whose llr_MVG < 0 to class 1\n",
    "print(f\"Predictions shape: {PVAL_bin_MVG.shape}\")\n",
    "print(f\"Predictions: {PVAL_bin_MVG}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error Rate for Binary MVG Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong predictions: 2\n",
      "Error Rate: 8.82%\n"
     ]
    }
   ],
   "source": [
    "error_count_bin_MVG = np.count_nonzero(PVAL_bin_MVG != LVAL)\n",
    "print(f\"Number of wrong predictions: {error_count_MVG}\")\n",
    "error_rate_bin_MVG = np.mean(PVAL_bin_MVG != LVAL)\n",
    "print(f\"Error Rate: {error_rate_bin_MVG:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
