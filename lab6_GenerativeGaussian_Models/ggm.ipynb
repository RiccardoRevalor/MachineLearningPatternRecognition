{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Gaussian Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use again the *iris* dataset, and solve the iris classification prolem using Gaussian classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from load_dataset import loadDataSet                                #for loading the dataset\n",
    "from train_validation_split import splitTrainingValidation          #for splitting the dataset into training and validation sets\n",
    "from mean_covariance import vcol, vrow, compute_mu_C              #for computing the empirical mean and the empirical covariance of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (4, 150)\n",
      "Labels shape:  (150,)\n"
     ]
    }
   ],
   "source": [
    "numFeatures = 4\n",
    "\n",
    "#load the iris dataset\n",
    "D, L = loadDataSet('iris.csv', numFeatures)\n",
    "print(\"Data shape: \", D.shape)\n",
    "print(\"Labels shape: \", L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (4, 100)\n",
      "Training labels shape:  (100,)\n",
      "Evaluation data shape:  (4, 50)\n",
      "Evaluation labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset into training and validation sets\n",
    "#DTR and LTR are training data and labels, DTE and LTE are evaluation (or more precisely validation) data and labels\n",
    "(DTR, LTR), (DVAL, LVAL) = splitTrainingValidation(2/3, D, L)\n",
    "print(\"Training data shape: \", DTR.shape)\n",
    "print(\"Training labels shape: \", LTR.shape)\n",
    "print(\"Evaluation data shape: \", DVAL.shape)\n",
    "print(\"Evaluation labels shape: \", LVAL.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 100 samples for training and 50 samples for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian Classifier\n",
    "The optimal Bayes decision is to select for each test point the class with highest **posterior probability**: having class $c$ and $x_{t}$ as test point, we can thus write:\n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) \\rightarrow We \\space assign \\space x_{t} \\space to \\space the \\space class \\space having \\space the \\space highest \\space Posterior \\space probability\n",
    "$$ \n",
    "We will assume that the samples are independent and identically distributed (*i.i.d.*) according to $(\\mathbf{X}_t, C_{t}) ∼ (\\mathbf{X}, C)$. <br>\n",
    "Let $f_{X,C}$ be the joint density of $X, C$: we can\n",
    "compute the joint likelihood for the hypothesized class $c$ for the observed test\n",
    "sample $x_{t}$ as $f_{X,C}(x_{t}, c)$ and then use **Bayes rule** to compute the class posterior\n",
    "probability:\n",
    "$$\n",
    "P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')}\n",
    "$$\n",
    "We can factorize the joint density as:\n",
    "$$\n",
    "f_{\\mathbf{X}_t, C_t}(\\mathbf{x}_t, c) = f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c) P(c)\n",
    "$$\n",
    "Where:\n",
    "- $f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c)$ is the conditional distribution \n",
    "- $P(c)$ is called *Prior probabilty*: it's application-dependent and describes the probability of the class being $c$ **before** we observe $x_{t}$ \n",
    "\n",
    "In this specific case, we assume that our data, given the class, can be described by a **Gaussian distribution**:\n",
    "$$\n",
    "(\\mathbf{X}_t \\mid C_{t} = c) ∼ (\\mathbf{X} \\mid C = c) ∼ \\mathcal{N}(\\mathbf{µ_{c}}, \\mathbf{Σ_{c}})\n",
    "$$\n",
    "If we knew $\\mathbf{µ_{c}}$, $\\mathbf{Σ_{c}}$ then we could compute the conditional this way;\n",
    "$$\n",
    "f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c) = \\mathcal{N}(\\mathbf{µ_{c}}, \\mathbf{Σ_{c}})\n",
    "$$\n",
    "The problem is that we don't have **these parameters** $\\theta = [(\\mathbf{µ_{1}}, \\mathbf{Σ_{1}}), . . . ,(\\mathbf{µ_{k}}, \\mathbf{Σ_{k}})] $, where $k$ is the number of different classes. <br>\n",
    "However, since we have at our disposal a *labeled Dataset*, we can assume:\n",
    "- Gaussian distribution for $\\mathbf{X} \\mid C$\n",
    "- That, given the model parameters $\\theta$, all the samples observations are *i.i.d* \n",
    "\n",
    "After (and only after) making these assumptions, we can plug in the **Maximum Likelihood Estimators** (*MLE*), which, for a **MVG** distribution, are the empirical mean and covariance matrix of each class:\n",
    "$$\n",
    "\\mu^{MLE}_{c} = \\frac{1}{N_c} \\sum_{i} x_{c,i}, \\quad \n",
    "\\Sigma^{MLE}_{c} = \\frac{1}{N_c} \\sum_{i} (x_{c,i} - \\mu^*_c)(x_{c,i} - \\mu^*_c)^T\n",
    "$$\n",
    "Where $x_{c,i}$ is the $i$-th sample of class $c$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_0:\n",
      "[[4.96129032]\n",
      " [3.42903226]\n",
      " [1.46451613]\n",
      " [0.2483871 ]]\n",
      "Shape: (4, 1)\n",
      "mu_1:\n",
      "[[5.91212121]\n",
      " [2.78484848]\n",
      " [4.27272727]\n",
      " [1.33939394]]\n",
      "Shape: (4, 1)\n",
      "mu_2:\n",
      "[[6.45555556]\n",
      " [2.92777778]\n",
      " [5.41944444]\n",
      " [1.98888889]]\n",
      "Shape: (4, 1)\n",
      "C_0:\n",
      "[[0.13140479 0.11370447 0.02862643 0.01187305]\n",
      " [0.11370447 0.16270552 0.01844953 0.01117586]\n",
      " [0.02862643 0.01844953 0.03583767 0.00526535]\n",
      " [0.01187305 0.01117586 0.00526535 0.0108845 ]]\n",
      "Shape: (4, 4)\n",
      "C_1:\n",
      "[[0.26470156 0.09169881 0.18366391 0.05134068]\n",
      " [0.09169881 0.10613407 0.08898072 0.04211203]\n",
      " [0.18366391 0.08898072 0.21955923 0.06289256]\n",
      " [0.05134068 0.04211203 0.06289256 0.03208448]]\n",
      "Shape: (4, 4)\n",
      "C_2:\n",
      "[[0.30080247 0.08262346 0.18614198 0.04311728]\n",
      " [0.08262346 0.08533951 0.06279321 0.05114198]\n",
      " [0.18614198 0.06279321 0.18434414 0.04188272]\n",
      " [0.04311728 0.05114198 0.04188272 0.0804321 ]]\n",
      "Shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "#Compute the MLE estimators of a MVG distribtion, which are the empirical mean and covariance of the training data\n",
    "mu_0, C_0, = compute_mu_C(DTR[:, LTR == 0])\n",
    "mu_1, C_1, = compute_mu_C(DTR[:, LTR == 1])\n",
    "mu_2, C_2, = compute_mu_C(DTR[:, LTR == 2])\n",
    "\n",
    "print(f\"mu_0:\\n{mu_0}\\nShape: {mu_0.shape}\")\n",
    "print(f\"mu_1:\\n{mu_1}\\nShape: {mu_1.shape}\")\n",
    "print(f\"mu_2:\\n{mu_2}\\nShape: {mu_2.shape}\")\n",
    "print(f\"C_0:\\n{C_0}\\nShape: {C_0.shape}\")\n",
    "print(f\"C_1:\\n{C_1}\\nShape: {C_1.shape}\")\n",
    "print(f\"C_2:\\n{C_2}\\nShape: {C_2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the estimated model, we now turn our attention towards inference for a test sample $x$. As we\n",
    "have seen, the final goal is to compute class posterior probabilities $P(c \\mid \\mathbf{x})$. We split the process in three\n",
    "stages:\n",
    "\n",
    "*Stage 1*: For each sample we compute the likelihoods, so the class conditional probabilities as:\n",
    "$$\n",
    "f_{X|C} (x_t | c) = \\mathcal{N} (x_t | \\mu^*_c, \\Sigma^*_c)\n",
    "$$\n",
    "\n",
    "**Beware**: model params were estimated using the *training samples*, whereas densities are computed using *estimation samples*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logpdf_0 Shape: (50,)\n",
      "logpdf_1 Shape: (50,)\n",
      "logpdf_2 Shape: (50,)\n"
     ]
    }
   ],
   "source": [
    "from logpdf_loglikelihood_GAU import logpdf_GAU_ND\n",
    "\n",
    "#For each class Compute the log-pdf of the training data given the MLE parameters of the MVG distribution\n",
    "#It's better to compute the log-pdf and not the pdf, because the pdf can be very small and can cause numerical problems (underflow)\n",
    "#Then the logpdf gets exponentiated and the numerical problems are avoided\n",
    "logpdf_0 = logpdf_GAU_ND(DVAL, mu_0, C_0)\n",
    "logpdf_1 = logpdf_GAU_ND(DVAL, mu_1, C_1)\n",
    "logpdf_2 = logpdf_GAU_ND(DVAL, mu_2, C_2)\n",
    "\n",
    "print(f\"logpdf_0 Shape: {logpdf_0.shape}\")\n",
    "print(f\"logpdf_1 Shape: {logpdf_1.shape}\")\n",
    "print(f\"logpdf_2 Shape: {logpdf_2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now in order to compute the pds I exponentiate the log-likelihoods\n",
    "pds_0 = np.exp(logpdf_0)\n",
    "pds_1 = np.exp(logpdf_1)\n",
    "pds_2 = np.exp(logpdf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood of class 0: 92.71859585946842\n",
      "Likelihood of class 1: 21.00076276268103\n",
      "Likelihood of class 2: 7.586133742614116\n"
     ]
    }
   ],
   "source": [
    "#Now i compute the likelihood of the training data given the MLE parameters of the MVG distribution\n",
    "#The likelihood is the sum of the pdfs of all classes\n",
    "likelihood_0 = np.sum(pds_0)\n",
    "likelihood_1 = np.sum(pds_1)\n",
    "likelihood_2 = np.sum(pds_2)\n",
    "print(f\"Likelihood of class 0: {likelihood_0}\")\n",
    "print(f\"Likelihood of class 1: {likelihood_1}\")\n",
    "print(f\"Likelihood of class 2: {likelihood_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreMatrix_LogPdf_GAU(D, params):\n",
    "    \"\"\"\n",
    "    Compute the log-Pdf of the data given the parameters of a Gaussian distribution\n",
    "    and populate the score matrix S with the log-pdf of each class\n",
    "    #The score matrix is filled with the log-pdfs of the training data given the MLE parameters of the MVG distribution\n",
    "    #S[i, j] is the log-pdf of the j-th sample given the i-th class\n",
    "\n",
    "    Parameters:\n",
    "    - D: the data matrix of shape (numFeatures, numSamples)\n",
    "    - params: the model parameters, so  list of tuples (mu, C) where mu is the mean vector fo class c and C is the covariance matrix of class c\n",
    "\n",
    "    Returned Values:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "\n",
    "    \"\"\"\n",
    "    numClasses = len(params) #number of classes, since for each class we have a tuple (mu, C)\n",
    "    S = np.zeros((numClasses, D.shape[1]))\n",
    "    for label in range(numClasses):\n",
    "        S[label, :] = np.exp(logpdf_GAU_ND(D, params[label][0], params[label][1]))\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#Compute score matrix S of log likelihoods for each sample and class\n",
    "S_logLikelihoods = scoreMatrix_LogPdf_GAU(DVAL, [(mu_0, C_0), (mu_1, C_1), (mu_2, C_2)])\n",
    "print(f\"Score matrix shape: {S_logLikelihoods.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 2*: We multiply the class conditional probabilities, computed before, with the class *Prior* probabilities. In\n",
    "the following we assume that the three classes have the same Prior probability $P(c) = 1/3$. We can thus\n",
    "compute the joint distribution for samples and classes as:\n",
    "$$\n",
    "f_{X,C}(x_t, c) = f_{X|C}(x_t | c) P_C(c)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSJoint(S, Priors):\n",
    "    \"\"\"\n",
    "    Compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    #The joint densities are the product of the score matrix S with the Priors\n",
    "\n",
    "    Parameters:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "    - Priors: the priors of the classes, so a list of length numClasses\n",
    "\n",
    "    Returned Values:\n",
    "    - SJoint: the joint densities of shape (numClasses, numSamples) where each row is the joint density of the class given the sample\n",
    "    \"\"\"\n",
    "    numClasses = len(Priors) #number of classes, since we have 1 prior for each class\n",
    "\n",
    "    for classIndex in range(numClasses):\n",
    "        #multiply each row of S (where 1 row corresponds to a class) with the prior of the class\n",
    "        S[classIndex, :] *= Priors[classIndex]\n",
    "\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint densities shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "SJoint_MVG = computeSJoint(S_logLikelihoods, [1/3, 1/3, 1/3])\n",
    "print(f\"Joint densities shape: {SJoint_MVG.shape}\")\n",
    "\n",
    "SJoint_MVG_Sol = np.load(\"./solutions/SJoint_MVG.npy\")\n",
    "\n",
    "#Check if the joint densities are equal to the solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.58575943e+000, 1.04243514e+000, 2.72957564e-062,\n",
       "        2.24806026e-182, 1.41705326e-209, 3.71159641e+000,\n",
       "        2.73554357e+000, 1.77783391e+000, 1.88494475e-075,\n",
       "        1.38281331e-073, 6.75473622e-001, 1.25923986e-001,\n",
       "        3.27963237e-067, 2.60765569e-001, 4.86691243e-246,\n",
       "        1.59659471e-034, 6.14615463e-184, 5.56896774e-073,\n",
       "        1.52018824e-001, 1.47475180e-106, 5.73724955e-003,\n",
       "        2.69557088e-213, 2.93005929e+000, 2.04486323e+000,\n",
       "        4.46535964e-173, 3.57224919e+000, 6.01202795e-177,\n",
       "        1.73618815e-061, 1.22661255e-052, 3.04373991e-037,\n",
       "        5.93105804e-156, 1.88023605e-152, 5.65586443e-100,\n",
       "        4.20695998e-108, 8.00340945e-001, 1.16366505e-043,\n",
       "        2.36030153e-176, 1.06685624e-127, 3.61775980e+000,\n",
       "        5.06643905e-078, 7.20278540e-061, 3.35571731e-106,\n",
       "        1.18718880e-076, 7.59266808e-002, 1.64162425e+000,\n",
       "        1.05273322e+000, 1.23817483e-141, 2.02173642e-053,\n",
       "        1.05499498e-212, 3.09755432e+000],\n",
       "       [6.08640958e-024, 1.95465160e-033, 2.91097165e-001,\n",
       "        8.76063229e-010, 6.16015123e-009, 4.52783275e-033,\n",
       "        2.95947302e-025, 1.25637525e-031, 5.11128161e-001,\n",
       "        4.20878657e-001, 3.87913841e-029, 2.73177670e-020,\n",
       "        6.92228972e-001, 1.68956457e-026, 1.38423481e-017,\n",
       "        1.53535581e-001, 5.03791878e-005, 2.07340928e-001,\n",
       "        2.29609794e-049, 8.33809600e-003, 7.27413641e-048,\n",
       "        5.63633508e-010, 4.89636004e-035, 5.55754061e-026,\n",
       "        2.09078036e-006, 2.75851333e-029, 6.94109798e-012,\n",
       "        1.77500223e+000, 1.17155473e+000, 9.73146906e-002,\n",
       "        5.58670076e-011, 2.88916748e-013, 5.91911897e-002,\n",
       "        9.44835444e-002, 1.59205283e-023, 3.22217561e-001,\n",
       "        7.31922850e-014, 6.04496336e-006, 1.19968749e-030,\n",
       "        4.00077791e-001, 6.58890169e-001, 1.49472377e-002,\n",
       "        8.94386146e-002, 1.83467991e-035, 2.59248167e-030,\n",
       "        2.73380835e-028, 5.69186492e-004, 3.19604324e-002,\n",
       "        7.93258201e-007, 8.89769339e-027],\n",
       "       [2.58576482e-044, 1.80925293e-057, 6.23416882e-004,\n",
       "        3.90924084e-001, 1.02135606e-002, 5.79282339e-056,\n",
       "        2.32858320e-045, 9.86805113e-055, 2.59010597e-003,\n",
       "        7.48370134e-006, 1.31191397e-053, 2.56520558e-042,\n",
       "        7.95902295e-006, 2.11272864e-051, 1.50502559e-006,\n",
       "        4.04850938e-008, 4.50554495e-004, 1.02252404e-006,\n",
       "        1.41602339e-070, 1.28492985e-001, 1.86920065e-077,\n",
       "        9.47486207e-004, 8.09701706e-059, 3.88058888e-044,\n",
       "        5.98414217e-002, 2.14428119e-051, 8.99802589e-002,\n",
       "        8.24370427e-005, 3.01937269e-006, 1.36694998e-009,\n",
       "        3.75054710e-001, 1.24172319e-001, 8.91055359e-002,\n",
       "        4.25103455e-002, 1.21320294e-042, 9.22767028e-007,\n",
       "        3.34642376e-001, 4.81209605e-001, 4.87671951e-052,\n",
       "        9.75564937e-005, 8.14180576e-006, 1.89768822e-002,\n",
       "        3.11804991e-004, 2.71100391e-061, 5.26310178e-055,\n",
       "        8.16321272e-047, 3.77745977e-001, 3.11154282e-006,\n",
       "        7.04617248e-004, 4.51183803e-047]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SJoint_MVG_Sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.58575943e+000, 1.04243514e+000, 2.72957564e-062,\n",
       "        2.24806026e-182, 1.41705326e-209, 3.71159641e+000,\n",
       "        2.73554357e+000, 1.77783391e+000, 1.88494475e-075,\n",
       "        1.38281331e-073, 6.75473622e-001, 1.25923986e-001,\n",
       "        3.27963237e-067, 2.60765569e-001, 4.86691243e-246,\n",
       "        1.59659471e-034, 6.14615463e-184, 5.56896774e-073,\n",
       "        1.52018824e-001, 1.47475180e-106, 5.73724955e-003,\n",
       "        2.69557088e-213, 2.93005929e+000, 2.04486323e+000,\n",
       "        4.46535964e-173, 3.57224919e+000, 6.01202795e-177,\n",
       "        1.73618815e-061, 1.22661255e-052, 3.04373991e-037,\n",
       "        5.93105804e-156, 1.88023605e-152, 5.65586443e-100,\n",
       "        4.20695998e-108, 8.00340945e-001, 1.16366505e-043,\n",
       "        2.36030153e-176, 1.06685624e-127, 3.61775980e+000,\n",
       "        5.06643905e-078, 7.20278540e-061, 3.35571731e-106,\n",
       "        1.18718880e-076, 7.59266808e-002, 1.64162425e+000,\n",
       "        1.05273322e+000, 1.23817483e-141, 2.02173642e-053,\n",
       "        1.05499498e-212, 3.09755432e+000],\n",
       "       [6.08640958e-024, 1.95465160e-033, 2.91097165e-001,\n",
       "        8.76063229e-010, 6.16015123e-009, 4.52783275e-033,\n",
       "        2.95947302e-025, 1.25637525e-031, 5.11128161e-001,\n",
       "        4.20878657e-001, 3.87913841e-029, 2.73177670e-020,\n",
       "        6.92228972e-001, 1.68956457e-026, 1.38423481e-017,\n",
       "        1.53535581e-001, 5.03791878e-005, 2.07340928e-001,\n",
       "        2.29609794e-049, 8.33809600e-003, 7.27413641e-048,\n",
       "        5.63633508e-010, 4.89636004e-035, 5.55754061e-026,\n",
       "        2.09078036e-006, 2.75851333e-029, 6.94109798e-012,\n",
       "        1.77500223e+000, 1.17155473e+000, 9.73146906e-002,\n",
       "        5.58670076e-011, 2.88916748e-013, 5.91911897e-002,\n",
       "        9.44835444e-002, 1.59205283e-023, 3.22217561e-001,\n",
       "        7.31922850e-014, 6.04496336e-006, 1.19968749e-030,\n",
       "        4.00077791e-001, 6.58890169e-001, 1.49472377e-002,\n",
       "        8.94386146e-002, 1.83467991e-035, 2.59248167e-030,\n",
       "        2.73380835e-028, 5.69186492e-004, 3.19604324e-002,\n",
       "        7.93258201e-007, 8.89769339e-027],\n",
       "       [2.58576482e-044, 1.80925293e-057, 6.23416882e-004,\n",
       "        3.90924084e-001, 1.02135606e-002, 5.79282339e-056,\n",
       "        2.32858320e-045, 9.86805113e-055, 2.59010597e-003,\n",
       "        7.48370134e-006, 1.31191397e-053, 2.56520558e-042,\n",
       "        7.95902295e-006, 2.11272864e-051, 1.50502559e-006,\n",
       "        4.04850938e-008, 4.50554495e-004, 1.02252404e-006,\n",
       "        1.41602339e-070, 1.28492985e-001, 1.86920065e-077,\n",
       "        9.47486207e-004, 8.09701706e-059, 3.88058888e-044,\n",
       "        5.98414217e-002, 2.14428119e-051, 8.99802589e-002,\n",
       "        8.24370427e-005, 3.01937269e-006, 1.36694998e-009,\n",
       "        3.75054710e-001, 1.24172319e-001, 8.91055359e-002,\n",
       "        4.25103455e-002, 1.21320294e-042, 9.22767028e-007,\n",
       "        3.34642376e-001, 4.81209605e-001, 4.87671951e-052,\n",
       "        9.75564937e-005, 8.14180576e-006, 1.89768822e-002,\n",
       "        3.11804991e-004, 2.71100391e-061, 5.26310178e-055,\n",
       "        8.16321272e-047, 3.77745977e-001, 3.11154282e-006,\n",
       "        7.04617248e-004, 4.51183803e-047]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SJoint_MVG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
