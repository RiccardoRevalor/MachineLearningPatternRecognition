{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Gaussian Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use again the *iris* dataset, and solve the iris classification prolem using Gaussian classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from load_dataset import loadDataSet                                #for loading the dataset\n",
    "from train_validation_split import splitTrainingValidation          #for splitting the dataset into training and validation sets\n",
    "from mean_covariance import vcol, vrow, compute_mu_C                #for computing the empirical mean and the empirical covariance of the dataset \n",
    "from logpdf_loglikelihood_GAU import logpdf_GAU_ND                  #for computing the log-likelihood of the Gaussian distribution\n",
    "from sklearn.metrics import classification_report                   #for generating the classification report of the models\n",
    "from scipy.special import logsumexp                                 #for scipy.special.logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (4, 150)\n",
      "Labels shape:  (150,)\n"
     ]
    }
   ],
   "source": [
    "numFeatures = 4\n",
    "\n",
    "#load the iris dataset\n",
    "D, L = loadDataSet('iris.csv', numFeatures)\n",
    "print(\"Data shape: \", D.shape)\n",
    "print(\"Labels shape: \", L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (4, 100)\n",
      "Training labels shape:  (100,)\n",
      "Evaluation data shape:  (4, 50)\n",
      "Evaluation labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset into training and validation sets\n",
    "#DTR and LTR are training data and labels, DTE and LTE are evaluation (or more precisely validation) data and labels\n",
    "(DTR, LTR), (DVAL, LVAL) = splitTrainingValidation(2/3, D, L)\n",
    "print(\"Training data shape: \", DTR.shape)\n",
    "print(\"Training labels shape: \", LTR.shape)\n",
    "print(\"Evaluation data shape: \", DVAL.shape)\n",
    "print(\"Evaluation labels shape: \", LVAL.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 100 samples for training and 50 samples for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian Classifier\n",
    "The optimal Bayes decision is to select for each test point the class with highest **posterior probability**: having class $c$ and $x_{t}$ as test point, we can thus write:\n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) \\rightarrow We \\space assign \\space x_{t} \\space to \\space the \\space class \\space having \\space the \\space highest \\space Posterior \\space probability\n",
    "$$ \n",
    "We will assume that the samples are independent and identically distributed (*i.i.d.*) according to $(\\mathbf{X}_t, C_{t}) ∼ (\\mathbf{X}, C)$. <br>\n",
    "Let $f_{X,C}$ be the joint density of $X, C$: we can\n",
    "compute the joint likelihood for the hypothesized class $c$ for the observed test\n",
    "sample $x_{t}$ as $f_{X,C}(x_{t}, c)$ and then use **Bayes rule** to compute the class posterior\n",
    "probability:\n",
    "$$\n",
    "P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')}\n",
    "$$\n",
    "We can factorize the joint density as:\n",
    "$$\n",
    "f_{\\mathbf{X}_t, C_t}(\\mathbf{x}_t, c) = f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c) P(c)\n",
    "$$\n",
    "Where:\n",
    "- $f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c)$ is the class conditional distribution \n",
    "- $P(c)$ is called *Prior probabilty*: it's application-dependent and describes the probability of the class being $c$ **before** we observe $x_{t}$ \n",
    "\n",
    "In this specific case, we assume that our data, given the class, can be described by a **Gaussian distribution**:\n",
    "$$\n",
    "(\\mathbf{X}_t \\mid C_{t} = c) ∼ (\\mathbf{X} \\mid C = c) ∼ \\mathcal{N}(\\mathbf{µ_{c}}, \\mathbf{Σ_{c}})\n",
    "$$\n",
    "If we knew $\\mathbf{µ_{c}}$, $\\mathbf{Σ_{c}}$ then we could compute the conditional this way;\n",
    "$$\n",
    "f_{\\mathbf{X} \\mid C}(\\mathbf{x}_t \\mid c) = \\mathcal{N}(\\mathbf{µ_{c}}, \\mathbf{Σ_{c}})\n",
    "$$\n",
    "The problem is that we don't have **these parameters** $\\theta = [(\\mathbf{µ_{1}}, \\mathbf{Σ_{1}}), . . . ,(\\mathbf{µ_{k}}, \\mathbf{Σ_{k}})] $, where $k$ is the number of different classes. <br>\n",
    "However, since we have at our disposal a *labeled Dataset*, we can assume:\n",
    "- Gaussian distribution for $\\mathbf{X} \\mid C$\n",
    "- That, given the model parameters $\\theta$, all the samples observations are *i.i.d* \n",
    "\n",
    "After (and only after) making these assumptions, we can plug in the **Maximum Likelihood Estimators** (*MLE*), which, for a **MVG** distribution, are the empirical mean and covariance matrix of each class. So, for each class $c$ we can compute the two estimators:\n",
    "$$\n",
    "\\mu^{MLE}_{c} = \\frac{1}{N_c} \\sum_{i} x_{c,i}, \\quad \n",
    "\\Sigma^{MLE}_{c} = \\frac{1}{N_c} \\sum_{i} (x_{c,i} - \\mu^*_c)(x_{c,i} - \\mu^*_c)^T\n",
    "$$\n",
    "Where $x_{c,i}$ is the $i$-th sample of class $c$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_0:\n",
      "[[4.96129032]\n",
      " [3.42903226]\n",
      " [1.46451613]\n",
      " [0.2483871 ]]\n",
      "Shape: (4, 1)\n",
      "mu_1:\n",
      "[[5.91212121]\n",
      " [2.78484848]\n",
      " [4.27272727]\n",
      " [1.33939394]]\n",
      "Shape: (4, 1)\n",
      "mu_2:\n",
      "[[6.45555556]\n",
      " [2.92777778]\n",
      " [5.41944444]\n",
      " [1.98888889]]\n",
      "Shape: (4, 1)\n",
      "C_0:\n",
      "[[0.13140479 0.11370447 0.02862643 0.01187305]\n",
      " [0.11370447 0.16270552 0.01844953 0.01117586]\n",
      " [0.02862643 0.01844953 0.03583767 0.00526535]\n",
      " [0.01187305 0.01117586 0.00526535 0.0108845 ]]\n",
      "Shape: (4, 4)\n",
      "C_1:\n",
      "[[0.26470156 0.09169881 0.18366391 0.05134068]\n",
      " [0.09169881 0.10613407 0.08898072 0.04211203]\n",
      " [0.18366391 0.08898072 0.21955923 0.06289256]\n",
      " [0.05134068 0.04211203 0.06289256 0.03208448]]\n",
      "Shape: (4, 4)\n",
      "C_2:\n",
      "[[0.30080247 0.08262346 0.18614198 0.04311728]\n",
      " [0.08262346 0.08533951 0.06279321 0.05114198]\n",
      " [0.18614198 0.06279321 0.18434414 0.04188272]\n",
      " [0.04311728 0.05114198 0.04188272 0.0804321 ]]\n",
      "Shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "#Compute the MLE estimators of a MVG distribtion, which are the empirical mean and covariance of the training data\n",
    "mu_0, C_0, = compute_mu_C(DTR[:, LTR == 0])\n",
    "mu_1, C_1, = compute_mu_C(DTR[:, LTR == 1])\n",
    "mu_2, C_2, = compute_mu_C(DTR[:, LTR == 2])\n",
    "\n",
    "print(f\"mu_0:\\n{mu_0}\\nShape: {mu_0.shape}\")\n",
    "print(f\"mu_1:\\n{mu_1}\\nShape: {mu_1.shape}\")\n",
    "print(f\"mu_2:\\n{mu_2}\\nShape: {mu_2.shape}\")\n",
    "print(f\"C_0:\\n{C_0}\\nShape: {C_0.shape}\")\n",
    "print(f\"C_1:\\n{C_1}\\nShape: {C_1.shape}\")\n",
    "print(f\"C_2:\\n{C_2}\\nShape: {C_2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the estimated model, we now turn our attention towards inference for a test sample $x$. As we\n",
    "have seen, the final goal is to compute class posterior probabilities $P(c \\mid \\mathbf{x})$. We split the process in three\n",
    "stages:\n",
    "\n",
    "*Stage 1*: For each sample we compute the likelihoods, so the class conditional probabilities as:\n",
    "$$\n",
    "f_{X|C} (x_t | c) = \\mathcal{N} (x_t | \\mu^{MLE}_c, \\Sigma^{MLE}_c)\n",
    "$$\n",
    "\n",
    "**Beware**: model params were estimated using the *training samples*, whereas densities are computed using *estimation samples*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logpdf_0 Shape: (50,)\n",
      "logpdf_1 Shape: (50,)\n",
      "logpdf_2 Shape: (50,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#For each class Compute the log-pdf of the training data given the MLE parameters of the MVG distribution\n",
    "#It's better to compute the log-pdf and not the pdf, because the pdf can be very small and can cause numerical problems (underflow)\n",
    "#Then the logpdf gets exponentiated and the numerical problems are avoided\n",
    "logpdf_0 = logpdf_GAU_ND(DVAL, mu_0, C_0)\n",
    "logpdf_1 = logpdf_GAU_ND(DVAL, mu_1, C_1)\n",
    "logpdf_2 = logpdf_GAU_ND(DVAL, mu_2, C_2)\n",
    "\n",
    "print(f\"logpdf_0 Shape: {logpdf_0.shape}\")\n",
    "print(f\"logpdf_1 Shape: {logpdf_1.shape}\")\n",
    "print(f\"logpdf_2 Shape: {logpdf_2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now in order to compute the pds I exponentiate the log-likelihoods\n",
    "pds_0 = np.exp(logpdf_0)\n",
    "pds_1 = np.exp(logpdf_1)\n",
    "pds_2 = np.exp(logpdf_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can automate the process and compute a *Score Matrix* having for each row i the conditional of class i and so $S[i, j]$ is the pdf of the j-th sample given the i-th class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreMatrix_Pdf_GAU(D, params):\n",
    "    \"\"\"\n",
    "    Compute the Pdf of the data given the parameters of a Gaussian distribution\n",
    "    and populate the score matrix S with the log-pdf of each class\n",
    "    #The score matrix is filled with the pdfs of the training data given the MLE parameters of the MVG distribution\n",
    "    #S[i, j] is the pdf of the j-th sample given the i-th class\n",
    "\n",
    "    Parameters:\n",
    "    - D: the data matrix of shape (numFeatures, numSamples)\n",
    "    - params: the model parameters, so  list of tuples (mu, C) where mu is the mean vector fo class c and C is the covariance matrix of class c\n",
    "\n",
    "    Returned Values:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "\n",
    "    \"\"\"\n",
    "    numClasses = len(params) #number of classes, since for each class we have a tuple (mu, C)\n",
    "\n",
    "    \n",
    "    S = np.zeros((numClasses, D.shape[1]))\n",
    "    for label in range(numClasses):\n",
    "        S[label, :] = np.exp(logpdf_GAU_ND(D, params[label][0], params[label][1]))\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#Compute score matrix S of log likelihoods for each sample and class\n",
    "S_Likelihoods = scoreMatrix_Pdf_GAU(DVAL, [(mu_0, C_0), (mu_1, C_1), (mu_2, C_2)])\n",
    "print(f\"Score matrix shape: {S_Likelihoods.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 2*: We multiply the class conditional probabilities, computed before, with the class *Prior* probabilities. In\n",
    "the following we assume that the three classes have the same Prior probability $P(c) = 1/3$. We can thus\n",
    "compute the joint distribution for samples and classes as:\n",
    "$$\n",
    "f_{X,C}(x_t, c) = f_{X|C}(x_t | c) P_C(c)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSJoint(S, Priors):\n",
    "    \"\"\"\n",
    "    Compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    #The joint densities are the product of the score matrix S with the Priors\n",
    "\n",
    "    Parameters:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "    - Priors: the priors of the classes, so a list of length numClasses\n",
    "\n",
    "    Returned Values:\n",
    "    - SJoint: the joint densities of shape (numClasses, numSamples) where each row is the joint density of the class given the sample\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #Old implementation of computeSJoint:\n",
    "\n",
    "\n",
    "    numClasses = len(Priors) #number of classes, since we have 1 prior for each class\n",
    "    newS = np.zeros((numClasses, S.shape[1])) #initialize newS with zeros\n",
    "\n",
    "    for classIndex in range(numClasses):\n",
    "        #multiply each row of S (where 1 row corresponds to a class) with the prior of the class\n",
    "        newS[classIndex, :] = S[classIndex, :] * Priors[classIndex]\n",
    "\n",
    "\n",
    "    return newS\n",
    "    \"\"\"\n",
    "\n",
    "    #S has shape: (numClasses, numSamples)\n",
    "    #Priors has shape: (numClasses, ) -> it's a row vector\n",
    "    #To correctly perform the multiplication, we need to transpose Priors to make it a column vector\n",
    "    return S * vcol(Priors) #multiply each row of S (where 1 row corresponds to a class) with the prior of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint densities shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "SJoint_MVG = computeSJoint(S_Likelihoods, np.ones((3, )) / 3.) #compute the joint densities by multiplying the score matrix S with the Priors\n",
    "print(f\"Joint densities shape: {SJoint_MVG.shape}\")\n",
    "\n",
    "SJoint_MVG_Sol = np.load(\"./solutions/SJoint_MVG.npy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if the joint densities are equal to the solution\n",
    "#Beware: the joint densities are not equal to the solution, but they are very close to the solution due to numerical problems\n",
    "np.allclose(SJoint_MVG, SJoint_MVG_Sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem stemming from this technique is that these calculations originate many numeric problems! That's why the expressions like: \n",
    "```python\n",
    "SJoint_MVG==SJoint_MVG_Sol\n",
    "```\n",
    "return False whereas expressions like:\n",
    "```python\n",
    "np.allclose(SJoint_MVG, SJoint_MVG_Sol)\n",
    "```\n",
    "return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 3*: Finally, we can compute the class Posteriors probabilities as:\n",
    "$$\n",
    "P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')}\n",
    "$$\n",
    "At the denominator we sum the joint probability over all classes to compute the marginal densities for each sample wich are $f_{\\mathbf{X}}(\\mathbf{x}_t)$ and have shape ```(1, DVAL.shape[1])```. The *axis_0* has shape equal to 1 since we sum over all the rows, corresponding to the joints for all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vrow(SJoint_MVG_Sol.sum(0)).shape #check if the first column of the joint densities are equal to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePosteriors(SJoint):\n",
    "    \"\"\"\n",
    "    Compute the posteriors by normalizing the joint densities\n",
    "    The posteriors are the joint densities divided by the sum of the joint densities which are the marginals\n",
    "\n",
    "    Parameters:\n",
    "    - SJoint: the joint densities of shape (numClasses, numSamples) where each row is the joint density of the class \n",
    "\n",
    "    Returned Values:\n",
    "    - SPost: the posteriors of shape (numClasses, numSamples) where each row is the posterior of the class given the sample\n",
    "    \"\"\"\n",
    "    #1. Compute marginals\n",
    "    SMarginal = vrow(SJoint.sum(0)) #sum over the rows (axis=0) to get the marginal of each sample\n",
    "\n",
    "    #2. Compute posteriors by dividing the joint densities by the marginals\n",
    "    SPost = SJoint / SMarginal #element wise division\n",
    "\n",
    "    return SPost\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "SPost_MVG = computePosteriors(SJoint_MVG) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"Posteriors shape: {SPost_MVG.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Rule**: As said before, the optimal Bayes decision is to select for each test sample the class with highest **posterior probability**: \n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 2 1 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "#Select for each sample the class with the highest posterior probability\n",
    "PVAL_MVG = np.argmax(SPost_MVG, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "print(f\"Predictions shape: {PVAL_MVG.shape}\")\n",
    "print(f\"Predictions: {PVAL_MVG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error calculation for the MVG ggm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong predictions: 2\n",
      "Error Rate: 4.00%\n"
     ]
    }
   ],
   "source": [
    "error_count_MVG = np.count_nonzero(PVAL_MVG != LVAL)\n",
    "print(f\"Number of wrong predictions: {error_count_MVG}\")\n",
    "error_rate_MVG = np.mean(PVAL_MVG != LVAL)\n",
    "print(f\"Error Rate: {error_rate_MVG:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Precision, Recall, F-1 Score for the MVG gmm model: <br>\n",
    "With 3 classes, accuracy is compute as:\n",
    "$$\n",
    "acc = \\frac{Correct \\space predictions}{Tot \\space samples} = \\frac{T0+T1+T2}{T0+T1+T2+F0+F1+F2} = 1 - Error \\space Rate\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        19\n",
      "           1      1.000     0.882     0.938        17\n",
      "           2      0.875     1.000     0.933        14\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.958     0.961     0.957        50\n",
      "weighted avg      0.965     0.960     0.960        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(LVAL, PVAL_MVG, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already discussed, working directly with densities is often problematic, due to numerical\n",
    "issues. It’s useful to implement the whole procedure directly in terms of log-densities (if we need, we can\n",
    "recover posterior probabilities at the end). <br>\n",
    "Working in the *log-domain*, the three stages for computing the class posterior probabilities $P(c \\mid \\mathbf{x})$ are: <br>\n",
    "*Stage 1*: For each sample we compute the log-likelihoods, so the class conditional log-probabilities as:\n",
    "$$\n",
    "\\log f_{X|C} (x_t | c) = \\log \\mathcal{N} (x_t | \\mu^{MLE}_c, \\Sigma^{MLE}_c)\n",
    "$$\n",
    "\n",
    "**Beware**: model params were estimated using the *training samples*, whereas densities are computed using *estimation samples*! <br>\n",
    "We can thus rewrite and extend the function `scoreMatrix_Pdf_GAU` written before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreMatrix_Pdf_GAU(D, params, useLog=False):  \n",
    "    #Compute the (log?)-Pdf of the data given the parameters of a Gaussian distribution and populate the score matrix S with the (log?)-pdf of each class\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - D: the data matrix of shape (numFeatures, numSamples)\n",
    "    - params: the model parameters, so  list of tuples (mu, C) where mu is the mean vector fo class c and C is the covariance matrix of class c\n",
    "    - useLog: if True, compute the log-pdf, else compute the pdf\n",
    "\n",
    "    Returned Values:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #The score matrix is filled with the pdfs of the training data given the MLE parameters of the MVG distribution\n",
    "    #S[i, j] is the pdf of the j-th sample given the i-th class\n",
    "    \n",
    "    numClasses = len(params) #number of classes, since for each class we have a tuple (mu, C)\n",
    "    S = np.zeros((numClasses, D.shape[1]))\n",
    "    for label in range(numClasses):\n",
    "        if useLog:\n",
    "            #if useLog is True, then compute the log-pdf\n",
    "            S[label, :] = logpdf_GAU_ND(D, params[label][0], params[label][1])\n",
    "        else:\n",
    "            #if useLog is False, then compute the pdf\n",
    "            S[label, :] = np.exp(logpdf_GAU_ND(D, params[label][0], params[label][1]))\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#Compute score matrix S of log likelihoods for each sample and class\n",
    "S_logLikelihoods = scoreMatrix_Pdf_GAU(DVAL, [(mu_0, C_0), (mu_1, C_1), (mu_2, C_2)], useLog=True)\n",
    "print(f\"log Score matrix shape: {S_logLikelihoods.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.exp(S_logLikelihoods), S_Likelihoods) #check if the log score matrix, upon exponentiation, is equal to the score matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 2*: We add the log class conditional probabilities, computed before, to the log of the class *Prior* probabilities. In\n",
    "the following we assume that the three classes have the same Prior probability $P(c) = 1/3$. We can thus\n",
    "compute the joint distribution for samples and classes in the *log-domain* as:\n",
    "$$\n",
    "l_{c} = \\log f_{X,C}(x_t, c) = \\log \\left( f_{X|C}(x_t | c) P_C(c) \\right) = \\log f_{X|C}(x_t | c) + \\log P_C(c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSJoint(S, Priors, useLog=False):\n",
    "    # Compute the joint densities by multiplying the score matrix S with the Priors\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "    - Priors: the priors of the classes, so a list of length numClasses\n",
    "    - useLog: if True, compute the log-joint densities, else compute the joint densities\n",
    "\n",
    "    Returned Values:\n",
    "    - SJoint: the (log?)joint densities of shape (numClasses, numSamples) where each row is the joint density of the class given the sample\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if (useLog):\n",
    "        #S needs to be already in log scale, so we just need to add the log of the priors\n",
    "        return S + vcol(np.log(Priors)) #multiply each row of S (where 1 row corresponds to a class) with the prior of the class\n",
    "    else:\n",
    "        return S * vcol(Priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_MVG = computeSJoint(S_logLikelihoods, np.ones((3, )) / 3., useLog=True) #compute the joint densities by multiplying the score matrix S with the Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_MVG_Sol = np.load(\"./solutions/logSJoint_MVG.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.exp(SJoint_log_MVG), SJoint_MVG) #check if the log joint densities, upon exponentiation, are equal to the joint densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 3*: Finally, we can compute the log class Posteriors probabilities as:\n",
    "$$\n",
    "\\log P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')} \\right) = \n",
    "\\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{f_{\\mathbf{X}}(\\mathbf{x}_t)} \\right) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log  f_{\\mathbf{X}}(\\mathbf{x}_t) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log \\sum_{c} e^{l_{c}}\n",
    "$$ \n",
    "where $l_{c}$ are all the log-joints. <br>\n",
    "However, we need to take care that computing the exponential terms may result again in numerical\n",
    "errors. A robust method to comute $\\log \\sum_{c} e^{l_{c}}$ is to rewrite it as:\n",
    "$$\n",
    "\\log \\sum_{c} e^{l_{c}} = l + \\log \\sum_{c} e^{l_{c} - l}\n",
    "$$\n",
    "where $l$ is the highest of the log-joints: $l = max_{c} {l_{c}}$\n",
    "This is known as the *log-sum-exp* trick, and is already implemented in *scipy* as `scipy.special.logsumexp`. We can thus use `scipy.special.logsumexp(s)`,\n",
    "where `s` is the array that contains the joint log-probabilities for a given sample, to compute the log-marginals $\\log f_X(x_{t})$. <br>\n",
    "`scipy.special.logsumexp` also allows specifying an axis, thus we can directly compute the array of\n",
    "marginals for all samples directly from the matrix of joint log-densities as we did before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePosteriors(SJoint, useLog=False):\n",
    "    \"\"\"\n",
    "    Compute the posteriors by normalizing the joint densities\n",
    "    The posteriors are the joint densities divided by the sum of the joint densities which are the marginals\n",
    "\n",
    "    Parameters:\n",
    "    - SJoint: the joint densities of shape (numClasses, numSamples) where each row is the joint density of the class \n",
    "\n",
    "    Returned Values:\n",
    "    - SPost: the posteriors of shape (numClasses, numSamples) where each row is the posterior of the class given the sample\n",
    "    \"\"\"\n",
    "    if useLog:\n",
    "        #1. Compute marginals usign the logsumexp trick to minimize numerical problems\n",
    "        #logsumexp is a function that computes the log of the sum of exponentials of input elements\n",
    "        #It is more numerically stable than computing the sum of exponentials directly\n",
    "        #It computes log(exp(a) + exp(b)) in a numerically stable way\n",
    "\n",
    "        #sum over the rows (axis=0) to get the marginal of each sample\n",
    "        SMarginal = logsumexp(SJoint, axis=0)\n",
    "        #SMarginal has now shape = (numSamples, ) -> it's a row vector\n",
    "        #I need to make it of shape (1, numSamples) \n",
    "        SPost = SJoint - vrow(SMarginal) #element wise division in log scale, so I just need to subtract the marginals from the joint densities\n",
    "        \n",
    "\n",
    "    else:\n",
    "        \n",
    "        #1. Compute marginals\n",
    "        SMarginal = vrow(SJoint.sum(0)) #sum over the rows (axis=0) to get the marginal of each sample\n",
    "\n",
    "        #2. Compute posteriors by dividing the joint densities by the marginals\n",
    "        SPost = SJoint / SMarginal #element wise division\n",
    "\n",
    "    return SPost\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#calculate log S post\n",
    "SPost_log_MVG = computePosteriors(SJoint_log_MVG, useLog=True) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"log Posteriors shape: {SPost_log_MVG.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check that these posterios probabilities, unpon exponentiation, are the same as the one previosly computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPost_log_MVG_Sol = np.load(\"./solutions/logPosterior_MVG.npy\")\n",
    "#Check if the log posteriors are equal to the solution\n",
    "np.allclose(SPost_log_MVG, SPost_log_MVG_Sol) #check if the log posteriors are equal to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.exp(SPost_log_MVG), SPost_MVG) #check if the posteriors, upon exponentiation, are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Gaussian Classifier\n",
    "We now consider the Naive Bayes version of the classifier. As we have seen, the Naive Bayes version of\n",
    "the MVG is simply a Gaussian classifier where the **covariance matrices are diagonal**. The MLE solution\n",
    "for the mean parameters is the same. For the covariance matrices we diagonalize the MLE solution:\n",
    "$$\n",
    "\\Sigma_c^{MLE, \\space Naive Bayes} = \\text{diag}(\\Sigma_c^{MLE}) = \\text{diag} \\left[ \\frac{1}{N_c} \\sum_i (x_{c,i} - \\mu_c^{MLE})(x_{c,i} - \\mu_c^{MLE})^T \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu_0, mu_1, mu_2 are good\n",
    "# compute C_0, C_1, C_2 for Naive Bayes\n",
    "\n",
    "\"\"\"\n",
    "np.diag(np.array([1, 2, 3])) gives as result:\n",
    "array([[1, 0, 0],\n",
    "       [0, 2, 0],\n",
    "       [0, 0, 3]])\n",
    "You have to repeat twice np.diag since theinner np.diag extracts the diagonal, the outer np.diag creates a diagonal matrix from the vector\n",
    "\"\"\"\n",
    "C_0_NB = np.diag(np.diag(C_0))\n",
    "C_1_NB = np.diag(np.diag(C_1))\n",
    "C_2_NB = np.diag(np.diag(C_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the procedure is exactly the **same** as with the MVG model: <br>\n",
    "*It's always better to work in the log-domain*\n",
    "- We compute the log class Posteriors in three stages (here they're represented in the equation from the last to the first):\n",
    "$$\n",
    "\\log P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')} \\right) = \n",
    "\\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{f_{\\mathbf{X}}(\\mathbf{x}_t)} \\right) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log  f_{\\mathbf{X}}(\\mathbf{x}_t) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log \\sum_{c} e^{l_{c}}\n",
    "$$ \n",
    "- We apply the classification rule (i.e. we assign text sample $x_{t}$ to the class $c$ having the highest log Posterior):\n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} \\log P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = argmax_{c} P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 1*: compute log Class Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "S_logLikelihoods_NB = scoreMatrix_Pdf_GAU(DVAL, [(mu_0, C_0_NB), (mu_1, C_1_NB), (mu_2, C_2_NB)], useLog=True)\n",
    "print(f\"log Score matrix shape: {S_logLikelihoods_NB.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 2*: compute log Joint Score Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_NB = computeSJoint(S_logLikelihoods_NB, np.ones((3, )) / 3., useLog=True) #compute the joint densities by multiplying the score matrix S with the Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SJoint_log_NB_Sol = np.load(\"./solutions/logSJoint_NaiveBayes.npy\")\n",
    "np.allclose(SJoint_log_NB, SJoint_log_NB_Sol) #check if the log joint densities are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stage 3*: compute log Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#calculate log S post\n",
    "SPost_log_NB = computePosteriors(SJoint_log_NB, useLog=True) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"log Posteriors shape: {SPost_log_MVG.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPost_log_NB_Sol = np.load(\"./solutions/logPosterior_NaiveBayes.npy\")\n",
    "np.allclose(SPost_log_NB, SPost_log_NB_Sol) #check if the log posteriors are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Classification rule*:\n",
    "$\n",
    "c_{t}^{*} = argmax_{c} \\log P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 1 2 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "#Select for each sample the class with the highest posterior probability\n",
    "PVAL_NB = np.argmax(SPost_log_NB_Sol, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "print(f\"Predictions shape: {PVAL_NB.shape}\")\n",
    "print(f\"Predictions: {PVAL_NB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error calculation for the Naive Bayes ggm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong predictions: 2\n",
      "Error Rate: 4.00%\n"
     ]
    }
   ],
   "source": [
    "error_count_NB = np.count_nonzero(PVAL_NB != LVAL)\n",
    "print(f\"Number of wrong predictions: {error_count_NB}\")\n",
    "error_rate_NB = np.mean(PVAL_NB != LVAL)\n",
    "print(f\"Error Rate: {error_rate_NB:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Precision, Recall, F-1 Score for the Naive Bayes gmm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        19\n",
      "           1      1.000     0.882     0.938        17\n",
      "           2      0.875     1.000     0.933        14\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.958     0.961     0.957        50\n",
      "weighted avg      0.965     0.960     0.960        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(LVAL, PVAL_MVG, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tied Covariance Gaussian Classifier\n",
    "We now consider the Tied covariance version of the classifier, which assumes that the **covariance matrices of the different classes are tied**. This means that the noise is class-independent, so the\n",
    "distribution of class samples around the class mean is the same. <br>\n",
    "We have seen that the ML solution for the class means is again the same. The\n",
    "ML solution for the covariance matrix is given by the empirical within-class covariance matrix ($S_{w}$):\n",
    "$$\n",
    "\\Sigma^* = S_{w} = \\frac{1}{N} \\sum_c \\sum_i (x_{c,i} - \\mu_c^*) (x_{c,i} - \\mu_c^*)^T\n",
    "$$\n",
    "It's important, again, to underline te fact that $\\Sigma^* = S_{w}$ does **not** depend on the class $c$.\n",
    "Remember that we have already computed within-class covariance matrices when we implemented LDA, but beware: this **happens just using the Maximum Likelihood estimators**, with other techniques this is not the same. <br>\n",
    "This function below is taken from the LAB3 Code and modified to compute just $S_{w}$ and not $S_{b}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSw(D, L):\n",
    "    '''\n",
    "    Params:\n",
    "    - D: Dataset features matrix, not ceCntered\n",
    "    - L: Labels of the samples\n",
    "\n",
    "    Returned Values:\n",
    "    - Sw: Within-class scatter matrix\n",
    "    '''\n",
    "\n",
    "    #find the unique labels for each class\n",
    "    uniqueLabels = np.unique(L)\n",
    "\n",
    "    #nc in the formula is computed as the number of samples of class c\n",
    "    #separate data into classes\n",
    "    DC = [D[:, L == label] for label in uniqueLabels]  #DC[0] -> samples of class 0, DC[1] -> samples of class 1 etc...\n",
    "\n",
    "    #compute nc for each class\n",
    "    #each element in DC has a shape which is (4, DC_i.shape[1]) (assuming samples are not equally distributed among all the classes which is true in 99% of cases...)\n",
    "    #So for nc I just have to take DC_i.shape[1] for each i in DC\n",
    "    nc = [DC_i.shape[1] for DC_i in DC]\n",
    "\n",
    "    #Compute the mean as done before with PCA\n",
    "    mu = D.mean(axis=1)\n",
    "    mu = mu.reshape((mu.shape[0], 1))\n",
    "\n",
    "    #Now compute the mean for each class\n",
    "    muC = [DC[label].mean(axis=1) for label, labelName in enumerate(uniqueLabels)]\n",
    "    muC = [mc.reshape((mc.shape[0], 1)) for mc in muC]\n",
    "\n",
    "    Sw = 0  #within  matrix initialization\n",
    "\n",
    "    #iterate over all the classes to execute the summations to calculate the Sw matrix\n",
    "    for label, labelName in enumerate(uniqueLabels):\n",
    "\n",
    "        #add up to the Sw (within) matrix\n",
    "        #for diff1 subtract the the class mean from the samples of each class, i.e center center the samples for each class \n",
    "        diff1 = DC[label] - muC[label]  #x_{c, i} - muC done by rows\n",
    "\n",
    "        #SHORTCUT: compute the Sw matrix as a weighted sum of the covariance matrices of each class\n",
    "        #so for each class:\n",
    "        #Compute the Covariance Matrix C using DC = D - mu\n",
    "        C_i = (diff1 @ diff1.T) / float(diff1.shape[1])  #Covariance matrix for class i\n",
    "\n",
    "        #weighted sum of all the C_i\n",
    "        Sw += nc[label] * C_i\n",
    "\n",
    "    \n",
    "    #at the end of the summations, just multiply by 1/N (N is the number of samples)\n",
    "    Sw = Sw / D.shape[1]\n",
    "\n",
    "    #return both matrices\n",
    "    return Sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tied covariance matrix shape: (4, 4)\n",
      "Tied covariance matrix:\n",
      "[[0.23637589 0.09525344 0.1364944  0.03614529]\n",
      " [0.09525344 0.11618517 0.05768855 0.0357726 ]\n",
      " [0.1364944  0.05768855 0.14992811 0.03746458]\n",
      " [0.03614529 0.0357726  0.03746458 0.04291763]]\n"
     ]
    }
   ],
   "source": [
    "#Compute the Sw matrix which is the single tied covariance matrix for all classes\n",
    "C_Tied = computeSw(DTR, LTR) #compute the Sw matrix which is the single tied covariance matrix for all classes\n",
    "print(f\"Tied covariance matrix shape: {C_Tied.shape}\")\n",
    "print(f\"Tied covariance matrix:\\n{C_Tied}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can observe that, given $\\Sigma^*_c$ which is the ML solution for class $c$ for the MVG Model, we can compute the tied Covariance Matrix as:\n",
    "$$\n",
    "\\Sigma^* = \\frac{1}{N} \\sum_c N_c \\Sigma^*_c\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeParams_ML_TiedCov(D, labels, useLDAForTiedCov=False):\n",
    "    #Compute the ML (Maximum Likelihood) parameters of the MVG Tied Covariance model\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - D: the data matrix of shape (numFeatures, numSamples)\n",
    "    - labels: the labels of the data, so a list of length numSamples\n",
    "    - useLDAForTiedCov: if True, compute the covariance matrix using the LDA method, else compute the covariance matrix summing all the Covariance of each class * Nc and dividing by N\n",
    "\n",
    "    Returned Values:\n",
    "    params:\n",
    "    - CTied: the tied covariance matrix of shape (numFeatures, numFeatures) which is the same for all classes\n",
    "    - mu: the mean vectors of shape (numFeatures, numClasses) where each column is the mean vector of the class c\n",
    "    \"\"\"\n",
    "\n",
    "    params = []\n",
    "    numClasses = np.unique(labels).shape[0] #number of classes\n",
    "\n",
    "    if (useLDAForTiedCov):\n",
    "        #compute the covariance matrix using the LDA method\n",
    "        Sw = computeSw(D, labels)\n",
    "        for label in range(numClasses):\n",
    "            #compute MLE meanst of each class i\n",
    "            mu, _ = compute_mu_C(D[:, labels == label])\n",
    "            params.append((mu, Sw))\n",
    "\n",
    "        return params\n",
    "\n",
    "    else:\n",
    "        CTied = 0                               #initialize the tied covariance matrix\n",
    "        muVect = []                        #initialize the mean vectors list\n",
    "        for label in range(numClasses):\n",
    "            #compute MLE estimates of mean and covariance matrix for each class i\n",
    "            D_c = D[:, labels == label]\n",
    "            Nc = D_c.shape[1]                   #Nc is the number of samples of class c\n",
    "            mu, C = compute_mu_C(D_c)\n",
    "            muVect.append(mu)\n",
    "            CTied += Nc * C\n",
    "\n",
    "        #at the end do: CTied / N\n",
    "        CTied = CTied / D.shape[1]              #N = D.shape[1] is the number of samples\n",
    "\n",
    "        #put everything in the params list\n",
    "        for label in range(numClasses):\n",
    "            params.append((muVect[label], CTied))\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tied covariance matrix shape: (4, 4)\n",
      "Tied covariance matrix:\n",
      "[[0.23637589 0.09525344 0.1364944  0.03614529]\n",
      " [0.09525344 0.11618517 0.05768855 0.0357726 ]\n",
      " [0.1364944  0.05768855 0.14992811 0.03746458]\n",
      " [0.03614529 0.0357726  0.03746458 0.04291763]]\n"
     ]
    }
   ],
   "source": [
    "#Compute CTied and muVect for the training data usign MLE\n",
    "TiedCov_Params = computeParams_ML_TiedCov(DTR, LTR) #compute the CTied and muVect for the training data usign MLE\n",
    "CTied = TiedCov_Params[0][1]                        #get the tied covariance matrix f\n",
    "print(f\"Tied covariance matrix shape: {C_Tied.shape}\")\n",
    "print(f\"Tied covariance matrix:\\n{C_Tied}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeParams_ML_TiedCov(DTR, LTR, useLDAForTiedCov=True)[0][1] == computeParams_ML_TiedCov(DTR, LTR)[0][1] #check if the covariance matrix is the same for both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Score matrix shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "S_logLikelihoods_TiedCov = scoreMatrix_Pdf_GAU(DVAL, TiedCov_Params, useLog=True) #compute the log-likelihoods of the data given the MLE parameters of the MVG distribution with tied covariance matrix\n",
    "print(f\"log Score matrix shape: {S_logLikelihoods_NB.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "SJoint_log_TiedCov = computeSJoint(S_logLikelihoods_TiedCov, np.ones((3, )) / 3., useLog=True) #compute the joint densities by multiplying the score matrix S with the Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SJoint_log_TiedCov_Sol = np.load(\"./solutions/logSJoint_TiedMVG.npy\")\n",
    "np.allclose(SJoint_log_TiedCov, SJoint_log_TiedCov_Sol) #check if the log joint densities are equal to the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Posteriors shape: (3, 50)\n"
     ]
    }
   ],
   "source": [
    "#calculate log S post\n",
    "SPost_log_TiedCov = computePosteriors(SJoint_log_TiedCov, useLog=True) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"log Posteriors shape: {SPost_log_TiedCov.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPost_log_TiedCov_Sol = np.load(\"./solutions/logPosterior_TiedMVG.npy\")\n",
    "np.allclose(SPost_log_TiedCov, SPost_log_TiedCov_Sol) #check if the log posteriors are equal to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Classification rule*:\n",
    "$\n",
    "c_{t}^{*} = argmax_{c} \\log P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (50,)\n",
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 1 1 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "#Select for each sample the class with the highest posterior probability\n",
    "PVAL_TiedCov = np.argmax(SPost_log_TiedCov_Sol, axis=0) #select the class with the highest posterior probability for each sample, set axis=0 to select the class with the highest posterior probability for each sample\n",
    "print(f\"Predictions shape: {PVAL_TiedCov.shape}\")\n",
    "print(f\"Predictions: {PVAL_TiedCov}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error calculation for the Tied Cov. MVG ggm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wrong predictions: 1\n",
      "Error Rate: 2.00%\n"
     ]
    }
   ],
   "source": [
    "error_count_TiedCov = np.count_nonzero(PVAL_TiedCov != LVAL)\n",
    "print(f\"Number of wrong predictions: {error_count_TiedCov}\")\n",
    "error_rate_TiedCov = np.mean(PVAL_TiedCov != LVAL)\n",
    "print(f\"Error Rate: {error_rate_TiedCov:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the **lowest** Error Rate among all the three models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Precision, Recall, F-1 Score for the Tied Cov. MVG gmm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        19\n",
      "           1      1.000     0.941     0.970        17\n",
      "           2      0.933     1.000     0.966        14\n",
      "\n",
      "    accuracy                          0.980        50\n",
      "   macro avg      0.978     0.980     0.978        50\n",
      "weighted avg      0.981     0.980     0.980        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(LVAL, PVAL_TiedCov, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate MVG: 4.00%\n",
      "Error Rate Naive Bayes: 4.00%\n",
      "Error Rate Tied Covariance MVG: 2.00%\n"
     ]
    }
   ],
   "source": [
    "#print all the 3 error rates\n",
    "print(f\"Error Rate MVG: {error_rate_MVG:.2%}\")\n",
    "print(f\"Error Rate Naive Bayes: {error_rate_NB:.2%}\")\n",
    "print(f\"Error Rate Tied Covariance MVG: {error_rate_TiedCov:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_rates = [\n",
    "    (\"MVG\", error_rate_MVG), \n",
    "    (\"Naive Bayes\", error_rate_NB),\n",
    "    (\"Tied Covariance MVG\", error_rate_TiedCov)\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 2))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = [[name, f\"{error:.2%}\"] for name, error in error_rates]\n",
    "table = ax.table(cellText=table_data, colLabels=[\"Classifier\", \"Error Rate\"], loc=\"center\")\n",
    "\n",
    "#center text in the cells\n",
    "for (i, j), cell in table.get_celld().items():\n",
    "    if i == 0:\n",
    "        cell.set_text_props(weight='bold')\n",
    "    else:\n",
    "        cell.set_text_props(ha='center', va='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(14)\n",
    "table.auto_set_column_width([0, 1])\n",
    "table.scale(1.5, 1.5)  # Adjust the scale for better visibility\n",
    "plt.title(\"Error Rates of Generative Gaussian Classifiers on Iris Dataset\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
