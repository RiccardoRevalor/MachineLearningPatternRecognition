{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf8488a",
   "metadata": {},
   "source": [
    "# A multinomial model for text\n",
    "\n",
    "Dante’s “Divina Commedia” is a well known example of a poem in which the author uses different styles\n",
    "through the different parts of the work. The poem is divided in three Cantiche, respectively “Inferno”\n",
    "(Hell), “Purgatorio” (Purgatory) and “Paradiso” (Heaven). Each part is written using different linguistic\n",
    "styles, moving from less to more aulic as we progress from Hell towards Heaven. Each Cantica is divided\n",
    "in Canti, 34 for Inferno, and 33 for Purgatorio and Paradiso. Each Canto consists of a variable number\n",
    "of verses (115 to 160), organized in tercets. <br>\n",
    "In this laboratory we will use statistical methods to analyze how the stylistic differences can be exploited\n",
    "to understand the Cantica of a given tercet. In particular, we will build a multinomial word model for\n",
    "the three Canticas and classify tercets excerpts. To avoid biased results, the tercets used to train the\n",
    "model will be different from those we evaluate on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c2c20",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "The files data/inferno.txt, data/purgatorio.txt, data/paradiso.txt contain tercets for the\n",
    "three different parts. To simplifying parsing, the files are already organized so that each line corresponds\n",
    "to a tercet. We use $25%$ of the tercets as validation, and the remaining ones as\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d9854cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from load import load_data, split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd94be0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lInf - size (number of tercets): 1597\n",
      "lPur - size (number of tercets): 1608\n",
      "lPar - size (number of tercets): 1607\n"
     ]
    }
   ],
   "source": [
    "lInf, lPur, lPar = load_data()\n",
    "print(\"lInf - size (number of tercets):\", len(lInf))\n",
    "print(\"lPur - size (number of tercets):\", len(lPur))\n",
    "print(\"lPar - size (number of tercets):\", len(lPar)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd16c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lInf_train - size (number of tercets): 1197\n",
      "lInf_evaluation - size (number of tercets): 400\n",
      "lPur_train - size (number of tercets): 1206\n",
      "lPur_evaluation - size (number of tercets): 402\n",
      "lPar_train - size (number of tercets): 1205\n",
      "lPar_evaluation - size (number of tercets): 402\n"
     ]
    }
   ],
   "source": [
    "#Split the data: reserve 25% for validation, 75% for training\n",
    "lInf_train, lInf_evaluation = split_data(lInf, 4)\n",
    "lPur_train, lPur_evaluation = split_data(lPur, 4)\n",
    "lPar_train, lPar_evaluation = split_data(lPar, 4)\n",
    "\n",
    "print(\"lInf_train - size (number of tercets):\", len(lInf_train))\n",
    "print(\"lInf_evaluation - size (number of tercets):\", len(lInf_evaluation))\n",
    "print(\"lPur_train - size (number of tercets):\", len(lPur_train))\n",
    "print(\"lPur_evaluation - size (number of tercets):\", len(lPur_evaluation))\n",
    "print(\"lPar_train - size (number of tercets):\", len(lPar_train))\n",
    "print(\"lPar_evaluation - size (number of tercets):\", len(lPar_evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d1cd4",
   "metadata": {},
   "source": [
    "Now, we create a dictionary of all te tercets used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7f8085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTR = {\n",
    "    \"lInf\": lInf_train,\n",
    "    \"lPur\": lPur_train,\n",
    "    \"lPar\": lPar_train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ae49933",
   "metadata": {},
   "outputs": [],
   "source": [
    "DVAL = lInf_evaluation + lPur_evaluation + lPar_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ecf67d",
   "metadata": {},
   "source": [
    "## Model #1\n",
    "We can assume the document is composed of $N$ words (for ex: if the document is just a simple sentence like \"How are you\", we have $N = 3$). <br>\n",
    "More precisely, each word corresponds to a **token**: in the *NLP* field, a token is a single unit of text, and in the simplest case, the one considered here, a token is a word. <br>\n",
    "Moreover, we can abstract and consider that we have $N$ Random Variables $X_0, X_1, X_2, ... , X_n \\in D$ that describe the $N$ tokens of the document. $D$ is the *set* of all the possible *distinct* words in the document, has size equal to $M$, and is called *dictionary*: if for example the document is \"How are you, you are good\", $D$ contains just {\"How\", \"are\", \"you\", \"good\"} and $M = 4$. <br>\n",
    "Having said that, we can consider the whole document to be the realization (i.e. observation) of the $N$ R.V. $(X_1, ..., X_n)$. <br>\n",
    "We also consider all the tokens to be *i.i.d.*, so, for any pair of tokens $X_p$ and $X_z$, we have:\n",
    "$$\n",
    "P(X_p, X_z) = P(X_p) \\cdot P(X_z) \\quad \\text{for } p \\neq z\n",
    "$$\n",
    "\n",
    "We can model everything in terms of a **Categorical Distribution**: <br>\n",
    "Each $X_i$ (for $i = 1, \\ldots, N$) is a categorical random variable that takes values in the dictionary $D = \\{D_1, D_2, \\ldots, D_M\\}$ with probability:\n",
    "\n",
    "$$\n",
    "P(X_i = D_j) = \\pi_j \\quad \\text{for } j = 1, \\ldots, M\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\pi_j$ is the probability of observing the $j$-th word from the dictionary,\n",
    "- $\\pi_j \\geq 0$ for all $j$,\n",
    "- and $\\sum_{j=1}^{M} \\pi_j = 1$.\n",
    "\n",
    "The whole document can be seen as a sequence of $N$ independent draws from this categorical distribution. \n",
    "\n",
    "### Parameters estimation via Maximum Likelihood approach\n",
    "$\\Pi = (\\pi_1, \\pi_2, ..., \\pi_M)$ are the model parameters that we have to estimate in order to compute the likelihoods. <br>\n",
    "Recalling the **Categorical Distribution** formulas, we can say that each token $X_i$ is a categorical random variable with probability mass function:\n",
    "\n",
    "$$\n",
    "f_{X_i}(x) = P(X_i = x) = \\prod_{j=1}^{M} \\pi_j^{\\mathbb{I}[x = D_j]}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\pi_j$ is the probability of word $D_j$ in the dictionary,\n",
    "- $\\mathbb{I}[x = D_j]$ is the indicator function that is $1$ if $x = D_j$ and $0$ otherwise.\n",
    "\n",
    "If, for example, we have a document having Tokens: [\"dog\", \"cat\", \"dog\", \"cat\", \"dog\"] and dictionary $D$ = {dog, cat} (so, $N =5$, $M=2$), we would compute the following probability mass functions:\n",
    "$$\n",
    "f_{X_1}(\\text{\"dog\"}) = \\pi_{\"dog\"}^1 \\times \\pi_{\"cat\"}^0 = \\pi_{\"dog\"}\\\\\n",
    "f_{X_2}(\\text{\"cat\"}) = \\pi_{\"dog\"}^0 \\times \\pi_{\"cat\"}^1 = \\pi_{\"cat\"}\n",
    "$$\n",
    "\n",
    "As a result, we can then compute the **Likelihood function** as the product of all the probability mass functions for each one of the $N$ independent tokens in the document:\n",
    "$$\n",
    "\\mathcal{L} \\left( \\Pi \\right) = \\mathcal{L}(\\pi_1, \\pi_2, \\dots, \\pi_M) = P(X_1, X_2, \\dots, X_N) = \\prod_{i=1}^{N} \\prod_{j=1}^{M} \\pi_j^{\\mathbb{I}[X_i = D_j]}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "- $\\pi_j$ is the probability of observing the $j$-th word in the dictionary $D$,\n",
    "- $\\mathbb{I}[X_i = D_j]$ is the indicator function that is 1 if $X_i = D_j$ and 0 otherwise.\n",
    "\n",
    "The first product runs over all tokens $X_1, X_2, \\dots, X_N$, and the second product runs over all possible words in the dictionary $D_1, D_2, \\dots, D_M$. <br>\n",
    "Shifting to the *log-domain*, we can write:\n",
    "$$\n",
    "\\mathcal{l}(\\Pi) = \\log \\mathcal{L}(\\Pi) = \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\mathbb{I}[x_i = j] \\log \\pi_j = \\sum_{j=1}^{M} N_j \\log \\pi_j\n",
    "$$\n",
    "Where:\n",
    "- $\\mathbb{I}[x_i = j]$ is the indicator function (1 if $x_i = j$, 0 otherwise),\n",
    "- $N_j$ is the number of times word $j$ appears in the document.\n",
    "\n",
    "We have the constraint that the probabilities must sum to 1:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{M} \\pi_j = 1\n",
    "$$\n",
    "\n",
    "To solve the **Maximum Likelihood Estimation (MLE)** problem under this constraint, we use the **method of Lagrange multipliers**.\n",
    "\n",
    "We define the **Lagrangian function** $\\mathcal{l}(\\Pi, \\lambda)$ to include this contraint when maximizing the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{l}(\\Pi, \\lambda) = \\mathcal{l}(\\pi_1, \\dots, \\pi_M, \\lambda) = \\sum_{j=1}^{M} N_j \\log \\pi_j + \\lambda \\left(1 - \\sum_{j=1}^{M} \\pi_j \\right)\n",
    "$$\n",
    "\n",
    "Then, we compute the partial derivatives and set them to zero:\n",
    "\n",
    "For each $j = 1, \\dots, M$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{l}}{\\partial \\pi_j} = \\frac{N_j}{\\pi_j} - \\lambda = 0\n",
    "$$\n",
    "\n",
    "And for the constraint:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{l}}{\\partial \\lambda} = 1 - \\sum_{j=1}^{M} \\pi_j = 0\n",
    "$$\n",
    "\n",
    "Solving this system gives us the MLE estimates for $\\pi_j$. <br>\n",
    "The ML estimates of the model parameters can be obtained by maximizing the log-likelihood function for each *cantica* $c$:\n",
    "the ML estimate of the probability of word $j$ in cantica $c$ is given by:\n",
    "$$\n",
    "\\pi_{c,j} = \\frac{N_{c,j}}{N_c}\n",
    "$$\n",
    "Where:\n",
    "- $N_{c,j}$ is the number of occurrences of each word $j$ (belonging to $D$) in cantica $c$, \n",
    "- $N_c = \\sum_{j=1}^{M} N_{c,j}$ is the total number of tokens in cantica $c$.\n",
    "\n",
    "This means that:\n",
    "\n",
    "> The ML estimate $\\pi_{c,j}$ is simply the **relative frequency** of word $j$ in cantica $c$.\n",
    "\n",
    "> This is a fundamental result: under the assumption that words are *i.i.d.* samples from a categorical distribution, the MLE of each word's probability is **just the proportion of times the word appears in the document**. <br>\n",
    "\n",
    "So, the complete formula in the log-domain for the likelihood is:\n",
    "$$\n",
    "\\mathcal{l}(\\Pi) = \\log \\mathcal{L}(\\Pi) = \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\mathbb{I}[x_i = j] \\log \\pi_j = \\sum_{j=1}^{M} N_j \\log \\pi_j = \\sum_{j=1}^{M} N_j \\left[ \\log N_{c,j} - \\log N_c \\right]\n",
    "$$\n",
    "\n",
    "**NOTE**: for some classes word frequencies may be 0. Since the prediction is based on the logarithm of\n",
    "the frequencies, this implies that we will get $- \\infty $ values for samples that contain words that do not\n",
    "appear in one cantica, corresponding to a class-conditional probability (likelihood) of 0. This may lead\n",
    "to numerical issues, since we may have prediction likelihoods for different canticas that are 0 for all three\n",
    "classes (e.g., a tercet containg a word that does not appear in Inferno, a second word that does not\n",
    "appear in Purgatory and a third word that does not appear in Paradiso). <br>\n",
    "A solution is to introduce augmented counts, so instead of using $N_{c,j}$ we use its *smoothed* version:\n",
    "$$\n",
    "N_{bc,j} = N_{c,j} + \\varepsilon\n",
    "$$\n",
    "Where $\\varepsilon$ is a small positive value and it's considered an hyperparameter. <br>\n",
    "So the smoothed formula for the log-likelihood, the one that will be used in the code, is this:\n",
    "$$\n",
    "\\mathcal{l}(\\Pi) \\approx \\mathcal{l} (\\Pi, \\varepsilon) = \\sum_{j=1}^{M} N_j \\left[ \\log \\left( N_{c,j} + \\varepsilon \\right) - \\log N_c \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a015613",
   "metadata": {},
   "source": [
    "To compute the frequencies we first need to build the dictionary of all possible words. We consider only\n",
    "words that appear in the training set — if test or validation tercets contain unknown words we can\n",
    "simply discard them, as they would not, in any case, provide support for any of the three hypotheses\n",
    "under consideration. <br>\n",
    "The best way to do this is using a dictionary where the keys are the distinct words, and the values are occurrences of each word in the cantica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45663bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model1_buildDict(canticaTercets):\n",
    "    \"\"\"\n",
    "    This function builds a dictionary of words from the cantica tercets.\n",
    "    It returns a set of words.\n",
    "\n",
    "    Parameters:\n",
    "    canticaTercets (list): A list of tercets from a cantica.\n",
    "\n",
    "    Returns:\n",
    "    Dc: A dictionary of words with their frequencies.\n",
    "    Nc: the number of all the tokens (i.e. the sum of appearances of each word) appearing in the canticaTercets\n",
    "    \"\"\"\n",
    "\n",
    "    Dc = {}\n",
    "    Nc = 0\n",
    "\n",
    "    for tercet in canticaTercets:\n",
    "        words = tercet.split()\n",
    "        for word in words:\n",
    "            Nc += 1\n",
    "            if (word not in Dc.keys()):\n",
    "                Dc[word] = 1\n",
    "            else:\n",
    "                Dc[word] += 1\n",
    "\n",
    "\n",
    "    return Dc, Nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d64eb",
   "metadata": {},
   "source": [
    "To compute the model estimators using the *ML* approach, let's call with function which returns a dictionary where, for each cantica c, it contains another dictionary storing the relative frequency in the cantica for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e20de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model1_Estimators(DTR, eps = 0.001):\n",
    "    \"\"\"\n",
    "    This function computes, for each cantica, a dictionary storing the relative frequency for each word in the cantica, corresponding to the MLE.\n",
    "\n",
    "    Parameters:\n",
    "    - DTR: A dictionary of key=canticaLabel, value=list of tercets in the cantica\n",
    "    - eps: epsilon, so the value in order to smooth frequencies\n",
    "\n",
    "    Returned Values:\n",
    "    - MLE: log Maximum Likelihood Estimators for each word in each cantica c: keys: cantica labels, values: list where index i is the SMOOTHED log frequency of word i of cantica c\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    MLE = {}\n",
    "\n",
    "    globalDict = set() #global dict computed as the union between the cantica dicts keys sets\n",
    "    canticaTokenCounts = {} #dictionary storing each Nc (token count) for each cantica c\n",
    "    canticaDicts = {}  #dictionary storing canticaDict foe each cantica\n",
    "\n",
    "    #For each cantica c\n",
    "    #1. Build a dictionary of key: word, value: #times the word appears in the cantica c\n",
    "    #2. Find Nc which is the number of all tokens (i.e. the sum of appearances of each word) appearing in the canticaTercets\n",
    "    #3. The MLE are found by calculating the frequency of each word in the cantica c\n",
    "\n",
    "    #1-2. Dictionary building, Nc for each cantica\n",
    "    for canticaLabel in DTR.keys():\n",
    "        # canticaLabel can be lInf, lPur, lPar\n",
    "\n",
    "        canticaDict, Nc = Model1_buildDict(DTR[canticaLabel])\n",
    "        globalDict = globalDict.union(canticaDict.keys())\n",
    "        canticaTokenCounts[canticaLabel] = Nc\n",
    "        canticaDicts[canticaLabel] = canticaDict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #3. Compute smoothed MLE\n",
    "    #Smoothing: in order to not have frequency = zero for a certain word, set all the frequencies to eps, so augment the word count with pseudo-count\n",
    "    for canticaLabel in DTR.keys(): \n",
    "        MLE[canticaLabel] = {}\n",
    "        canticaDict = canticaDicts[canticaLabel]\n",
    "        Nc = canticaTokenCounts[canticaLabel]\n",
    "\n",
    "        for word in globalDict:\n",
    "            #if the word of the globalDict appears also in the canticaDict -> frequency is log{(Nc,j + eps) / Nc}\n",
    "            #otherwise, frequency is log{(0 + eps) / Nc} = log{eps / Nc}\n",
    "\n",
    "            #use dict.get(word, defaultNone) in order to return Nc,j = 0 if the word is NOT present in the canticaDict\n",
    "            #simply using canticaDict[word] would raise an exception in this case\n",
    "\n",
    "            N_cj = canticaDict.get(word, 0)\n",
    "\n",
    "            MLE[canticaLabel][word] = np.log(N_cj + eps) - np.log(Nc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return MLE, globalDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf1569",
   "metadata": {},
   "source": [
    "Here are the MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acb2fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE, globalDict = Model1_Estimators(DTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db808d19",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ritrovai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mMLE\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlPar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mritrovai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ritrovai'"
     ]
    }
   ],
   "source": [
    "MLE[\"lPar\"][\"ritrovai\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0b085",
   "metadata": {},
   "source": [
    "### Predicting the cantica\n",
    "We now turn to prediction for a given tercet $t$. Again, we can represent the tercet in terms of its tokens\n",
    "$x = [x_1, x_2, ..., x_N ]$ (here $N$ refers to the length of the validation tercet, and $x_i$\n",
    "is the $i$-th word of the tercet). <br>\n",
    "As said before, the likelihood of a word $x$ is expressed as:\n",
    "$$\n",
    "P(x \\mid c) = P(x_1, x_2, \\dots, x_N \\mid c) = \\prod_{i=1}^{N} P(x_i \\mid c)\n",
    "$$\n",
    "In the *log-domain* we express it as:\n",
    "$$\n",
    "\\log P(x \\mid c) = \\sum_{i=1}^{N} \\log \\pi_{c, x_i}\n",
    "$$\n",
    "**So, in order to compute the log-likelihood of each word, we have to sum all the log-probabilities of each word.** <br>\n",
    "Let's consider a scenario where we are predicting the cantica for a tercet. Suppose we have the following words in the tercet: \n",
    "$$\n",
    "x = [x_1, x_2, x_3] = [\\text{fire}, \\text{burning}, \\text{night}]\n",
    "$$\n",
    "We also have the word probabilities (calculated from the MLE) for a specific cantica, say *Inferno*, stored in $\\pi_{\\text{Inferno}, x}$, which represents the probability of each word occurring in the *Inferno* cantica.\n",
    "\n",
    "For this example, suppose the probabilities for each word are as follows:\n",
    "- $P(\\text{fire} \\mid \\text{Inferno}) = 0.2$\n",
    "- $P(\\text{burning} \\mid \\text{Inferno}) = 0.3$\n",
    "- $P(\\text{night} \\mid \\text{Inferno}) = 0.1$\n",
    "\n",
    "We can calculate the log-likelihood for this tercet in the *Inferno* cantica as:\n",
    "$$\n",
    "\\log P(x \\mid \\text{Inferno}) = \\log \\{P(\\text{fire} \\mid \\text{Inferno}) \\cdot P(\\text{burning} \\mid \\text{Inferno}) \\cdot P(\\text{night} \\mid \\text{Inferno})\\}\n",
    "$$\n",
    "Using the formula for the log of a product, we get:\n",
    "$$\n",
    "\\log P(x \\mid \\text{Inferno}) = \\log P(\\text{fire} \\mid \\text{Inferno}) + \\log P(\\text{burning} \\mid \\text{Inferno}) + \\log P(\\text{night} \\mid \\text{Inferno})\n",
    "$$\n",
    "Substituting the values, we have:\n",
    "$$\n",
    "\\log P(x \\mid \\text{Inferno}) = \\log 0.2 + \\log 0.3 + \\log 0.1\n",
    "$$\n",
    "This simplifies to:\n",
    "$$\n",
    "\\log P(x \\mid \\text{Inferno}) \\approx -1.6094 - 1.2040 - 2.3026 = -5.116\n",
    "$$\n",
    "\n",
    "Thus, the log-likelihood for the tercet $x = [\\text{fire}, \\text{burning}, \\text{night}]$ in the *Inferno* cantica is approximately $-5.116$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0fe81a",
   "metadata": {},
   "source": [
    "In pratice (i.e. in the code), we create and compute a **matrix of scores** $S$, where each row corresponds to a class and each column to a test sample (tercet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7819a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model1_compute_LogLikelihoods(logMLE, DVAL):\n",
    "    \"\"\"\n",
    "    Compute log-likelihoods for all the classes given a DVAL validation text\n",
    "\n",
    "    Parameters:\n",
    "    - logMLE: log estimators for each word witin each class\n",
    "    - DVAL: text on which ti compute the log-likelihoods\n",
    "\n",
    "    Returned Values:\n",
    "    - ll: loglikelihoods\n",
    "    \"\"\"\n",
    "\n",
    "    ll = {canticaLabel: 0 for canticaLabel in logMLE.keys()}\n",
    "\n",
    "    for canticaLabel in logMLE.keys():\n",
    "        logMLECanticaLabel = logMLE[canticaLabel]\n",
    "        for word in DVAL.split():\n",
    "            logProbWord = logMLECanticaLabel.get(word.strip(), 0.001)\n",
    "            ll[canticaLabel] += logProbWord\n",
    "\n",
    "\n",
    "    return ll   #3 values: {ll_lInf, ll_lPur, ll_lPar}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6fd6618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model1_matrixScore(logMLE, tercetsList):\n",
    "    \"\"\"\n",
    "    Compute a score matrix M having each row equal to each class (lInf, lPur, lPar) and each column to a test sample (tercet)\n",
    "    \"\"\"\n",
    "\n",
    "    numClasses = len(logMLE.keys())\n",
    "    S = np.zeros((numClasses, len(tercetsList)))\n",
    "\n",
    "    for tercetColumnCount, tercet in enumerate(tercetsList):\n",
    "        #tercetColumnCount is for inserting the computed likelihoods in the right trailing column\n",
    "        logLikelihoods = Model1_compute_LogLikelihoods(logMLE, tercet)\n",
    "\n",
    "        for row, canticaLabel in enumerate(logMLE.keys()):\n",
    "            S[row, tercetColumnCount] = logLikelihoods[canticaLabel]\n",
    "\n",
    "    return S\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f5e7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1_S = Model1_matrixScore(MLE, DVAL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
