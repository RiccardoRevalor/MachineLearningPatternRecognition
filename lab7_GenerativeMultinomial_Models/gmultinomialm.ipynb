{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf8488a",
   "metadata": {},
   "source": [
    "# A multinomial model for text\n",
    "\n",
    "Dante’s “Divina Commedia” is a well known example of a poem in which the author uses different styles\n",
    "through the different parts of the work. The poem is divided in three Cantiche, respectively “Inferno”\n",
    "(Hell), “Purgatorio” (Purgatory) and “Paradiso” (Heaven). Each part is written using different linguistic\n",
    "styles, moving from less to more aulic as we progress from Hell towards Heaven. Each Cantica is divided\n",
    "in Canti, 34 for Inferno, and 33 for Purgatorio and Paradiso. Each Canto consists of a variable number\n",
    "of verses (115 to 160), organized in tercets. <br>\n",
    "In this laboratory we will use statistical methods to analyze how the stylistic differences can be exploited\n",
    "to understand the Cantica of a given tercet. In particular, we will build a multinomial word model for\n",
    "the three Canticas and classify tercets excerpts. To avoid biased results, the tercets used to train the\n",
    "model will be different from those we evaluate on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c2c20",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "The files data/inferno.txt, data/purgatorio.txt, data/paradiso.txt contain tercets for the\n",
    "three different parts. To simplifying parsing, the files are already organized so that each line corresponds\n",
    "to a tercet. We use $25%$ of the tercets as validation, and the remaining ones as\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9854cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import load_data, split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd94be0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lInf - size (number of lines): 1597\n",
      "lPur - size (number of lines): 1608\n",
      "lPar - size (number of lines): 1607\n"
     ]
    }
   ],
   "source": [
    "lInf, lPur, lPar = load_data()\n",
    "print(\"lInf - size (number of lines):\", len(lInf))\n",
    "print(\"lPur - size (number of lines):\", len(lPur))\n",
    "print(\"lPar - size (number of lines):\", len(lPar)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd16c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lInf_train - size (number of lines): 1197\n",
      "lInf_evaluation - size (number of lines): 400\n",
      "lPur_train - size (number of lines): 1206\n",
      "lPur_evaluation - size (number of lines): 402\n",
      "lPar_train - size (number of lines): 1205\n",
      "lPar_evaluation - size (number of lines): 402\n"
     ]
    }
   ],
   "source": [
    "#Split the data: reserve 25% for validation, 75% for training\n",
    "lInf_train, lInf_evaluation = split_data(lInf, 4)\n",
    "lPur_train, lPur_evaluation = split_data(lPur, 4)\n",
    "lPar_train, lPar_evaluation = split_data(lPar, 4)\n",
    "\n",
    "print(\"lInf_train - size (number of lines):\", len(lInf_train))\n",
    "print(\"lInf_evaluation - size (number of lines):\", len(lInf_evaluation))\n",
    "print(\"lPur_train - size (number of lines):\", len(lPur_train))\n",
    "print(\"lPur_evaluation - size (number of lines):\", len(lPur_evaluation))\n",
    "print(\"lPar_train - size (number of lines):\", len(lPar_train))\n",
    "print(\"lPar_evaluation - size (number of lines):\", len(lPar_evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ecf67d",
   "metadata": {},
   "source": [
    "## Model #1\n",
    "We can assume the document is composed of $N$ words (for ex: if the document is just a simple sentence like \"How are you\", we have $N = 3$). <br>\n",
    "More precisely, each word corresponds to a **token**: in the *NLP* field, a token is a single unit of text, and in the simplest case, the one considered here, a token is a word. <br>\n",
    "Moreover, we can abstract and consider that we have $N$ Random Variables $X_0, X_1, X_2, ... , X_n \\in D$ that describe the $N$ tokens of the document. $D$ is the *set* of all the possible *distinct* words in the document, has size equal to $M$, and is called *dictionary*: if for example the document is \"How are you, you are good\", $D$ contains just {\"How\", \"are\", \"you\", \"good\"} and $M = 4$. <br>\n",
    "Having said that, we can consider the whole document to be the realization (i.e. observation) of the $N$ R.V. $(X_1, ..., X_n)$. <br>\n",
    "We also consider all the tokens to be *i.i.d.*, so, for any pair of tokens $X_p$ and $X_z$, we have:\n",
    "$$\n",
    "P(X_p, X_z) = P(X_p) \\cdot P(X_z) \\quad \\text{for } p \\neq z\n",
    "$$\n",
    "\n",
    "We can model everything in terms of a **Categorical Distribution**: <br>\n",
    "Each $X_i$ (for $i = 1, \\ldots, N$) is a categorical random variable that takes values in the dictionary $D = \\{D_1, D_2, \\ldots, D_M\\}$ with probability:\n",
    "\n",
    "$$\n",
    "P(X_i = D_j) = \\pi_j \\quad \\text{for } j = 1, \\ldots, M\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\pi_j$ is the probability of observing the $j$-th word from the dictionary,\n",
    "- $\\pi_j \\geq 0$ for all $j$,\n",
    "- and $\\sum_{j=1}^{M} \\pi_j = 1$.\n",
    "\n",
    "The whole document can be seen as a sequence of $N$ independent draws from this categorical distribution. <br>\n",
    "Recalling the **Categorical Distribution** formulas, we can say that each token $X_i$ is a categorical random variable with probability mass function:\n",
    "\n",
    "$$\n",
    "f_{X_i}(x) = P(X_i = x) = \\prod_{j=1}^{M} \\pi_j^{\\mathbb{I}[x = D_j]}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\pi_j$ is the probability of word $D_j$ in the dictionary,\n",
    "- $\\mathbb{I}[x = D_j]$ is the indicator function that is $1$ if $x = D_j$ and $0$ otherwise.\n",
    "\n",
    "If, for example, we have a document having Tokens: [\"dog\", \"cat\", \"dog\", \"cat\", \"dog\"] and dictionary $D$ = {dog, cat} (so, $N =5$, $M=2$), we would compute the following probability mass functions:\n",
    "$$\n",
    "f_{X_1}(\\text{\"dog\"}) = \\pi_{\"dog\"}^1 \\times \\pi_{\"cat\"}^0 = \\pi_{\"dog\"}\\\\\n",
    "f_{X_2}(\\text{\"cat\"}) = \\pi_{\"dog\"}^0 \\times \\pi_{\"cat\"}^1 = \\pi_{\"cat\"}\n",
    "$$\n",
    "\n",
    "As a result, we can then compute the **Likelihood function** as the product of all the probability mass functions for each one of the $N$ tokens in the document:\n",
    "$$\n",
    "\\mathcal{L} \\left( \\Pi \\right) = \\mathcal{L}(\\pi_1, \\pi_2, \\dots, \\pi_M) = P(X_1, X_2, \\dots, X_N) = \\prod_{i=1}^{N} \\prod_{j=1}^{M} \\pi_j^{\\mathbb{I}[X_i = D_j]}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "- \\( \\pi_j \\) is the probability of observing the \\( j \\)-th word in the dictionary \\( D \\),\n",
    "- \\( \\mathbb{I}[X_i = D_j] \\) is the indicator function that is 1 if \\( X_i = D_j \\) and 0 otherwise.\n",
    "\n",
    "The first product runs over all tokens \\( X_1, X_2, \\dots, X_N \\), and the second product runs over all possible words in the dictionary \\( D_1, D_2, \\dots, D_M \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f157df",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
