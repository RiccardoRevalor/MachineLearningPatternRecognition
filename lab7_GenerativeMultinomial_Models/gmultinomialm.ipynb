{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf8488a",
   "metadata": {},
   "source": [
    "# A multinomial model for text\n",
    "\n",
    "Dante’s “Divina Commedia” is a well known example of a poem in which the author uses different styles\n",
    "through the different parts of the work. The poem is divided in three Cantiche, respectively “Inferno”\n",
    "(Hell), “Purgatorio” (Purgatory) and “Paradiso” (Heaven). Each part is written using different linguistic\n",
    "styles, moving from less to more aulic as we progress from Hell towards Heaven. Each Cantica is divided\n",
    "in Canti, 34 for Inferno, and 33 for Purgatorio and Paradiso. Each Canto consists of a variable number\n",
    "of verses (115 to 160), organized in tercets. <br>\n",
    "In this laboratory we will use statistical methods to analyze how the stylistic differences can be exploited\n",
    "to understand the Cantica of a given tercet. In particular, we will build a multinomial word model for\n",
    "the three Canticas and classify tercets excerpts. To avoid biased results, the tercets used to train the\n",
    "model will be different from those we evaluate on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c2c20",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "The files data/inferno.txt, data/purgatorio.txt, data/paradiso.txt contain tercets for the\n",
    "three different parts. To simplifying parsing, the files are already organized so that each line corresponds\n",
    "to a tercet. We use $25%$ of the tercets as validation, and the remaining ones as\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9854cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from load import load_data, split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd94be0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lInf - size (number of tercets): 1597\n",
      "lPur - size (number of tercets): 1608\n",
      "lPar - size (number of tercets): 1607\n"
     ]
    }
   ],
   "source": [
    "lInf, lPur, lPar = load_data()\n",
    "print(\"lInf - size (number of tercets):\", len(lInf))\n",
    "print(\"lPur - size (number of tercets):\", len(lPur))\n",
    "print(\"lPar - size (number of tercets):\", len(lPar)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd16c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lInf_train - size (number of tercets): 1197\n",
      "lInf_evaluation - size (number of tercets): 400\n",
      "lPur_train - size (number of tercets): 1206\n",
      "lPur_evaluation - size (number of tercets): 402\n",
      "lPar_train - size (number of tercets): 1205\n",
      "lPar_evaluation - size (number of tercets): 402\n"
     ]
    }
   ],
   "source": [
    "#Split the data: reserve 25% for validation, 75% for training\n",
    "lInf_train, lInf_evaluation = split_data(lInf, 4)\n",
    "lPur_train, lPur_evaluation = split_data(lPur, 4)\n",
    "lPar_train, lPar_evaluation = split_data(lPar, 4)\n",
    "\n",
    "print(\"lInf_train - size (number of tercets):\", len(lInf_train))\n",
    "print(\"lInf_evaluation - size (number of tercets):\", len(lInf_evaluation))\n",
    "print(\"lPur_train - size (number of tercets):\", len(lPur_train))\n",
    "print(\"lPur_evaluation - size (number of tercets):\", len(lPur_evaluation))\n",
    "print(\"lPar_train - size (number of tercets):\", len(lPar_train))\n",
    "print(\"lPar_evaluation - size (number of tercets):\", len(lPar_evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d1cd4",
   "metadata": {},
   "source": [
    "Now, we create a dictionary of all te tercets used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f8085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTR = {\n",
    "    \"lInf\": lInf_train,\n",
    "    \"lPur\": lPur_train,\n",
    "    \"lPar\": lPar_train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c0e9d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Inferno tercets: 400\n",
      "Number of Purgatorio tercets: 402\n",
      "Number of Paradiso tercets: 402\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Inferno tercets: {len(lInf_evaluation)}\")\n",
    "print(f\"Number of Purgatorio tercets: {len(lPur_evaluation)}\")\n",
    "print(f\"Number of Paradiso tercets: {len(lPar_evaluation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae49933",
   "metadata": {},
   "outputs": [],
   "source": [
    "DVAL = lInf_evaluation + lPur_evaluation + lPar_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c1ae1",
   "metadata": {},
   "source": [
    "We build the true labels for each tercet, since we know that the first $400$ tercets belong to *Inferno*, the next $402$ to *Purgatorio* and the last $402$ to *Paradiso*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc689b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LVAL has shape: (1204,)\n"
     ]
    }
   ],
   "source": [
    "LVAL = [0] * len(lInf_evaluation) + [1] * len(lPur_evaluation) + [2] * len(lPar_evaluation)\n",
    "LVAL = np.array(LVAL)\n",
    "print(f\"LVAL has shape: {LVAL.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ecf67d",
   "metadata": {},
   "source": [
    "## Model #1\n",
    "We can assume the document is composed of $N$ words (for ex: if the document is just a simple sentence like \"How are you\", we have $N = 3$). <br>\n",
    "More precisely, each word corresponds to a **token**: in the *NLP* field, a token is a single unit of text, and in the simplest case, the one considered here, a token is a word. <br>\n",
    "Moreover, we can abstract and consider that we have $N$ Random Variables $X_0, X_1, X_2, ... , X_n \\in D$ that describe the $N$ tokens of the document. $D$ is the *set* of all the possible *distinct* words in the document, has size equal to $M$, and is called *dictionary*: if for example the document is \"How are you, you are good\", $D$ contains just {\"How\", \"are\", \"you\", \"good\"} and $M = 4$. <br>\n",
    "Having said that, we can consider the whole document to be the realization (i.e. observation) of the $N$ R.V. $(X_1, ..., X_n)$. <br>\n",
    "We also consider all the tokens to be *i.i.d.*, so, for any pair of tokens $X_p$ and $X_z$, we have:\n",
    "$$\n",
    "P(X_p, X_z) = P(X_p) \\cdot P(X_z) \\quad \\text{for } p \\neq z\n",
    "$$\n",
    "\n",
    "We can model everything in terms of a **Categorical Distribution**: <br>\n",
    "Each $X_i$ (for $i = 1, \\ldots, N$) is a categorical random variable that takes values in the dictionary $D = \\{D_1, D_2, \\ldots, D_M\\}$ with probability:\n",
    "\n",
    "$$\n",
    "P(X_i = D_j) = \\pi_j \\quad \\text{for } j = 1, \\ldots, M\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\pi_j$ is the probability of observing the $j$-th word from the dictionary,\n",
    "- $\\pi_j \\geq 0$ for all $j$,\n",
    "- and $\\sum_{j=1}^{M} \\pi_j = 1$.\n",
    "\n",
    "The whole document can be seen as a sequence of $N$ independent draws from this categorical distribution. \n",
    "\n",
    "### Parameters estimation via Maximum Likelihood approach\n",
    "$\\Pi = (\\pi_1, \\pi_2, ..., \\pi_M)$ are the model parameters that we have to estimate in order to compute the likelihoods. <br>\n",
    "Recalling the **Categorical Distribution** formulas, we can say that each token $X_i$ is a categorical random variable with probability mass function:\n",
    "\n",
    "$$\n",
    "f_{X_i}(x) = P(X_i = x) = \\prod_{j=1}^{M} \\pi_j^{\\mathbb{I}[x = D_j]}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\pi_j$ is the probability of word $D_j$ in the dictionary,\n",
    "- $\\mathbb{I}[x = D_j]$ is the indicator function that is $1$ if $x = D_j$ and $0$ otherwise.\n",
    "\n",
    "If, for example, we have a document having Tokens: [\"dog\", \"cat\", \"dog\", \"cat\", \"dog\"] and dictionary $D$ = {dog, cat} (so, $N =5$, $M=2$), we would compute the following probability mass functions:\n",
    "$$\n",
    "f_{X_1}(\\text{\"dog\"}) = \\pi_{\"dog\"}^1 \\times \\pi_{\"cat\"}^0 = \\pi_{\"dog\"}\\\\\n",
    "f_{X_2}(\\text{\"cat\"}) = \\pi_{\"dog\"}^0 \\times \\pi_{\"cat\"}^1 = \\pi_{\"cat\"}\n",
    "$$\n",
    "\n",
    "As a result, we can then compute the **Likelihood function** as the product of all the probability mass functions for each one of the $N$ independent tokens in the document:\n",
    "$$\n",
    "\\mathcal{L}_X \\left( \\Pi \\right) = \\mathcal{L}_X(\\pi_1, \\pi_2, \\dots, \\pi_M) = P(X_1, X_2, \\dots, X_N) = \\prod_{i=1}^{N} \\prod_{j=1}^{M} \\pi_j^{\\mathbb{I}[X_i = D_j]}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "- $\\pi_j$ is the probability of observing the $j$-th word in the dictionary $D$,\n",
    "- $\\mathbb{I}[X_i = D_j]$ is the indicator function that is 1 if $X_i = D_j$ and 0 otherwise.\n",
    "\n",
    "The first product runs over all tokens $X_1, X_2, \\dots, X_N$, and the second product runs over all possible words in the dictionary $D_1, D_2, \\dots, D_M$. <br>\n",
    "Shifting to the *log-domain*, we can write:\n",
    "$$\n",
    "\\ell_X(\\Pi) = \\log \\mathcal{L}_X(\\Pi) = \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\mathbb{I}[x_i = j] \\log \\pi_j = \\sum_{j=1}^{M} N_j \\log \\pi_j\n",
    "$$\n",
    "Where:\n",
    "- $\\mathbb{I}[x_i = j]$ is the indicator function (1 if $x_i = j$, 0 otherwise),\n",
    "- $N_j$ is the number of times word $j$ appears in the document.\n",
    "\n",
    "We have the constraint that the probabilities must sum to 1:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{M} \\pi_j = 1\n",
    "$$\n",
    "\n",
    "To solve the **Maximum Likelihood Estimation (MLE)** problem under this constraint, we use the **method of Lagrange multipliers**.\n",
    "\n",
    "We define the **Lagrangian function** $\\ell_X(\\Pi, \\lambda)$ to include this contraint when maximizing the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ell_X(\\Pi, \\lambda) = \\ell_X(\\pi_1, \\dots, \\pi_M, \\lambda) = \\sum_{j=1}^{M} N_j \\log \\pi_j + \\lambda \\left(1 - \\sum_{j=1}^{M} \\pi_j \\right)\n",
    "$$\n",
    "\n",
    "Then, we compute the partial derivatives and set them to zero:\n",
    "\n",
    "For each $j = 1, \\dots, M$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_X}{\\partial \\pi_j} = \\frac{N_j}{\\pi_j} - \\lambda = 0\n",
    "$$\n",
    "\n",
    "And for the constraint:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_X}{\\partial \\lambda} = 1 - \\sum_{j=1}^{M} \\pi_j = 0\n",
    "$$\n",
    "\n",
    "Solving this system gives us the MLE estimates for $\\pi_j$. <br>\n",
    "The ML estimates of the model parameters can be obtained by maximizing the log-likelihood function for each *cantica* $c$:\n",
    "the ML estimate of the probability of word $j$ in cantica $c$ is given by:\n",
    "$$\n",
    "\\pi_{c,j} = \\frac{N_{c,j}}{N_c}\n",
    "$$\n",
    "Where:\n",
    "- $N_{c,j}$ is the number of occurrences of each word $j$ (belonging to $D$) in cantica $c$, \n",
    "- $N_c = \\sum_{j=1}^{M} N_{c,j}$ is the total number of tokens in cantica $c$.\n",
    "\n",
    "This means that:\n",
    "\n",
    "> The ML estimate $\\pi_{c,j}$ is simply the **relative frequency** of word $j$ in cantica $c$.\n",
    "\n",
    "> This is a fundamental result: under the assumption that words are *i.i.d.* samples from a categorical distribution, the MLE of each word's probability is **just the proportion of times the word appears in the document**. <br>\n",
    "\n",
    "So, the complete formula in the log-domain for the likelihood is:\n",
    "$$\n",
    "\\ell_X(\\Pi) = \\log \\ell_X(\\Pi) = \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\mathbb{I}[x_i = j] \\log \\pi_j = \\sum_{j=1}^{M} N_j \\log \\pi_j = \\sum_{j=1}^{M} N_j \\left[ \\log N_{c,j} - \\log N_c \\right]\n",
    "$$\n",
    "\n",
    "**NOTE**: for some classes word frequencies may be 0. Since the prediction is based on the logarithm of\n",
    "the frequencies, this implies that we will get $- \\infty $ values for samples that contain words that do not\n",
    "appear in one cantica, corresponding to a class-conditional probability (likelihood) of 0. This may lead\n",
    "to numerical issues, since we may have prediction likelihoods for different canticas that are 0 for all three\n",
    "classes (e.g., a tercet containg a word that does not appear in Inferno, a second word that does not\n",
    "appear in Purgatory and a third word that does not appear in Paradiso). <br>\n",
    "A solution is to introduce augmented counts, so instead of using $N_{c,j}$ we use its *smoothed* version:\n",
    "$$\n",
    "N_{bc,j} = N_{c,j} + \\varepsilon\n",
    "$$\n",
    "Where $\\varepsilon$ is a small positive value and it's considered an hyperparameter. <br>\n",
    "So the smoothed formula for the log-likelihood, the one that will be used in the code, is this:\n",
    "$$\n",
    "\\ell_X(\\Pi) \\approx \\ell_X (\\Pi, \\varepsilon) = \\sum_{j=1}^{M} N_j \\left[ \\log \\left( N_{c,j} + \\varepsilon \\right) - \\log N_c \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a015613",
   "metadata": {},
   "source": [
    "To compute the frequencies we first need to build the dictionary of all possible words. We consider only\n",
    "words that appear in the training set — if test or validation tercets contain unknown words we can\n",
    "simply discard them, as they would not, in any case, provide support for any of the three hypotheses\n",
    "under consideration. <br>\n",
    "The best way to do this is using a dictionary where the keys are the distinct words, and the values are occurrences of each word in the cantica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f45663bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model1_buildDict(canticaTercets):\n",
    "    \"\"\"\n",
    "    This function builds a dictionary of words from the cantica tercets.\n",
    "    It returns a set of words.\n",
    "\n",
    "    Parameters:\n",
    "    canticaTercets (list): A list of tercets from a cantica.\n",
    "\n",
    "    Returns:\n",
    "    Dc: A dictionary of words with their frequencies.\n",
    "    Nc: the number of all the tokens (i.e. the sum of appearances of each word) appearing in the canticaTercets\n",
    "    \"\"\"\n",
    "\n",
    "    Dc = {}\n",
    "    Nc = 0\n",
    "\n",
    "    for tercet in canticaTercets:\n",
    "        words = tercet.split()\n",
    "        for word in words:\n",
    "            Nc += 1\n",
    "            if (word not in Dc.keys()):\n",
    "                Dc[word] = 1\n",
    "            else:\n",
    "                Dc[word] += 1\n",
    "\n",
    "\n",
    "    return Dc, Nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d64eb",
   "metadata": {},
   "source": [
    "To compute the model estimators using the *ML* approach, let's call with function which returns a dictionary where, for each cantica c, it contains another dictionary storing the relative frequency in the cantica for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e20de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model1_Estimators(DTR, eps = 0.001):\n",
    "    \"\"\"\n",
    "    This function computes, for each cantica, a dictionary storing the relative frequency for each word in the cantica, corresponding to the MLE.\n",
    "\n",
    "    Parameters:\n",
    "    - DTR: A dictionary of key=canticaLabel, value=list of tercets in the cantica\n",
    "    - eps: epsilon, so the value in order to smooth frequencies\n",
    "\n",
    "    Returned Values:\n",
    "    - MLE: log Maximum Likelihood Estimators for each word in each cantica c: keys: cantica labels, values: list where index i is the SMOOTHED log frequency of word i of cantica c\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    MLE = {}\n",
    "\n",
    "    globalDict = set() #global dict computed as the union between the cantica dicts keys sets\n",
    "    canticaTokenCounts = {} #dictionary storing each Nc (token count) for each cantica c\n",
    "    canticaDicts = {}  #dictionary storing canticaDict foe each cantica\n",
    "\n",
    "    #For each cantica c\n",
    "    #1. Build a dictionary of key: word, value: #times the word appears in the cantica c\n",
    "    #2. Find Nc which is the number of all tokens (i.e. the sum of appearances of each word) appearing in the canticaTercets\n",
    "    #3. The MLE are found by calculating the frequency of each word in the cantica c\n",
    "\n",
    "    #1-2. Dictionary building, Nc for each cantica\n",
    "    for canticaLabel in DTR.keys():\n",
    "        # canticaLabel can be lInf, lPur, lPar\n",
    "\n",
    "        canticaDict, Nc = Model1_buildDict(DTR[canticaLabel])\n",
    "        globalDict = globalDict.union(canticaDict.keys())\n",
    "        canticaTokenCounts[canticaLabel] = Nc\n",
    "        canticaDicts[canticaLabel] = canticaDict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #3. Compute smoothed MLE\n",
    "    #Smoothing: in order to not have frequency = zero for a certain word, set all the frequencies to eps, so augment the word count with pseudo-count\n",
    "    for canticaLabel in DTR.keys(): \n",
    "        MLE[canticaLabel] = {}\n",
    "        canticaDict = canticaDicts[canticaLabel]\n",
    "        Nc = canticaTokenCounts[canticaLabel]\n",
    "\n",
    "        for word in globalDict:\n",
    "            #if the word of the globalDict appears also in the canticaDict -> frequency is log{(Nc,j + eps) / Nc}\n",
    "            #otherwise, frequency is log{(0 + eps) / Nc} = log{eps / Nc}\n",
    "\n",
    "            #use dict.get(word, defaultNone) in order to return Nc,j = 0 if the word is NOT present in the canticaDict\n",
    "            #simply using canticaDict[word] would raise an exception in this case\n",
    "\n",
    "            N_cj = canticaDict.get(word, 0)\n",
    "\n",
    "            MLE[canticaLabel][word] = np.log(N_cj + eps) - np.log(Nc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return MLE, globalDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf1569",
   "metadata": {},
   "source": [
    "Here are the MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb2fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE, globalDict = Model1_Estimators(DTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0b085",
   "metadata": {},
   "source": [
    "### Predicting the cantica\n",
    "We now turn to prediction for a given tercet $t$. Again, we can represent the tercet in terms of its tokens\n",
    "$x = [x_1, x_2, ..., x_N ]$ (here $N$ refers to the length of the validation tercet, and $x_i$\n",
    "is the $i$-th word of the tercet). <br>\n",
    "As said before, the likelihood of a word $x$ is expressed as:\n",
    "$$\n",
    "P(x \\mid c) = P(x_1, x_2, \\dots, x_N \\mid c) = \\prod_{i=1}^{N} P(x_i \\mid c)\n",
    "$$\n",
    "In the *log-domain* we express it as:\n",
    "$$\n",
    "\\log P(x \\mid c) = \\sum_{i=1}^{N} \\log \\pi_{c, x_i}\n",
    "$$\n",
    "**So, in order to compute the log-likelihood of each word, we have to sum all the log-probabilities of each word.** <br>\n",
    "Let's consider a scenario where we are predicting the cantica for a tercet. Suppose we have the following words in the tercet: \n",
    "$$\n",
    "x = [x_1, x_2, x_3] = [\\text{fire}, \\text{burning}, \\text{night}]\n",
    "$$\n",
    "We also have the word probabilities (calculated from the MLE) for a specific cantica, say *Inferno*, stored in $\\pi_{\\text{Inferno}, x}$, which represents the probability of each word occurring in the *Inferno* cantica.\n",
    "\n",
    "For this example, suppose the probabilities for each word are as follows:\n",
    "- $P(\\text{fire} \\mid \\text{Inferno}) = 0.2$\n",
    "- $P(\\text{burning} \\mid \\text{Inferno}) = 0.3$\n",
    "- $P(\\text{night} \\mid \\text{Inferno}) = 0.1$\n",
    "\n",
    "We can calculate the log-likelihood for this tercet in the *Inferno* cantica as:\n",
    "$$\n",
    "\\log P(x \\mid \\text{Inferno}) = \\log \\{P(\\text{fire} \\mid \\text{Inferno}) \\cdot P(\\text{burning} \\mid \\text{Inferno}) \\cdot P(\\text{night} \\mid \\text{Inferno})\\}\n",
    "$$\n",
    "Using the formula for the log of a product, we get:\n",
    "$$\n",
    "\\log P(x \\mid \\text{Inferno}) = \\log P(\\text{fire} \\mid \\text{Inferno}) + \\log P(\\text{burning} \\mid \\text{Inferno}) + \\log P(\\text{night} \\mid \\text{Inferno})\n",
    "$$\n",
    "Substituting the values, we have:\n",
    "$$\n",
    "\\log P(x \\mid \\text{Inferno}) = \\log 0.2 + \\log 0.3 + \\log 0.1\n",
    "$$\n",
    "This simplifies to:\n",
    "$$\n",
    "\\log P(x \\mid \\text{Inferno}) \\approx -1.6094 - 1.2040 - 2.3026 = -5.116\n",
    "$$\n",
    "\n",
    "Thus, the log-likelihood for the tercet $x = [\\text{fire}, \\text{burning}, \\text{night}]$ in the *Inferno* cantica is approximately $-5.116$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0fe81a",
   "metadata": {},
   "source": [
    "In pratice (i.e. in the code), we create and compute a **matrix of scores** $S$, where each row corresponds to a class and each column to a test sample (tercet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7819a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model1_compute_LogLikelihoods(logMLE, DVAL):\n",
    "    \"\"\"\n",
    "    Compute log-likelihoods for all the classes given a DVAL validation text\n",
    "\n",
    "    Parameters:\n",
    "    - logMLE: log estimators for each word witin each class\n",
    "    - DVAL: text on which ti compute the log-likelihoods\n",
    "\n",
    "    Returned Values:\n",
    "    - ll: loglikelihoods\n",
    "    \"\"\"\n",
    "\n",
    "    ll = {canticaLabel: 0 for canticaLabel in logMLE.keys()}\n",
    "\n",
    "    for canticaLabel in logMLE.keys():\n",
    "        logMLECanticaLabel = logMLE[canticaLabel]\n",
    "        for word in DVAL.split():\n",
    "            logProbWord = logMLECanticaLabel.get(word.strip(), 0.001)\n",
    "            ll[canticaLabel] += logProbWord\n",
    "\n",
    "\n",
    "    return ll   #3 values: {ll_lInf, ll_lPur, ll_lPar}\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0cc33",
   "metadata": {},
   "source": [
    "$S$ has 3 rows, which correspond to:\n",
    "- row $0$: *Inferno*\n",
    "- row $1$: *Purgatorio*\n",
    "- row $2$: *Paradiso*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd6618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model1_matrixScore(logMLE, tercetsList):\n",
    "    \"\"\"\n",
    "    Compute a score matrix M having each row equal to each class (lInf, lPur, lPar) and each column to a test sample (tercet)\n",
    "    \"\"\"\n",
    "\n",
    "    numClasses = len(logMLE.keys())\n",
    "    S = np.zeros((numClasses, len(tercetsList)))\n",
    "\n",
    "    for tercetColumnCount, tercet in enumerate(tercetsList):\n",
    "        #tercetColumnCount is for inserting the computed likelihoods in the right trailing column\n",
    "        logLikelihoods = Model1_compute_LogLikelihoods(logMLE, tercet)\n",
    "\n",
    "        for row, canticaLabel in enumerate(logMLE.keys()):\n",
    "            S[row, tercetColumnCount] = logLikelihoods[canticaLabel]\n",
    "\n",
    "    return S\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f5e7d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1_S shape: (3, 1204)\n"
     ]
    }
   ],
   "source": [
    "Model1_S = Model1_matrixScore(MLE, DVAL)\n",
    "print(f\"Model1_S shape: {Model1_S.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb9d96",
   "metadata": {},
   "source": [
    "### Closed-set multiclass classification\n",
    "\n",
    "In order to perform classification we have to act like we did with the countinous models: we compute the log-Posteriors and then choose for each sample te highest one. <br>\n",
    "By assuming uniform Priors $P(l\\_lInf) = P(l\\_lPur) = P(l\\_lPar) = \\frac{1}{3}$ we can compute the log joints by summing together the log likelihoods with the log Priors like in the continous case:\n",
    "$$\n",
    "\\log f_{X,C}(x_t, c) = \\log \\left( f_{X|C}(x_t | c) P_C(c) \\right) = \\log f_{X|C}(x_t | c) + \\log P_C(c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dd6e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mean_covariance import vcol, vrow\n",
    "from scipy.special import logsumexp                                 #for scipy.special.logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f415060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSJoint(S, Priors):\n",
    "    #Compute the log joint densities by multiplying the score matrix S with the Priors\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - S: the score matrix of shape (numClasses, numSamples) where each row is the score of the class given the sample\n",
    "    - Priors: the priors of the classes, so a list of length numClasses\n",
    "    \n",
    "\n",
    "    Returned Values:\n",
    "    - SJoint: the log- joint densities of shape (numClasses, numSamples) where each row is the joint density of the class given the sample\n",
    "    \"\"\"\n",
    "\n",
    "    return S + vcol(np.log(Priors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24c01b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1_SJoint shape: (3, 1204)\n"
     ]
    }
   ],
   "source": [
    "Model1_SJoint = computeSJoint(Model1_S, np.ones((3, )) / 3.)\n",
    "print(f\"Model1_SJoint shape: {Model1_SJoint.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f3722",
   "metadata": {},
   "source": [
    "Finally, exactly in the same way we did with the continous case, we can compute the log class Posteriors probabilities as:\n",
    "$$\n",
    "\\log P(C_t = c \\mid \\mathbf{X}_t = \\mathbf{x}_t) = \\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{\\sum_{c' \\in C} f_{\\mathbf{X},C}(\\mathbf{x}_t, c')} \\right) = \n",
    "\\log \\left( \\frac{f_{\\mathbf{X},C}(\\mathbf{x}_t, c)}{f_{\\mathbf{X}}(\\mathbf{x}_t)} \\right) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log  f_{\\mathbf{X}}(\\mathbf{x}_t) = \\log f_{\\mathbf{X},C}(\\mathbf{x}_t, c) - \\log \\sum_{c} e^{l_{c}}\n",
    "$$ \n",
    "where $l_{c}$ are all the log-joints. <br>\n",
    "However, we need to take care that computing the exponential terms may result again in numerical\n",
    "errors. A robust method to comute $\\log \\sum_{c} e^{l_{c}}$ is to rewrite it as:\n",
    "$$\n",
    "\\log \\sum_{c} e^{l_{c}} = l + \\log \\sum_{c} e^{l_{c} - l}\n",
    "$$\n",
    "where $l$ is the highest of the log-joints: $l = max_{c} {l_{c}}$\n",
    "This is known as the *log-sum-exp* trick, and is already implemented in *scipy* as `scipy.special.logsumexp`. We can thus use `scipy.special.logsumexp(s)`,\n",
    "where `s` is the array that contains the joint log-probabilities for a given sample, to compute the log-marginals $\\log f_X(x_{t})$. <br>\n",
    "`scipy.special.logsumexp` also allows specifying an axis, thus we can directly compute the array of\n",
    "marginals for all samples directly from the matrix of joint log-densities as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4c0013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePosteriors(SJoint):\n",
    "    \"\"\"\n",
    "    Compute the log posteriors by normalizing the joint densities\n",
    "    The posteriors are the log joint densities minus the log of the sum of the joint densities which are the marginals\n",
    "\n",
    "    Parameters:\n",
    "    - SJoint: the log joint densities of shape (numClasses, numSamples) where each row is the joint density of the class \n",
    "\n",
    "    Returned Values:\n",
    "    - SPost: the log posteriors of shape (numClasses, numSamples) where each row is the posterior of the class given the sample\n",
    "    \"\"\"\n",
    "\n",
    "    #1. Compute marginals usign the logsumexp trick to minimize numerical problems\n",
    "    #logsumexp is a function that computes the log of the sum of exponentials of input elements\n",
    "    #It is more numerically stable than computing the sum of exponentials directly\n",
    "    #It computes log(exp(a) + exp(b)) in a numerically stable way\n",
    "\n",
    "    #sum over the rows (axis=0) to get the marginal of each sample\n",
    "    SMarginal = logsumexp(SJoint, axis=0)\n",
    "    #SMarginal has now shape = (numSamples, ) -> it's a row vector\n",
    "    #I need to make it of shape (1, numSamples) \n",
    "    SPost = SJoint - vrow(SMarginal) #element wise division in log scale, so I just need to subtract the marginals from the joint densities\n",
    "    \n",
    "    return SPost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2250771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log Posteriors shape: (3, 1204)\n"
     ]
    }
   ],
   "source": [
    "#calculate log S post\n",
    "Model1_SPost = computePosteriors(Model1_SJoint) #compute the posteriors by normalizing the joint densities\n",
    "print(f\"log Posteriors shape: {Model1_SPost.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4d8b5",
   "metadata": {},
   "source": [
    "**Classification Rule**: As done with the continous case, the optimal Bayes decision is to select for each test sample the class with highest **log Posterior probability**: \n",
    "$$\n",
    "c_{t}^{*} = argmax_{c} \\log P (C_{t} = c \\mid \\mathbf{X}_t = \\mathbf{x}_t)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bc01f",
   "metadata": {},
   "source": [
    "For predictions:\n",
    "- value $0$: *Inferno*\n",
    "- value $1$: *Purgatorio*\n",
    "- value $2$: *Paradiso*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b0bc8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (1204,)\n",
      "First 10 Predictions: [1 0 1 1 1 0 1 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "#apply the classification rule\n",
    "Model1_PVAL = np.argmax(Model1_SPost, axis=0)\n",
    "print(f\"Predictions shape: {Model1_PVAL.shape}\")\n",
    "print(f\"First 10 Predictions: {Model1_PVAL[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d3fd7",
   "metadata": {},
   "source": [
    "Model #1 - Overall Error Rate and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e700f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1 - Number of overall wrong predictions: 573\n",
      "Model1 - Overall Error Rate: 47.59%\n",
      "Model1 - Overall Accuracy: 52.41%\n"
     ]
    }
   ],
   "source": [
    "Model1_error_count = np.count_nonzero(Model1_PVAL != LVAL)\n",
    "print(f\"Model1 - Number of overall wrong predictions: {Model1_error_count}\")\n",
    "Model1_error_rate = np.mean(Model1_PVAL != LVAL)\n",
    "print(f\"Model1 - Overall Error Rate: {Model1_error_rate:.2%}\")\n",
    "Model1_accuracy = 1 - Model1_error_rate\n",
    "print(f\"Model1 - Overall Accuracy: {Model1_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe65059",
   "metadata": {},
   "source": [
    "Model #1 - Per class Error Rate and Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2866909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 (Inferno) - Error Count: 190\n",
      "Class 0 (Inferno) - Error Rate: 47.50%\n",
      "Class 0 (Inferno) - Accuracy: 52.50%\n",
      "----------------------------------------\n",
      "Class 1 (Purgatorio) - Error Count: 211\n",
      "Class 1 (Purgatorio) - Error Rate: 52.49%\n",
      "Class 1 (Purgatorio) - Accuracy: 47.51%\n",
      "----------------------------------------\n",
      "Class 2 (Paradiso) - Error Count: 172\n",
      "Class 2 (Paradiso) - Error Rate: 42.79%\n",
      "Class 2 (Paradiso) - Accuracy: 57.21%\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "canticaLabels = {0: \"Inferno\", 1: \"Purgatorio\", 2: \"Paradiso\"}\n",
    "Model1_accs = []\n",
    "Model1_err = []\n",
    "\n",
    "for class_label in range(len(DTR.keys())):\n",
    "    class_indices = np.where(LVAL == class_label)[0]\n",
    "    pred_class = Model1_PVAL[class_indices]\n",
    "    true_class = LVAL[class_indices]\n",
    "\n",
    "    error_count = np.count_nonzero(pred_class != true_class)\n",
    "    error_rate = error_count / len(class_indices)\n",
    "    accuracy = 1 - error_rate\n",
    "\n",
    "    Model1_err.append(error_rate)\n",
    "    Model1_accs.append(accuracy)\n",
    "\n",
    "    print(f\"Class {class_label} ({canticaLabels[class_label]}) - Error Count: {error_count}\")\n",
    "    print(f\"Class {class_label} ({canticaLabels[class_label]}) - Error Rate: {error_rate:.2%}\")\n",
    "    print(f\"Class {class_label} ({canticaLabels[class_label]}) - Accuracy: {accuracy:.2%}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d03f288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = list(canticaLabels.values())\n",
    "colors = ['crimson', 'orange', 'royalblue']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(labels, Model1_err, color=colors)\n",
    "\n",
    "plt.title(\"Model Error Rates per Cantica\")\n",
    "plt.xlabel(\"Cantica\")\n",
    "plt.ylabel(\"Erro Rate\")\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f\"{yval:.2%}\", ha='center', fontsize=11)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Model1_error_rates.png\", dpi=300) #save the figure with a resolution of 300 dpi\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cbe586",
   "metadata": {},
   "source": [
    "## Model #2\n",
    "We can now introduce another model, slightly similar to the first one. Here the focus is related to the **occurrencies of each word**. In this case we have $M$ random variables $Y_j \\in \\mathbb{N}$ which describe how many times each token $x_i$ (which in the previous model was considered a random variblae but here it's not) compares in the text to analyze. So we can formalize all of this this way: <br>\n",
    "Let the document be represented by a sequence of tokens:\n",
    "\n",
    "$$\n",
    "x = [x_1, x_2, \\dots, x_N]\n",
    "$$\n",
    "\n",
    "We define $M$ random variables $Y_1, Y_2, \\dots, Y_M$ where each $Y_j \\in \\mathbb{N}$ denotes the number of occurrences of word $j$ in the document.\n",
    "\n",
    "The relation between the previous model (with $X_i$) and the new model is:\n",
    "\n",
    "$$\n",
    "Y_j = \\sum_{i=1}^{N} \\mathbb{I}[X_i = j]\n",
    "$$\n",
    "\n",
    "where $\\mathbb{I}[X_i = j]$ is the indicator function, equal to 1 if the $i$-th word is $j$, and 0 otherwise. <br>\n",
    "So, for example: <br>\n",
    "Let  \n",
    "$$\n",
    "x = [x_1, x_2, \\dots, x_6] = [\\text{love}, \\text{hope}, \\text{love}, \\text{peace}, \\text{hope}, \\text{hope}]\n",
    "$$\n",
    "\n",
    "Then we can define these random variables:\n",
    "$$\n",
    "Y_{\\text{love}} = \\sum_{i=1}^6 \\mathbb{I}[x_i = \\text{love}] = 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y_{\\text{peace}} = \\sum_{i=1}^6 \\mathbb{I}[x_i = \\text{peace}] = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y_{\\text{hope}} = \\sum_{i=1}^6 \\mathbb{I}[x_i = \\text{hope}] = 3\n",
    "$$\n",
    "This rapresentation is often called **Bag of Words** (*BoW*). We assume a **Multinomial distribution** for our new random variables, because we consider each word occurrence to be an **independet draw** from the **categorical distribution** over the vocabulary of words, and then we count how many times the words occurr in the document, so we can formalize everything this way:\n",
    "$$\n",
    "(Y_1, Y_2, ..., Y_M) \\sim \\text{Multinomial}(N, \\Pi)\n",
    "$$\n",
    "Where:\n",
    "- $N = \\sum_{j=1}^{M} Y_j$ is the total number of tokens of the text, computed summing all the occurrencies of each word together\n",
    "- $\\Pi = \\left[ \\pi_{c, 1}, \\pi_{c, 2}, ..., \\pi_{c, M}\\right]$ are the class-conditional probabilities of each word given class $c$, so they're again the model parameters.\n",
    "\n",
    "Now, we can calculate the likelihood by recalling the formula for the Multinomial density:\n",
    "$$\n",
    "\\mathcal{L}_Y(\\mathbf{\\Pi}) = \\frac{N!}{y_1! \\dots y_M!} \\prod_{j=1}^{m} \\pi_j^{y_j} = \\frac{N!}{N_1! \\dots N_M!} \\prod_{j=1}^{m} \\pi_j^{N_j}\n",
    "$$\n",
    "\n",
    "since $y_j$ is the number of occurrences of word $j$, i.e. $y_j = N_j$. The log-likelihood is thus\n",
    "\n",
    "$$\n",
    "\\ell_Y(\\mathbf{\\Pi}) = \\log \\mathcal{L}_Y(\\mathbf{\\Pi}) = \\xi + \\sum_{j=1}^{m} y_j \\log \\pi_j = \\xi + \\sum_{j=1}^{m} N_j \\log \\pi_j\n",
    "$$\n",
    "\n",
    "where the term $\\xi$ is the logarithm of the multinomial coefficients, which is constant with respect to the model parameters $\\Pi = \\left[ \\pi_{c, 1}, \\pi_{c, 2}, ..., \\pi_{c, M}\\right]$, so we don't consider it when maximizing the log-likelihood. <br>\n",
    "Apart from the constant, **this is exactly equal to the log-likelihood of the first model!**\n",
    "So we can write:\n",
    "$$\n",
    "\\mathcal{L}_X(\\mathbf{\\Pi}) \\propto \\mathcal{L}_Y(\\mathbf{\\Pi}) \\\\\n",
    "\\ell_X(\\mathbf{\\Pi}) \\propto \\ell_Y(\\mathbf{\\Pi})\n",
    "$$\n",
    "Since the likelihoods are proportional, **inference** will be the same using te Maximum Likelihood Approach (because we are trying to mazimize two proportional functions):\n",
    "- ML estimates will be the same for both models (the terms $\\xi$ is irrelevant for maximization)\n",
    "- Likelihood ratios and class posterior probabilities will also be the same, since the proportionality term $\\alpha = e^{\\xi}$ simplifies\n",
    "\n",
    "In conclusion, **from an implementation point of view, the computational steps of the second model are the same as the first one. The only difference is that we work with word counts instead of word sequences.** <br>\n",
    "The results, obviously, will be **the same**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44399c1",
   "metadata": {},
   "source": [
    "### Binary Classifier\n",
    "The lexical choices in Purgatorio are something in between those of Paradiso and Inferno. We\n",
    "thus consider pairs **Inferno - Paradiso**, **Inferno - Purgatorio** and **Purgatorio - Paradiso**. These are\n",
    "3 binary tasks. Since the two models, Model #1 and Model #2, are equivalent from a practical point of view, we can choose whatever we like. We can choose **Model #1**.<br>\n",
    "Since we are solving a binary problem, we can speed up the process by computing the **class posterior log-ratio**, which is the log of the ratio between the two Posteriors:\n",
    "$$\n",
    "\\log \\text{r}(x_{t}) = \\log \\frac{P (C_{t} = 2 \\mid \\mathbf{X}_t = \\mathbf{x}_t)}{P (C_{t} = 1 \\mid \\mathbf{X}_t = \\mathbf{x}_t)}\n",
    "$$\n",
    "where, if we are classifying the pair **Inferno - Paradiso**, for example, $C1$ will be *Inferno*, while $C2$ will be *Paradiso*. <br>\n",
    "In the log domain this is the sum of two terms:\n",
    "$$\n",
    "\\log \\text{r}(x_{t}) = \\log \\frac{f_{X|C}(x_t | 2)}{f_{X|C}(x_t | 1)} +\n",
    "\\log \\frac{P(C= 2)}{P(C=1)}\n",
    "$$\n",
    "The first term, which is the log of the ratio of the likelihoods, so the conditionals, of the two classes, is called **log-likelihood ratio** -- $\\text{llr}(x_t)$. <br>\n",
    "The classification rule is simple: we compare the $\\text{llr}(x_{t})$ to the **prior log-odds** $ \\left( \\log \\frac{P(C= 2)}{P(C=1)} \\right)$, which can be considered a selected *Threshold* depending on the application. This generates a decision surfaces used to separate and classify our data. <br>\n",
    "For the **prior log-odds** we can simplify it by choosing each prior to be equal to $1/2$ and uniformally distributed, following a uniform distribution, with respect to the other one. For example, if we are classifying the pair **Inferno - Paradiso** we set: $P(\\text{llInf}) = P(\\text{lPar}) = 1/2$. This way, the *Threshold* becomes equal to zero and we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c903348d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lInf', 'lPur', 'lPar']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(DTR.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d51e1952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lInf vs lPur - Number of wrong predictions: 306\n",
      "lInf vs lPur - Error Rate: 38.15%\n",
      "lInf vs lPur - Accuracy: 61.85%\n",
      "lInf vs lPar - Number of wrong predictions: 205\n",
      "lInf vs lPar - Error Rate: 25.56%\n",
      "lInf vs lPar - Accuracy: 74.44%\n",
      "lPur vs lPar - Number of wrong predictions: 281\n",
      "lPur vs lPar - Error Rate: 34.95%\n",
      "lPur vs lPar - Accuracy: 65.05%\n"
     ]
    }
   ],
   "source": [
    "#0 -> lInf vs lPur\n",
    "#1 -> lInf vs lPur\n",
    "#2 -> lPur vs lPar\n",
    "pairs = [(0, 1), (0, 2), (1, 2)]\n",
    "canticaLabels = list(DTR.keys())\n",
    "\n",
    "DVAL_Binary = {\n",
    "    \"lInf\": lInf_evaluation,\n",
    "    \"lPur\": lPur_evaluation,\n",
    "    \"lPar\": lPar_evaluation\n",
    "}\n",
    "\n",
    "for i, j in pairs:\n",
    "    #cantica0: first cantica to use -> index i\n",
    "    #cantica1: second cantica to use -> index j\n",
    "\n",
    "    cantica0 = canticaLabels[i]\n",
    "    cantica1 = canticaLabels[j]\n",
    "\n",
    "    #0. pre-processing: build DTR01, DVAL01, LVAL01\n",
    "    DTR01 = {\n",
    "        cantica0: DTR[cantica0],\n",
    "        cantica1: DTR[cantica1]\n",
    "    }\n",
    "\n",
    "    DVAL01 = DVAL_Binary[cantica0] + DVAL_Binary[cantica1]\n",
    "\n",
    "    LVAL01 = [0] * len(DVAL_Binary[cantica0]) + [1] * len(DVAL_Binary[cantica1])    #LVAL has just 0 and 1 as values\n",
    "\n",
    "    #1. compute MLE\n",
    "    MLE01, globalDict01 = Model1_Estimators(DTR01)\n",
    "\n",
    "    #2. compute matrix score for the two canticas\n",
    "    #S01 has two rows: row0 is for cantica0 log likelihood, row1 is for cantica1 log likelihood\n",
    "    S01 = Model1_matrixScore(MLE01, DVAL01)\n",
    "\n",
    "    #3. compute llr using the two log likelihoods\n",
    "    ll0 = S01[0, :]\n",
    "    ll1 = S01[1, :]\n",
    "    llr = ll1 - ll0\n",
    "\n",
    "    #4. Classification rule: assign samples whose llr >= 0 to class 1 and samples whose llr < 0 to class 0\n",
    "    PVAL01 = np.where(llr >= 0, 1, 0) #assign samples whose llr>= 0 to class 1 and samples whose llr < 0 to class 0\n",
    "\n",
    "    #5. compute error rate\n",
    "    error_count01 = np.count_nonzero(PVAL01 != LVAL01)\n",
    "    print(f\"{cantica0} vs {cantica1} - Number of wrong predictions: {error_count01}\")\n",
    "    error_rate01= np.mean(PVAL01 != LVAL01)\n",
    "    print(f\"{cantica0} vs {cantica1} - Error Rate: {error_rate01:.2%}\")\n",
    "\n",
    "    #6. compute accuracy\n",
    "    acc01 = 1 - error_rate01\n",
    "    print(f\"{cantica0} vs {cantica1} - Accuracy: {acc01:.2%}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a88ef3",
   "metadata": {},
   "source": [
    "**This confirms that Inferno and Paradiso are easier to discriminate (higher accuracy), whereas Purgatorio is more similar to both other canticas, being written in an intermediate style between the two other canticas!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b3b77",
   "metadata": {},
   "source": [
    "**NOTE**: we can “cheat” and obtain the solution to the binary problems from the score matrix\n",
    "$S$ of the three-class problem instead of calculating the score matrix $S01$ again by calling the function `Model1_matrixScore(MLE01, DVAL01)`. Indeed, if we consider only the rows of $S$ that we are interested\n",
    "in, these give us the matrix $Sb$ of class-conditional likelihoods for the two classes. The difference\n",
    "between this approach and re-training using only two classes is that in the former case the binary\n",
    "matrix $Sb$ contains also conditional likelihoods for words that may not appear in neither of the\n",
    "two considered class, but appear in the third one - these would be discarded if we trained from\n",
    "scratch. However, since the additional words would have a frequency proportional to $\\varepsilon$ for both\n",
    "classes, their contribution would disappear when computing class posteriors. <br>\n",
    "So at each iteration we could just do:\n",
    "```python\n",
    "S01 = S[[i, j], :]\n",
    "```\n",
    "Remember to put ```[i, j]``` with the *brackets* since i, j may not always be subsequent between each other (for ex with i = index of *Linf* = 0 and j = index of *lPar* = 2): in *numpy* this notation tells to select just the rows corresponding to the items of the list passed as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db0c7f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-122.71407912, -133.29565846, -134.35953538, ..., -112.18123665,\n",
       "         -85.99405904, -126.66813611],\n",
       "       [-115.78129281, -153.33404455, -126.10784451, ..., -103.53323003,\n",
       "         -80.4692083 , -124.71463026],\n",
       "       [-126.91783481, -150.8452464 , -135.27350217, ..., -104.91819178,\n",
       "         -79.87447066, -122.84802869]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model1_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28d400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-122.71407912, -133.29565846, -134.35953538, ..., -112.18123665,\n",
       "         -85.99405904, -126.66813611],\n",
       "       [-115.78129281, -153.33404455, -126.10784451, ..., -103.53323003,\n",
       "         -80.4692083 , -124.71463026]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model1_S[[0, 1], :] #for lInf (index 0) and lPur (index1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5fc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-122.71407912, -133.29565846, -134.35953538, ..., -112.18123665,\n",
       "         -85.99405904, -126.66813611],\n",
       "       [-126.91783481, -150.8452464 , -135.27350217, ..., -104.91819178,\n",
       "         -79.87447066, -122.84802869]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model1_S[[0, 2], :] #for lInf (index 0) and lPar (index 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
