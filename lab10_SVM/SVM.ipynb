{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c152af",
   "metadata": {},
   "source": [
    "# Linear Soft Margin SVM\n",
    "SVM is a classification algorithm that aims at finding the decision surface which maximizes the margin between itself and the class samples. <br>\n",
    "In the context of SVM, we define *margin* as the distance between the hyperplane and the sample which is the closest to it. <br>\n",
    "The linear function, wrt $\\mathbf{x}$:\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
    "$$\n",
    "represents the equation of the hyperplane, if put equal to zero:\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b = 0\n",
    "$$\n",
    "Recalling that the perpendicular distance of a point $\\mathbf{x}$ from an hyperplane $y(\\mathbf{x}) = 0$ is $\\mid y(\\mathbf{x}) \\mid / \\|\\mathbf{w}\\|$, we can write:\n",
    "$$\n",
    "d(\\mathbf{x}_i) = \\frac{\\mid \\mathbf{w}^T \\mathbf{x}_i + b \\mid}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "where $\\mathbf{w}$ si always the weights vector, orthogonal to the hyperplane. <br>\n",
    "We can introduce the same substitution for the classes we used when computing the average Loss Function for L.R.:\n",
    "$$\n",
    "z_i = 2c_i - 1 \\implies\n",
    "\\begin{cases}\n",
    "    z_i = 2* 0 - 1 = -1 & \\text{if } c_i = 0 \\\\\n",
    "    z_i = 2 * 1 - 1 = 1  & \\text{if } c_i = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "Since at the numerator we have the absolute value, the the distance doesn't change if we write:\n",
    "$$\n",
    "d(\\mathbf{x}_i) = \\frac{\\mid z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\mid}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "If we just want to consider solutions which **correctly classify all samples**, we can thus maximize, wrt the model parameters $(\\mathbf{w}, b)$, the objective function:\n",
    "$$\n",
    "\\operatorname*{argmax}_{\\mathbf{w}, b} \\left\\{ \\operatorname*{min}_{i} \\left\\{\\frac{\\mid z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\mid}{\\|\\mathbf{w}\\|} \\right\\} \\right\\} \\\\[1em]\n",
    "\\text{subject to: } z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) > 0\n",
    "$$\n",
    "So, in other words, we want to find the hyperplane, described by $(\\mathbf{w}, b)$, which has the maximum distance to its closest point, so the largest minimum distance from the samples. The contraint ensures we just select hyperplanes which make just correct clasifications. <br>\n",
    "We can write this in a more compact form, and meanwhile also drop the contraint and the absolute value at the denominator, observing that all solutions which correctly classify all samples meet the contraint for each sample $\\mathbf{x}_i$ and so for them we will always have $\\operatorname*{min}_{i} \\left\\{ z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\right\\} > 0$, so:\n",
    "$$\n",
    "\\operatorname*{argmax}_{\\mathbf{w}, b} \\left\\{ \\frac{1}{\\|\\mathbf{w}\\|} \\operatorname*{min}_{i} \\left\\{ z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\right\\} \\right\\}\n",
    "$$\n",
    "Now, we can exploit the property of this objective function being **invariant to rescaling**, i.e. given the rescaling factor $\\phi > 0$ we know that both functions:\n",
    "$$\n",
    "\\frac{1}{\\|\\mathbf{w}\\|} \\operatorname*{min}_{i} \\left\\{ z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\right\\} \\\\[1em]\n",
    "\\frac{1}{\\|\\phi \\mathbf{w}\\|} \\operatorname*{min}_{i} \\left\\{ z_i \\left( \\phi \\mathbf{w}^T \\mathbf{x}_i + \\phi b \\right) \\right\\} \\\\[1em]\n",
    "$$\n",
    "will lead to both optimal solutions $(\\mathbf{w}^*, b^*)$ or $(\\phi \\mathbf{w}^*, \\phi b^*)$. In other words, the collection of parameters $(\\phi \\mathbf{w}^*, \\phi b^*)\\mid_{\\phi > 0}$ forms an **equivalence class** of equivalent solutions. <br>\n",
    "Because of the fact that we can choose any solution among them, to simplify the objective function we choose the one corresponding to $\\phi = 1$, so for which we have:\n",
    "$$\n",
    "\\operatorname*{min}_{i} \\left\\{ z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\right\\} = 1\n",
    "$$\n",
    "The objective function thus becomes just:\n",
    "$$\n",
    "\\frac{1}{\\|\\mathbf{w}\\|} \\\\[1em]\n",
    "\\text{subject to: } \n",
    "\\begin{cases}\n",
    "\\operatorname*{min}_{i} \\left\\{ z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\right\\} = 1 \\\\\n",
    "z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\geq 1 \\space \\forall i\n",
    "\\end{cases}\n",
    "$$\n",
    "Then, in order to link the SVM objective to the L.R. one, we can make some constant transformations. Then we can also drop the first contraint observing that, since we're **minimizing** the objective function (because we now put $\\mathbf{w}$ at the numerator thanks to these transformations), optimal solutions will have just points such that $z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\geq 1$, so we can write the **Primal Formulation of the Hard-Margin SVM Problem**:\n",
    "$$\n",
    "\\operatorname*{argmin}_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\\\[1em]\n",
    "\\text{subject to: } z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\geq 1 \\space \\forall i\n",
    "$$\n",
    "It's called **Hard Margin** since all of this is based on the assumption that the optimal solution always correctly classify all points (and this is expressed in the contraint). <br>\n",
    "\n",
    "Then, the steps to recover the **Soft Margin** version of the primal formulation of the problem are pretty quick.<br>\n",
    "If classes are **not linearly separable**, we won't be abe to find a solution which satisfies the primal contraint $z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\geq 1 \\space \\forall i$. We can make a trade off and accept to have some samples which sit **inside the margin region** (for them we would have $ 0 \\leq z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\lt 1$) or, if we want, are even **missclassified** (i.e, they end up on the opposite side of the decision boundary after the margin, for them we would have $z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\lt 0$) and try to achieve the largest margin between all the correcty classified samples. In  practice, we introduce the so called **slack variables** $\\xi_i$ in the primal constraint: \n",
    "$$\n",
    "z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b \\right) \\geq 1 - \\xi_i \\\\\n",
    "\\xi_i \\geq 0\n",
    "$$\n",
    "Keep in mind that $\\xi_i \\geq 0$ is an additional primal contraint related to the Soft Margin formulation of the problem. So:\n",
    "- correctly classified points not inside the margin will have $\\xi_i = 0$\n",
    "- correctly classified points which sit inside the margin will have $0 \\lt \\xi_i \\lt 1$\n",
    "- missclassified points will have $\\xi_i \\geq 1$\n",
    "\n",
    "The functional:\n",
    "$$\n",
    "\\Phi(\\xi) = \\sum_{i = 1}^{n} (\\xi_i)^{\\sigma}\n",
    "$$\n",
    "- for small values of $\\sigma$ is approximately equal to the number of points violating the hard margin contraint\n",
    "- for $\\sigma = 1$ represents **an upper bound** on the number of samples violating the hard margin contraint\n",
    "\n",
    "Since using small values of $\\sigma$ makes the problem not convex anymore, we set $\\sigma = 1$ and can finally write the **Primal Formulation of the Soft-Margin SVM Problem**:\n",
    "$$\n",
    "\\operatorname*{min}_{\\mathbf{w}, b, \\mathbf{\\xi}} \\left\\{ \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i = 1}^{n} \\xi_i\\right\\} \\\\[1em]\n",
    "\\text{subject to: } \n",
    "\\begin{cases}\n",
    "z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b\\right) \\geq 1 - \\xi_i \\space \\forall i \\\\\n",
    "\\xi_i \\geq 0 \\space \\forall i\n",
    "\\end{cases}\n",
    "$$\n",
    "where $C$ is the hyperparameter which regulates the trade off:\n",
    "- for small values of $C$ we concentrate more on achieving the largest margin and less on minimizing samples violations, so the model will generalize better on unseen data, but te error rate on the training set will increase\n",
    "- for high values of $C$ we prefer first to minimize samples vuilations (because the second term in now stronger), and just after we concentrate on achieving a large margin for non violating points. In this case the model will achieve much better performances on the training set but won't be able to generalize as much.\n",
    "\n",
    "We can even turn this problem into an **unconstrained** one by remembering that:\n",
    "- correctly classified samples which are not inside the margin region have:\n",
    "    $$\n",
    "    \\begin{cases}\n",
    "        \\xi_i = 0 \\\\\n",
    "        z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b\\right) \\geq 1\n",
    "    \\end{cases}\n",
    "    $$\n",
    "- all the other samples are violating the hard margin primal constraint so they have:\n",
    "    $$\n",
    "    \\begin{cases}\n",
    "        \\xi_i \\gt 0 \\\\\n",
    "        z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b\\right) = 1 - \\xi_i\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "So $\\xi_i$ can either assume value $0$ (first case) or $1 - z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b\\right)$ (second case). <br>\n",
    "We can directly insert these two cases into the objective and get rid of the primal contraints:\n",
    "$$\n",
    "\\operatorname*{min}_{\\mathbf{w}, b, \\mathbf{\\xi}} \\left\\{ \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i = 1}^{n} \\operatorname*{max} \\left[ 0, 1 - z_i \\left( \\mathbf{w}^T \\mathbf{x}_i + b\\right) \\right]\\right\\}\n",
    "$$\n",
    "where $f(z_i s_i) = \\operatorname*{max} \\left[ 0, 1 - z_i s_i \\right]$ is called **Hinge Loss**. <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40baef",
   "metadata": {},
   "source": [
    "**Unfortunately, the unconstrained formulation of the primal objective function is non-differentiable. While the L-BFGS method may still be able to find the minimizer of the objective, we have no guarantee that the algorithm will stop close to the optimal value of $(\\mathbf{w}, b)$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be1a96",
   "metadata": {},
   "source": [
    "So, we can introduce (and later use) the **Lagrangian formulation** of this problem, incorporating the two primal contraints directly in the objective function, using the two Lagrangian multipliers $\\alpha_i \\geq 0$, $\\mu_i \\geq 0$:\n",
    "$$\n",
    "L(\\mathbf{w}, b, \\xi, \\alpha, \\mu) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i - \\sum_{i=1}^{n} \\alpha_i [z_i(\\mathbf{w}^T \\mathbf{x}_i + b) - 1 + \\xi_i] - \\sum_{i=1}^{n} \\mu_i \\xi_i\n",
    "$$\n",
    "The optimal solution has to satisfy the **KKT conditions**:\n",
    "1) **Stationary Conditions**: <br>\n",
    "    These conditions are obtained by taking the partial derivatives of the Lagrangian L with\n",
    "    respect to each of the primal variables and setting them to zero:\n",
    "    $$\n",
    "    \\nabla_{\\mathbf{w}} L(\\mathbf{w}, b, \\xi, \\alpha, \\mu) = \\mathbf{w} - \\sum_{i=1}^{n} \\alpha_i z_i \\mathbf{x}_i = 0\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial L(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\partial b} = - \\sum_{i=1}^{n} \\alpha_i z_i = 0\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial L(\\mathbf{w}, b, \\xi, \\alpha, \\mu)}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0 \\quad \\forall i\n",
    "    $$\n",
    "\n",
    "2) **Primal Feasibility Conditions**: <br>\n",
    "    These are the original inequality constraints of the SVM primal problem. The Lagrangian\n",
    "    was constructed to incorporate these:\n",
    "    $$\n",
    "    z_i(\\mathbf{w}^T \\mathbf{x}_i + b) - 1 + \\xi_i \\geq 0 \\quad \\forall i\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    \\xi_i \\geq 0 \\quad \\forall i\n",
    "    $$\n",
    "\n",
    "3) **Dual Feasibility Conditions**: <br>\n",
    "    The Lagrange multipliers associated with inequality constraints must be non-negative:\n",
    "    $$\\alpha_i \\ge 0 \\quad \\forall i$$\n",
    "    $$\\mu_i \\ge 0 \\quad \\forall i$$\n",
    "\n",
    "4) **Complementary Slackness Conditions**: <br>\n",
    "    For each inequality constraint, the product of the Lagrange multiplier and the constraint\n",
    "    itself must be zero at the optimal solution:\n",
    "    $$\n",
    "    \\alpha_i [z_i(\\mathbf{w}^T \\mathbf{x}_i + b) - 1 + \\xi_i] = 0 \\quad \\forall i\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    \\mu_i \\xi_i = 0 \\quad \\forall i\n",
    "    $$\n",
    "\n",
    "We can rewrite the objective in a simpler way using the precious results we obtain from the **Stationary Conditions**:\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_i z_i \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\alpha_i z_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_i = C - \\mu_i\n",
    "$$\n",
    "Also, combining the conditions: $\\alpha_i = C - \\mu_i$,  $\\alpha_i \\geq 0$,  $\\mu_i \\geq 0$ lets us find the so called **box contraints**:\n",
    "$$\n",
    "0 \\leq \\alpha_i \\leq C\n",
    "$$\n",
    "Keeping in mind we must satisfy the box contraints, we can replace $\\mathbf{w}$, $\\alpha_i$ and $\\xi_i$ inside the Lagrangian primal objective function to obtain\n",
    "the **Dual Soft Margin SVM Problem**:\n",
    "$$\n",
    "\\max_{\\alpha} L_D(\\alpha) = \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j z_i z_j \\mathbf{x}_i^T \\mathbf{x}_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to:}\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 \\leq \\alpha_i \\leq C, \\quad \\forall i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\alpha_i z_i = 0\n",
    "$$\n",
    "\n",
    "The SVM dual optimization problem can be expressed in **matrix form** as maximizing the objective function $J^D(\\alpha)$:\n",
    "\n",
    "$$\n",
    "J^D(\\mathbf{\\alpha}) = -\\frac{1}{2}\\mathbf{\\alpha}^T \\mathbf{H} \\mathbf{\\alpha} + \\mathbf{\\alpha}^T \\mathbf{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to:}\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 \\le \\alpha_i \\le C, \\quad \\forall i \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\alpha_i z_i = 0\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $\\mathbf{1}$ is an n-dimensional vector of ones.\n",
    "* $\\mathbf{H}$ is an $n \\times n$ matrix whose elements are given by $H_{ij} = z_i z_j \\mathbf{x}_i^T \\mathbf{x}_j$. This matrix is also known as the Gram matrix of the kernel function (in this case, a linear kernel).\n",
    "\n",
    "The SVM dual solution, denoted as $\\alpha^*$, is the value of $\\alpha$ that maximizes $J^D(\\alpha)$. The optimal weight vector $\\mathbf{w}^*$ of the primal problem is then related to the optimal $\\alpha^*$ by:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^* = \\sum_{i=1}^{n} \\alpha_i^* z_i \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805cd20",
   "metadata": {},
   "source": [
    "Now, the problem is that the L-BFGS algorithm is able to handle box constraints, however it cannot incorporate $\\sum_{i=1}^{n} \\alpha_i z_i = 0$. <br>\n",
    "The constraint derives from the presence of the **bias** term in the SVM primal formulation. <br>\n",
    "We therefore slightly modify the SVM problem as to make the constraint disappear. This way we will be able to use **L-BFGS-B** (the B stands for box-constraints) to **solve the dual problem**. <br>\n",
    "We reformulate the primal problem as the minimization of\n",
    "$$\n",
    "\\hat{J}(\\hat{\\mathbf{w}}) = \\frac{1}{2} \\|\\hat{\\mathbf{w}}\\|^2 + C \\sum_{i=1}^{n} \\max(0, 1 - z_i (\\hat{\\mathbf{w}}^T \\hat{\\mathbf{x}}_i))\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\hat{\\mathbf{x}}_i = \\begin{bmatrix} \\mathbf{x}_i \\\\ 1 \\end{bmatrix}, \\quad \\hat{\\mathbf{w}} = \\begin{bmatrix} \\mathbf{w} \\\\ b \\end{bmatrix}\n",
    "$$\n",
    "We can observe that $\\hat{\\mathbf{w}}^T \\hat{\\mathbf{x}}_i = \\mathbf{w}^T \\mathbf{x} + b$, i.e. the scoring rules have the same expression as the original formulation. However, in contrast with the original SVM problem, we are also regularizing the value of the bias term, since we regularize the norm of $\\hat{\\mathbf{w}}$:\n",
    "$$\n",
    "\\|\\hat{\\mathbf{w}}\\|^2 = \\|\\mathbf{w}\\|^2 + b^2\n",
    "$$\n",
    "Regularization of the bias can, in general, lead to sub-optimal decisions in terms of separating margin. We can mitigate this effect by using a mapping\n",
    "$$\n",
    "\\hat{\\mathbf{x}}_i = \\begin{bmatrix} \\mathbf{x}_i \\\\ K \\end{bmatrix}\n",
    "$$\n",
    "As K becomes larger, the effects of regularizing b become weaker. However, as K becomes larger, the dual problem also becomes harder to solve (i.e. the algorithm may require many additional iterations).\n",
    "\n",
    "The dual objective of the modified primal SVM becomes the maximization of\n",
    "$$\n",
    "\\hat{J}^D(\\alpha) = -\\frac{1}{2} \\alpha^T \\hat{\\mathbf{H}} \\alpha + \\alpha^T \\mathbf{1}\n",
    "$$\n",
    "$$\\text{subject to }$$\n",
    "$$0 \\le \\alpha_i \\le C \\quad \\forall i \\in \\{1...n\\}$$\n",
    "\n",
    "i.e., the same formulation as before but without the equality constraint and with matrix $\\hat{\\mathbf{H}}$ computed from the extended features $\\hat{\\mathbf{x}}_i$ rather than from the original features $\\mathbf{x}_i$:\n",
    "$$\n",
    "\\hat{\\mathbf{H}}_{i,j} = z_i z_j \\hat{\\mathbf{x}}_i^T \\hat{\\mathbf{x}}_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1317513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.shape: (4, 100)\n",
      "L.shape: (100,)\n",
      "DTR.shape: (4, 66)\n",
      "LTR.shape: (66,)\n",
      "DVAL.shape: (4, 34)\n",
      "LVAL.shape: (34,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#load iris and take just two out of three classes\n",
    "from load_dataset import loadDataSet\n",
    "from train_validation_split import splitTrainingValidation\n",
    "\n",
    "numFeatures = 4\n",
    "D, L = loadDataSet('./iris.csv', numFeatures)\n",
    "\n",
    "#remove iris-setosa (class 0)\n",
    "D = D[:, L != 0]\n",
    "L = L[L != 0]\n",
    "\n",
    "#assign label 0 to class 2\n",
    "L[L == 2] = 0\n",
    "\n",
    "#now we have class 0 and class 1\n",
    "print(f'D.shape: {D.shape}')\n",
    "print(f'L.shape: {L.shape}')\n",
    "\n",
    "#tran validation split\n",
    "(DTR, LTR), (DVAL, LVAL) = splitTrainingValidation(2/3, D, L)\n",
    "\n",
    "print(f'DTR.shape: {DTR.shape}')\n",
    "print(f'LTR.shape: {LTR.shape}')\n",
    "print(f'DVAL.shape: {DVAL.shape}')\n",
    "print(f'LVAL.shape: {LVAL.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e8c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mean_covariance import vcol, vrow\n",
    "import scipy.optimize as opt\n",
    "from decisionModelsEvaluation import computeEmpiricalBayesRisk_Normalized, computeMinEmpiricalBayesRisk_Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SVM_Linear_SoftMargin_Dual(DTR, LTR, C, K = 1):\n",
    "    #map DTR to DTR_hat = [DTR, K_vector]\n",
    "    #stack verticallu (row wise)\n",
    "    #so, at the end of DTR add an additional row of all K\n",
    "    DTR_hat = np.vstack((DTR, K * np.ones((1, DTR.shape[1]))))\n",
    "    #compute ZTR -> z_i = 2* c_i - 1\n",
    "    ZTR = 2 * LTR - 1\n",
    "    #compute n-dimensional vector of ones\n",
    "    Ones = np.ones((DTR.shape[1], 1))\n",
    "\n",
    "\n",
    "    def SVM_obj(alpha):\n",
    "        \"\"\"\n",
    "        Objective function for SVM\n",
    "        :param alpha: Lagrange multipliers vector -> (N, ) = (1, N)\n",
    "        :return: objective function value\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #1) compute H_hat = z_i * z_j * x_i^T * x_j\n",
    "        #G = x_i^T * x_j = <x_i, x_j>\n",
    "        G = DTR_hat.T @ DTR_hat\n",
    "        #use broadcasting to compute z_i * z_j -> vcol(z_i) * vrow(z_j)\n",
    "        #\n",
    "        #(N, N) * [(1, N) * (N, 1)] = (N, N) * (N, N) = (N, N)\n",
    "        #H_hat = G * (ZTR.reshape((1, ZTR.size)) * ZTR.reshape((ZTR.size, 1)))\n",
    "        H_hat = G * (vrow(ZTR) * vcol(ZTR)) #use * and not @ because it's an ELEMENT WISE product -> H_i,j = z_i * z_j * x_i^T * x_j -> ELEMENT WISE!!!\n",
    "\n",
    "        #2) compute the objective function value\n",
    "        #vrow(alpha) * H_hat * vcol(alpha)\n",
    "        #(1, N) * (N, N) * (N, 1) = (1, 1) -> SCALAR\n",
    "        #(1, N) * (N, N) = (1, N)\n",
    "        #(1, N) * (N, 1) = (1, 1)\n",
    "        #first_term = 0.5 * (alpha.reshape((1, alpha.size)) @ H_hat @ alpha.reshape((alpha.size, 1)))\n",
    "        first_term = 0.5 * (vrow(alpha) @ H_hat @ vcol(alpha))\n",
    "\n",
    "        #(1, N) * (N, 1) = (1, 1)\n",
    "        #second_term = - alpha.reshape((1, alpha.size)) @ Ones #this is the dot product between alpha and a vector of all ones -> this is equal to np.sum(alpha)\n",
    "        second_term = - (vrow(alpha) @ Ones)\n",
    "\n",
    "        #3) manually compute the gradient\n",
    "        #(N, N) * (N, 1) = (N, 1)\n",
    "        #Ones is (N, 1)\n",
    "        #gradient = H_hat @ alpha.reshape((alpha.size, 1)) - Ones\n",
    "        gradient = (H_hat @ vcol(alpha) - Ones).flatten()\n",
    "\n",
    "        return first_term + second_term, gradient\n",
    "    \n",
    "    #box contraints\n",
    "    box_contraints = (0, C)\n",
    "    bounds = [box_contraints * DTR.shape[1]]\n",
    "\n",
    "    #find alpha which maximizes the dual objective function \n",
    "    #since we are using L-BFGS-B, we need to minimize the negative of the objective function\n",
    "    bestAlpha, _, _ = opt.fmin_l_bfgs_b(func= SVM_obj, x0 = np.zeros(DTR_hat.shape[0]), approx_grad=False, bounds=bounds)\n",
    "\n",
    "    #then we can recover best_w_hat using the primal contraint, so the I KKT:ù\n",
    "    #w_hat = sum_i (bestAlpha * z_i * x_i)\n",
    "    #alpha: (N, )\n",
    "    #ZTR: (N, )\n",
    "    #alpha * ZTR = (N, )\n",
    "    #DTR_hat: (F + 1, N)\n",
    "    #alpha times DTR -> matrix vector multiplication -> I can explicitly tell to treat alpha * ZTR as a column vector (but it should be automatic)\n",
    "    #bestW_hat = (F + 1, N) * (N, 1) = (F + 1, 1)           (F = features, N = samples)\n",
    "    bestW_hat = DTR_hat @ vcol(bestAlpha * ZTR)\n",
    "\n",
    "\n",
    "    #then we return the optmal solutions\n",
    "    #the most efficient way is to extract the \"original\" bestW and bestB from bestW_hat, so revert the initial transformation\n",
    "    #since bestW_hat = np.array([bestW], [bestB])\n",
    "    bestW = bestW_hat[0:DTR.shape[0]] \n",
    "    bestB = bestW_hat[-1] * K #since K cannot be always = 1 (it's chosen by us), we have to scale the bias\n",
    "\n",
    "    return bestW, bestB\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7e781",
   "metadata": {},
   "source": [
    "Then we have to compute the score:\n",
    "$$\n",
    "s(\\mathbf{x}_t) = \\mathbf{w}^*{^T} \\mathbf{x}_t + b^*\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_SVM_Linear_SoftMargin(DTR, LTR, DVAL, LVAL, C, K = 1, appPriorTrue=0.5):\n",
    "    \"\"\"\n",
    "    Train and fit the SVM Linear Soft Margin classifier\n",
    "    \"\"\"\n",
    "\n",
    "    bestW, bestB = train_SVM_Linear_SoftMargin_Dual(DTR, LTR, C, K)\n",
    "\n",
    "    #then, compute the score\n",
    "    #compute s = bestW^T * DVAL + bestB\n",
    "    #bestW = (F, 1) -> bestW^T = (1, F)\n",
    "    #DVAL = (F, N)\n",
    "    #scores = (1, F) * (F, N) + (F, 1) = (1, N) + (F, 1) -> broadcasting -> (F, N)\n",
    "    scores = vrow(bestW) @ DVAL + bestB\n",
    "\n",
    "    #decision rule: assign class H_T if score > 0, otherwise assign class H_F\n",
    "    #DVAL = (F, N)\n",
    "    #score = (F, N)\n",
    "    #just doing PVAL = DVAL[score > 0] returns a vector of [True, False, True,....]\n",
    "    #so, since we know that True * 1 = 1; False * 1 = 0 (True and False are casted to 1, 0 if we do the multiplication)\n",
    "    PVAL = DVAL[scores > 0] * 1\n",
    "\n",
    "    #calculate error rate\n",
    "    errorRate = np.mean(LVAL != PVAL)\n",
    "\n",
    "    #calculate DCF, min DCF using appPriorTrue\n",
    "    minDCF = computeMinEmpiricalBayesRisk_Normalized(scores, LVAL, appPriorTrue, 0.5, 0.5)\n",
    "    DCF = computeEmpiricalBayesRisk_Normalized(scores, LVAL, appPriorTrue, 0.5, 0.5)\n",
    "\n",
    "    return bestW, bestB, minDCF, DCF, errorRate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
