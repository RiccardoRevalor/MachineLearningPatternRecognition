{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce94834b",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Numerical Optimization\n",
    "The Logistic Regression model is obtained by **minimizing the average cross-entropy between the model predictions and the observed labels**. As we have seen, this corresponds also to a **Maximum Likelihood solution for the observed labels**, or to **minimizing the average Logistic Loss function.** See the next section for all the theory related to Logistic Regression, here we just report the final formulas for the three objective functions:\n",
    "- log-likelihood: $l(\\mathbf{w}, b) = \\sum_{i=1}^{n} \\left[c_i \\log y_i + (1 - c_i) \\log(1 - y_i)\\right] \\rightarrow$ GOAL: maximize $l(\\mathbf{w}, b)$ wrt $\\mathbf{w}, b$.\n",
    "\n",
    "- average cross-entroy: $\\mathcal{J}(\\mathbf{w}, b) = -l(\\mathbf{w}, b) = - \\sum_{i=1}^{n} \\left[c_i \\log y_i + (1 - c_i) \\log(1 - y_i)\\right] \\rightarrow$ GOAL: minimize $\\mathcal{J}(\\mathbf{w}, b)$ wrt $\\mathbf{w}, b$.\n",
    "\n",
    "- average Logistic Loss function: $\\mathcal{J}(\\mathbf{w}, b) = \\sum_{i=1}^{n} \\log(1 + e^{-z_i(\\mathbf{w}^T \\mathbf{x}_i + b)})  \\rightarrow$ GOAL: minimize $\\mathcal{J}(\\mathbf{w}, b)$ wrt $\\mathbf{w}, b$.\n",
    "\n",
    "While for Gaussian models closed form expressions are available for the\n",
    "ML solutions, this is not the case for Logistic Regression. This means, **we can't just solve system of equations to find the optimal parameters**. This is because the **sigmoid** function involved in binary Logistic regression (and the **softmax** function involved in multiclass Logistic regression) make the loss function nonlinear and non-convex in general<br>\n",
    "Therefore, we turn to numerical optimization\n",
    "to find the maximizer of the class likelihoods, or, equivalently, the minimizer of the average cross-entropy or average Logistic Loss function. <br>\n",
    "Numerical optimization algorithms look for the minimum of a function $f(x)$ with respect to the argument\n",
    "$x$. Here we briefly explain two methods, the second one will be the one adopted by us:\n",
    "### 1) Gradient Descent (GD)\n",
    "with this iterative method, at each iteration $t$ we compute $x_{t+1}$ from $x_{t}$:\n",
    "- we compute the gradient $\\nabla f(x_t)$ of the loss function with respect to the current parameters $x_t$.\n",
    "- we then update the parameters by moving in the **opposite direction of the gradient** (this is done by multiplying the gradient by $-1$), scaled by a learning rate (also called step) $\\alpha_t$:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\alpha_t \\nabla f(x_t)\n",
    "$$\n",
    "\n",
    "Under the assumptions that the step becomes lower when iterations pass $\\left( \\alpha_t \\rightarrow 0 \\right)$ and that the whole sum of all the steps at each iteration is unbounded $\\left( \\sum_{t=1}^{\\infty} \\alpha_t \\rightarrow \\infty \\right)$, we are certain that the algorithm converges to a **local minimum** of $f$.\n",
    "#### Pros of GD\n",
    "- Easy to implement  \n",
    "- Low memory usage\n",
    "\n",
    "#### Cons of GD\n",
    "- Can be **very slow to converge**  \n",
    "- Sensitive to choice of learning rate  \n",
    "- Struggles with ill-conditioned loss surfaces\n",
    "\n",
    "### 2) L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)\n",
    "L-BFGS is a more advanced optimization algorithm that uses an **approximate second-order method**. Instead of relying solely on the gradient, it also uses curvature information so second order information, such as the Hessian of the function, from previous iterations to guide the search more efficiently. <br>\n",
    "\n",
    "#### Pros of L-BFGS\n",
    "- Much **faster convergence** than gradient descent  \n",
    "- No need to compute or store the full Hessian (which would be $\\mathcal{O}(d^2)$)\n",
    "\n",
    "#### Cons of L-BFGS\n",
    "- Slightly more complex and higher per-iteration cost ($\\mathcal{O}(md)$, whereas GD has just $\\mathcal{O}(d)$)\n",
    "\n",
    "\n",
    "This second algorithm is the one we''l use and is implemented in `scipy` (requires importing `scipy.optimize`). We will use the `scipy.optimize.fmin_l_bfgs_b` interface to the numerical solver.\n",
    "\n",
    "`scipy.optimize.fmin_l_bfgs_b` requires at least 2 arguments (check the documentation for more details):\n",
    "\n",
    "* `func`: the function we want to minimize.\n",
    "* `x0`: the starting value for the algorithm.\n",
    "\n",
    "The L-BFGS algorithm requires computing the objective function and its gradient. To pass the gradient we have different options:\n",
    "\n",
    "* Through `func`: `func` should return a tuple `(f(x), \\nabla_x f(x))`.\n",
    "* Through the optional parameter `fprime`: `fprime` is a function computing the gradient. In this case, `func` should only return the objective value $f(x)$.\n",
    "* Let the implementation compute an approximated gradient: pass `approx_grad = True`. In this case, `func` should only return the objective value $f(x)$.\n",
    "\n",
    "The last option does not require writing a function that computes the gradient, as an approximation of the gradient is automatically obtained through finite differences. While this has the advantage that we do not need to derive and implement the gradient, it has two drawbacks:\n",
    "\n",
    "* The gradient computed through finite differences may not be accurate enough.\n",
    "* The computations are much more expensive, since we need to evaluate the objective function a number of times at least $D$, where $D$ is the size of $x$, at each iteration, and if we want a more accurate approximation of the gradient we may need to evaluate $f$ many more times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851cbf6",
   "metadata": {},
   "source": [
    "As an example, we now try to apply the L-BFGS to the function:\n",
    "$$\n",
    "f(y, z) = (y + 3)^2 + \\sin(y) + (z + 1)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21fcc876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b17704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mimimum of f(y,z) is at x_min = [-2.57747138 -0.99999927]\n",
      "f(y_min,z_min) = f(x_min) = -0.3561430123647649\n",
      "Optimization info: {'grad': array([-1.49324998e-06,  1.46549439e-06]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 21, 'nit': 6, 'warnflag': 0}\n"
     ]
    }
   ],
   "source": [
    "#implementation of function f(y,z)\n",
    "def f(x):\n",
    "    #x is an numpy array of shape (2,)\n",
    "    #x[0] is y and x[1] is z\n",
    "    #te function returns the value of f(y,z) = (y+3)^2 + sin(y) + (z+1)^2\n",
    "    y = x[0]\n",
    "    z = x[1]\n",
    "\n",
    "    return (y+3)**2 + np.sin(y) + (z+1)**2\n",
    "\n",
    "\n",
    "#Now we call scipy.optimize.fmin_l_bfgs_b passing the function f and the initial x0 which is a numpy array of values [0,0] and approx_grad = True\n",
    "x_0 = np.array([0, 0])\n",
    "\n",
    "#x_min is the minimum point of the function f\n",
    "#f_min is the value of the function f at the minimum point x_min\n",
    "#d is a dictionary with information about the optimization process\n",
    "x_min, f_min, d = opt.fmin_l_bfgs_b(f, x_0, approx_grad=True)\n",
    "\n",
    "print(f\"Mimimum of f(y,z) is at x_min = {x_min}\")\n",
    "print(f\"f(y_min,z_min) = f(x_min) = {f_min}\")\n",
    "print(f\"Optimization info: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54d7da",
   "metadata": {},
   "source": [
    "We can check the number of times the function $f$ was called with this second approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ef5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function f called 21 times\n"
     ]
    }
   ],
   "source": [
    "print(f\"function f called {d['funcalls']} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f85984",
   "metadata": {},
   "source": [
    "We can also provide an explicit gradient, in this case te function is very simple so rather than approximate it we can just compute the two partial derivatives explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ecc4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mimimum of f(y,z) is at x_min = [-2.57747137 -0.99999927]\n",
      "f(y_min,z_min) = f(x_min) = -0.3561430123647611\n",
      "Optimization info: {'grad': array([-1.50318729e-06,  1.46120529e-06]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 7, 'nit': 6, 'warnflag': 0}\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "    #x is an numpy array of shape (2,)\n",
    "    #x[0] is y and x[1] is z\n",
    "    #te function returns the value of f(y,z) = (y+3)^2 + sin(y) + (z+1)^2\n",
    "    y = x[0]\n",
    "    z = x[1]\n",
    "\n",
    "    #provide explicit gradient of f\n",
    "    y_derivative = 2*(y+3) + np.cos(y)\n",
    "    z_derivative = 2*(z+1)\n",
    "\n",
    "    #gradient has shape (2,)\n",
    "    return (y+3)**2 + np.sin(y) + (z+1)**2, np.array([y_derivative, z_derivative])\n",
    "\n",
    "#Now we call scipy.optimize.fmin_l_bfgs_b passing the function f and the initial x0 which is a numpy array of values [0,0] and pass the explicitly computed  gradient of f\n",
    "x_0 = np.array([0, 0])\n",
    "\n",
    "#x_min is the minimum point of the function f\n",
    "#f_min is the value of the function f at the minimum point x_min\n",
    "#d is a dictionary with information about the optimization process\n",
    "x_min, f_min, d = opt.fmin_l_bfgs_b(f1, x_0)\n",
    "\n",
    "print(f\"Mimimum of f(y,z) is at x_min = {x_min}\")\n",
    "print(f\"f(y_min,z_min) = f(x_min) = {f_min}\")\n",
    "print(f\"Optimization info: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecf759",
   "metadata": {},
   "source": [
    "We can check the number of times the function $f$ was called with this second approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3767e747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function f called 7 times\n"
     ]
    }
   ],
   "source": [
    "print(f\"function f called {d['funcalls']} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87c2ec",
   "metadata": {},
   "source": [
    "Whereas with the first approach (i.e. let scipy automatically approximate the gradient) $f$ was called 21 times, in this second case it's called just 7 times, a third! <br>\n",
    "So, we can say that te automatic numerical approximation of the gradient is significantly more expensive, and the cost becomes\n",
    "relatively worse when the dimensionality of the domain of f increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84033454",
   "metadata": {},
   "source": [
    "## Binary logistic regression\n",
    "It's important to underline that, althougth it's in the name, this is a classification model (like the GGM), and not a regression model (such as linear regression). <br>\n",
    "We introduce this other model because in this case our goal becomes very different from when we studied the GGM. If, when we implemented and applied the GGM, our goal was to model the distribution of the observed samples $X \\mid C$, when using Logistic Regression we want to directly model the class posteriors distribution $C \\mid X$. <br>\n",
    "Using again a generative approach, the Posterior probability for class $h_1$ can be computed from the modeled priors and the modeled class conditional densities, using the Bayes' Theorem:\n",
    "$$\n",
    "P(C = h_1 \\mid \\mathbf{x}) = \\frac{f_{\\mathbf{X} \\mid C}(\\mathbf{x} \\mid C = h_1) P(C = h_1)}{\\sum_{i=0}^{K-1} f_{\\mathbf{X} \\mid C}(\\mathbf{x} | C = c_i) P(C = c_i)} = \\frac{f_{\\mathbf{X} \\mid C}(\\mathbf{x} \\mid C = h_1) P(C = h_1)}{ f_{\\mathbf{X} \\mid C}(\\mathbf{x} | C = h_0) P(C = h_0) + f_{\\mathbf{X} \\mid C}(\\mathbf{x} | C = h_1) P(C = h_1)}\n",
    "$$\n",
    "This can be rewritten as:\n",
    "$$\n",
    "P(C = h_1 \\mid \\mathbf{x}) = \\frac{1}{1+ e^{-s(\\mathbf{x})}} = \\sigma(s(\\mathbf{x}))\n",
    "$$\n",
    "Where:\n",
    "- $s(\\mathbf{x})$ is the score\n",
    "\n",
    "- $\\sigma (t) = \\frac{1}{1+ e^{-t}}$ is called **sigmoid**/logistic function. It has $\\lim_{t \\to - \\infty} \\sigma(t) = 0$ and $\\lim_{t \\to \\infty} \\sigma(t) = 1$. An important property of this function is that: $1 - \\sigma (t) = \\sigma (-t)$ <br>\n",
    "<img src=\"sigmoid.png\" alt=\"image.png\" style=\"background-color: #ADD8E6; width: 400px; height: 200px;\">\n",
    "\n",
    "For the score, we can use the **log-posterior ratio**, given in the log domain by the sum of the log-likelihood ratio and the prior log odds, as we already did with the GGM:\n",
    "$$\n",
    "s(\\mathbf{x}) = llr(\\mathbf{x}) + \\text{log}\\frac{\\pi}{1+\\pi} = \\text{log} \\frac{f_{\\mathbf{X} \\mid C}(\\mathbf{x} | C = h_1)}{f_{\\mathbf{X} \\mid C}(\\mathbf{x} | C = h_0)} + \\text{log}\\frac{\\pi}{1+\\pi}\n",
    "$$\n",
    "In this first phase, we can impose and use **linear classification rules**, so the score can be rewritten as:\n",
    "$$\n",
    "s(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
    "$$\n",
    "This expression is linear wrt features $\\mathbf{x}$. $\\mathbf{w}$ is the weigth vector and is **orthogonal** to the linear decision surface, $b$ is a scalar bias, which also absorbes information from the Priors. $s(\\mathbf{x})$ is positive for samples of class $h_1$ and negative for samples of class $h_0$. The equation:\n",
    "$$\n",
    "s(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b = 0\n",
    "$$\n",
    "defines the linear decision surface, so the linear hyperplane, which separates our classes. <br>\n",
    "So, to sum up all of this, we can write:\n",
    "$$\n",
    "P(C = h_1 \\mid \\mathbf{x}, \\mathbf{w}, b) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)\n",
    "$$\n",
    "Since we're speaking about probabilities and we just have two classes, we can also write the posterior for the other class, $h_0$, as:\n",
    "$$\n",
    "P(C = h_0 \\mid \\mathbf{x}, \\mathbf{w}, b) = 1 - P(C = h_1 \\mid \\mathbf{x}, \\mathbf{w}, b) = 1 - \\sigma(\\mathbf{w}^T \\mathbf{x} + b) = \\sigma(- \\mathbf{w}^T \\mathbf{x} + b)\n",
    "$$\n",
    "So, it's clear that we cannot compute these posteriors without knowing the model parameters, $(\\mathbf{w}, b)$. <br>\n",
    "Now, assuming we have a labeled training dataset $\\mathcal{D} = [(\\mathbf{x}_1, c_1), \\ldots, (\\mathbf{x}_n, c_n)]$ where classes are independently distributed, we can express the likelihood for the observed labels as\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, b) = P(C_1 = c_1, \\ldots, C_n = c_n | \\mathbf{x}_1, \\ldots, \\mathbf{x}_n, \\mathbf{w}, b) = \\prod_{i=1}^{n} P(C_i = c_i | \\mathbf{x}_i, \\mathbf{w}, b)\n",
    "$$\n",
    "\n",
    "We can thus apply a **ML approach** to estimate the model parameters that best describe the observed labels $(c_1, \\ldots, c_n)$. So we want to find the value of $\\mathbf{w}$ and b that maximize the likelihood of our training labels. <br>\n",
    "We assume that the classes $h_1, h_0$ have labels 1 and 0 respectively. Also, let $y_i = P(C_i = 1 | \\mathbf{x}_i, \\mathbf{w}, b) = \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b)$. It follows that $P(C_i = 0 | \\mathbf{x}_i, \\mathbf{w}, b) = 1 - y_i$. <br>\n",
    "So, the distribution for $C_i | \\mathbf{x}_i, \\mathbf{w}, b$ is a Bernoulli distribution:\n",
    "\n",
    "$$\n",
    "C_i | \\mathbf{x}_i, \\mathbf{w}, b \\sim \\text{Ber}(\\sigma(\\mathbf{w}^T \\mathbf{x}_i + b)) = \\text{Ber}(y_i)\n",
    "$$\n",
    "\n",
    "We can thus rewrite the likelihood using the Bernoulli formula for the density:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, b) = \\prod_{i=1}^{n} y_i^{c_i} (1 - y_i)^{(1 - c_i)}\n",
    "$$\n",
    "\n",
    "\n",
    "As always working with the log-likelihood is more practical, so we have\n",
    "\n",
    "$$\n",
    "l(\\mathbf{w}, b) = \\sum_{i=1}^{n} [c_i \\log y_i + (1 - c_i) \\log(1 - y_i)] = \\sum_{i=1}^{n} [c_i \\log \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) \\ + (1 - c_i) \\log \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) \\right)]\n",
    "$$\n",
    "\n",
    "\n",
    "Our goal is the maximization of $l$ with respect to $\\mathbf{w}$ and $b$.\n",
    "As briefly said before, we can do this but also follow other two approaches involving the minimization of the average cross-entropy or the minimization of the average Logistic Loss function. <br>\n",
    "The Average cross-entropy measures how good are the predictions made by te Recognizer, versus the actual labels present in the training data and it's just the negative of the likelihood:\n",
    "$$\n",
    "\\mathcal{J}(\\mathbf{w}, b) = - \\sum_{i=1}^{n} [c_i \\log y_i + (1 - c_i) \\log(1 - y_i)] = - \\sum_{i=1}^{n} [c_i \\log \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) \\ + (1 - c_i) \\log \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) \\right)]\n",
    "$$\n",
    "Te Logistic Loss function is just another way to see all of this, although from an algebric standpoint it's just equivalent to the average cross-entropy after some semplifications and a substitution. As a matter of facts, let:\n",
    "$$\n",
    "z_i = 2c_i - 1 \\implies\n",
    "\\begin{cases}\n",
    "    z_i = 2* 0 - 1 = -1 & \\text{if } c_i = 0 \\\\\n",
    "    z_i = 2 * 1 - 1 = 1  & \\text{if } c_i = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "If we introduce this substitution and we explicitly write the sigmoid formula we can obtain:\n",
    "$$\n",
    "\\mathcal{J}(\\mathbf{w}, b)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001cbed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
