{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce94834b",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Numerical Optimization\n",
    "The Logistic Regression model is obtained by **minimizing the average cross-entropy between the model predictions and the observed labels**. As we have seen, this corresponds also to a **Maximum Likelihood solution for the observed labels**, or to **minimizing the average Logistic Loss function.** See the next section for all the theory related to Logistic Regression, here we just report the final formulas for the three objective functions:\n",
    "- log-likelihood: $l(\\mathbf{w}, b) = \\sum_{i=1}^{n} \\left[c_i \\log y_i + (1 - c_i) \\log(1 - y_i)\\right] \\rightarrow$ GOAL: maximize $l(\\mathbf{w}, b)$ wrt $\\mathbf{w}, b$.\n",
    "\n",
    "- average cross-entroy: $\\mathcal{J}(\\mathbf{w}, b) = -l(\\mathbf{w}, b) = - \\sum_{i=1}^{n} \\left[c_i \\log y_i + (1 - c_i) \\log(1 - y_i)\\right] \\rightarrow$ GOAL: minimize $\\mathcal{J}(\\mathbf{w}, b)$ wrt $\\mathbf{w}, b$.\n",
    "\n",
    "- average Logistic Loss function: $\\mathcal{J}(\\mathbf{w}, b) = \\sum_{i=1}^{n} \\log(1 + e^{-z_i(\\mathbf{w}^T \\mathbf{x}_i + b)})  \\rightarrow$ GOAL: minimize $\\mathcal{J}(\\mathbf{w}, b)$ wrt $\\mathbf{w}, b$.\n",
    "\n",
    "While for Gaussian models closed form expressions are available for the\n",
    "ML solutions, this is not the case for Logistic Regression. This means, **we can't just solve system of equations to find the optimal parameters**. This is because the **sigmoid** function involved in binary Logistic regression (and the **softmax** function involved in multiclass Logistic regression) make the loss function nonlinear and non-convex in general<br>\n",
    "Therefore, we turn to numerical optimization\n",
    "to find the maximizer of the class likelihoods, or, equivalently, the minimizer of the average cross-entropy or average Logistic Loss function. <br>\n",
    "Numerical optimization algorithms look for the minimum of a function $f(x)$ with respect to the argument\n",
    "$x$. Here we briefly explain two methods, the second one will be the one adopted by us:\n",
    "### 1) Gradient Descent (GD)\n",
    "with this iterative method, at each iteration $t$ we compute $x_{t+1}$ from $x_{t}$:\n",
    "- we compute the gradient $\\nabla f(x_t)$ of the loss function with respect to the current parameters $x_t$.\n",
    "- we then update the parameters by moving in the **opposite direction of the gradient** (this is done by multiplying the gradient by $-1$), scaled by a learning rate (also called step) $\\alpha_t$:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\alpha_t \\nabla f(x_t)\n",
    "$$\n",
    "\n",
    "Under the assumptions that the step becomes lower when iterations pass $\\left( \\alpha_t \\rightarrow 0 \\right)$ and that the whole sum of all the steps at each iteration is unbounded $\\left( \\sum_{t=1}^{\\infty} \\alpha_t \\rightarrow \\infty \\right)$, we are certain that the algorithm converges to a **local minimum** of $f$.\n",
    "#### Pros of GD\n",
    "- Easy to implement  \n",
    "- Low memory usage\n",
    "\n",
    "#### Cons of GD\n",
    "- Can be **very slow to converge**  \n",
    "- Sensitive to choice of learning rate  \n",
    "- Struggles with ill-conditioned loss surfaces\n",
    "\n",
    "### 2) L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)\n",
    "L-BFGS is a more advanced optimization algorithm that uses an **approximate second-order method**. Instead of relying solely on the gradient, it also uses curvature information so second order information, such as the Hessian of the function, from previous iterations to guide the search more efficiently. <br>\n",
    "\n",
    "#### Pros of L-BFGS\n",
    "- Much **faster convergence** than gradient descent  \n",
    "- No need to compute or store the full Hessian (which would be $\\mathcal{O}(d^2)$)\n",
    "\n",
    "#### Cons of L-BFGS\n",
    "- Slightly more complex and higher per-iteration cost ($\\mathcal{O}(md)$, whereas GD has just $\\mathcal{O}(d)$)\n",
    "\n",
    "\n",
    "This second algorithm is the one we''l use and is implemented in `scipy` (requires importing `scipy.optimize`). We will use the `scipy.optimize.fmin_l_bfgs_b` interface to the numerical solver.\n",
    "\n",
    "`scipy.optimize.fmin_l_bfgs_b` requires at least 2 arguments (check the documentation for more details):\n",
    "\n",
    "* `func`: the function we want to minimize.\n",
    "* `x0`: the starting value for the algorithm.\n",
    "\n",
    "The L-BFGS algorithm requires computing the objective function and its gradient. To pass the gradient we have different options:\n",
    "\n",
    "* Through `func`: `func` should return a tuple `(f(x), \\nabla_x f(x))`.\n",
    "* Through the optional parameter `fprime`: `fprime` is a function computing the gradient. In this case, `func` should only return the objective value $f(x)$.\n",
    "* Let the implementation compute an approximated gradient: pass `approx_grad = True`. In this case, `func` should only return the objective value $f(x)$.\n",
    "\n",
    "The last option does not require writing a function that computes the gradient, as an approximation of the gradient is automatically obtained through finite differences. While this has the advantage that we do not need to derive and implement the gradient, it has two drawbacks:\n",
    "\n",
    "* The gradient computed through finite differences may not be accurate enough.\n",
    "* The computations are much more expensive, since we need to evaluate the objective function a number of times at least $D$, where $D$ is the size of $x$, at each iteration, and if we want a more accurate approximation of the gradient we may need to evaluate $f$ many more times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851cbf6",
   "metadata": {},
   "source": [
    "As an example, we now try to apply the L-BFGS to the function:\n",
    "$$\n",
    "f(y, z) = (y + 3)^2 + \\sin(y) + (z + 1)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fcc876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b17704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mimimum of f(y,z) is at x_min = [-2.57747138 -0.99999927]\n",
      "f(y_min,z_min) = f(x_min) = -0.3561430123647649\n",
      "Optimization info: {'grad': array([-1.49324998e-06,  1.46549439e-06]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 21, 'nit': 6, 'warnflag': 0}\n"
     ]
    }
   ],
   "source": [
    "#implementation of function f(y,z)\n",
    "def f(x):\n",
    "    #x is an numpy array of shape (2,)\n",
    "    #x[0] is y and x[1] is z\n",
    "    #te function returns the value of f(y,z) = (y+3)^2 + sin(y) + (z+1)^2\n",
    "    y = x[0]\n",
    "    z = x[1]\n",
    "\n",
    "    return (y+3)**2 + np.sin(y) + (z+1)**2\n",
    "\n",
    "\n",
    "#Now we call scipy.optimize.fmin_l_bfgs_b passing the function f and the initial x0 which is a numpy array of values [0,0] and approx_grad = True\n",
    "x_0 = np.array([0, 0])\n",
    "\n",
    "#x_min is the minimum point of the function f\n",
    "#f_min is the value of the function f at the minimum point x_min\n",
    "#d is a dictionary with information about the optimization process\n",
    "x_min, f_min, d = opt.fmin_l_bfgs_b(f, x_0, approx_grad=True)\n",
    "\n",
    "print(f\"Mimimum of f(y,z) is at x_min = {x_min}\")\n",
    "print(f\"f(y_min,z_min) = f(x_min) = {f_min}\")\n",
    "print(f\"Optimization info: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54d7da",
   "metadata": {},
   "source": [
    "We can check the number of times the function $f$ was called with this second approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ef5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function f called 21 times\n"
     ]
    }
   ],
   "source": [
    "print(f\"function f called {d['funcalls']} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f85984",
   "metadata": {},
   "source": [
    "We can also provide an explicit gradient, in this case te function is very simple so rather than approximate it we can just compute the two partial derivatives explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ecc4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mimimum of f(y,z) is at x_min = [-2.57747137 -0.99999927]\n",
      "f(y_min,z_min) = f(x_min) = -0.3561430123647611\n",
      "Optimization info: {'grad': array([-1.50318729e-06,  1.46120529e-06]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 7, 'nit': 6, 'warnflag': 0}\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "    #x is an numpy array of shape (2,)\n",
    "    #x[0] is y and x[1] is z\n",
    "    #te function returns the value of f(y,z) = (y+3)^2 + sin(y) + (z+1)^2\n",
    "    y = x[0]\n",
    "    z = x[1]\n",
    "\n",
    "    #provide explicit gradient of f\n",
    "    y_derivative = 2*(y+3) + np.cos(y)\n",
    "    z_derivative = 2*(z+1)\n",
    "\n",
    "    #gradient has shape (2,)\n",
    "    return (y+3)**2 + np.sin(y) + (z+1)**2, np.array([y_derivative, z_derivative])\n",
    "\n",
    "#Now we call scipy.optimize.fmin_l_bfgs_b passing the function f and the initial x0 which is a numpy array of values [0,0] and pass the explicitly computed  gradient of f\n",
    "x_0 = np.array([0, 0])\n",
    "\n",
    "#x_min is the minimum point of the function f\n",
    "#f_min is the value of the function f at the minimum point x_min\n",
    "#d is a dictionary with information about the optimization process\n",
    "x_min, f_min, d = opt.fmin_l_bfgs_b(f1, x_0)\n",
    "\n",
    "print(f\"Mimimum of f(y,z) is at x_min = {x_min}\")\n",
    "print(f\"f(y_min,z_min) = f(x_min) = {f_min}\")\n",
    "print(f\"Optimization info: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecf759",
   "metadata": {},
   "source": [
    "We can check the number of times the function $f$ was called with this second approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3767e747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function f called 7 times\n"
     ]
    }
   ],
   "source": [
    "print(f\"function f called {d['funcalls']} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87c2ec",
   "metadata": {},
   "source": [
    "Whereas with the first approach (i.e. let scipy automatically approximate the gradient) $f$ was called 21 times, in this second case it's called just 7 times, a third! <br>\n",
    "So, we can say that te automatic numerical approximation of the gradient is significantly more expensive, and the cost becomes\n",
    "relatively worse when the dimensionality of the domain of f increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84033454",
   "metadata": {},
   "source": [
    "## Binary logistic regression\n",
    "It's important to underline that, althougth it's in the name, this is a classification model (like the GGM), and not a regression model (such as linear regression). <br>\n",
    "We introduce this other model because in this case our goal becomes very different from when we studied the GGM. If, when we implemented and applied the GGM, our goal was to model the distribution of the observed samples $X \\mid C$, when using Logistic Regression we want to directly model the class posteriors distribution $C \\mid X$. <br>\n",
    "Using again a generative approach, the Posterior probability for class $h_1$ can be computed from the modeled priors and the modeled class conditional densities, using the Bayes' Theorem:\n",
    "$$\n",
    "P(C = h_1 \\mid \\mathbf{x}) = \\frac{f_{\\mathbf{X} \\mid C}(\\mathbf{x} \\mid C = h_1) P(C = h_1)}{\\sum_{i=0}^{K-1} f_{\\mathbf{X} \\mid C}(\\mathbf{x} | C = c_i) P(C = c_i)} = \\frac{f_{\\mathbf{X} \\mid C}(\\mathbf{x} \\mid C = h_1) P(C = h_1)}{ f_{\\mathbf{X} \\mid C}(\\mathbf{x} | C = h_0) P(C = h_0) + f_{\\mathbf{X} \\mid C}(\\mathbf{x} | C = h_1) P(C = h_1)}\n",
    "$$\n",
    "This can be rewritten as:\n",
    "$$\n",
    "P(C = h_1 \\mid \\mathbf{x}) = \\frac{1}{1+ e^{-s(\\mathbf{x})}} = \\sigma(s(\\mathbf{x}))\n",
    "$$\n",
    "Where:\n",
    "- $s(\\mathbf{x})$ is the score\n",
    "\n",
    "- $\\sigma (t) = \\frac{1}{1+ e^{-t}}$ is called **sigmoid**/logistic function. It has $\\lim_{t \\to - \\infty} \\sigma(t) = 0$ and $\\lim_{t \\to \\infty} \\sigma(t) = 1$. An important property of this function is that: $1 - \\sigma (t) = \\sigma (-t)$ <br>\n",
    "<img src=\"sigmoid.png\" alt=\"image.png\" style=\"background-color: #ADD8E6; width: 400px; height: 200px;\">\n",
    "\n",
    "For the score, we can use the **log-posterior ratio**, given in the log domain by the sum of the log-likelihood ratio and the prior log odds, as we already did with the GGM:\n",
    "$$\n",
    "s(\\mathbf{x}) = llr(\\mathbf{x}) + \\text{log}\\frac{\\pi}{1+\\pi} = \\text{log} \\frac{f_{\\mathbf{X} \\mid C}(\\mathbf{x} | C = h_1)}{f_{\\mathbf{X} \\mid C}(\\mathbf{x} | C = h_0)} + \\text{log}\\frac{\\pi}{1-\\pi}\n",
    "$$\n",
    "In this first phase, we can impose and use **linear classification rules**, so the score can be rewritten as:\n",
    "$$\n",
    "s(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
    "$$\n",
    "This expression is linear wrt features $\\mathbf{x}$. $\\mathbf{w}$ is the weigth vector and is **orthogonal** to the linear decision surface, $b$ is a scalar bias, which also absorbes information from the Priors. $s(\\mathbf{x})$ is positive for samples of class $h_1$ and negative for samples of class $h_0$. The equation:\n",
    "$$\n",
    "s(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b = 0\n",
    "$$\n",
    "defines the linear decision surface, so the linear hyperplane, which separates our classes. <br>\n",
    "So, to sum up all of this, we can write:\n",
    "$$\n",
    "P(C = h_1 \\mid \\mathbf{x}, \\mathbf{w}, b) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)\n",
    "$$\n",
    "Since we're speaking about probabilities and we just have two classes, we can also write the posterior for the other class, $h_0$, as:\n",
    "$$\n",
    "P(C = h_0 \\mid \\mathbf{x}, \\mathbf{w}, b) = 1 - P(C = h_1 \\mid \\mathbf{x}, \\mathbf{w}, b) = 1 - \\sigma(\\mathbf{w}^T \\mathbf{x} + b) = \\sigma(- \\mathbf{w}^T \\mathbf{x} + b)\n",
    "$$\n",
    "So, it's clear that we cannot compute these posteriors without knowing the model parameters, $(\\mathbf{w}, b)$. <br>\n",
    "Now, assuming we have a labeled training dataset $\\mathcal{D} = [(\\mathbf{x}_1, c_1), \\ldots, (\\mathbf{x}_n, c_n)]$ where classes are independently distributed, we can express the likelihood for the observed labels as\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, b) = P(C_1 = c_1, \\ldots, C_n = c_n | \\mathbf{x}_1, \\ldots, \\mathbf{x}_n, \\mathbf{w}, b) = \\prod_{i=1}^{n} P(C_i = c_i | \\mathbf{x}_i, \\mathbf{w}, b)\n",
    "$$\n",
    "\n",
    "We can thus apply a **ML approach** to estimate the model parameters that best describe the observed labels $(c_1, \\ldots, c_n)$. So we want to find the value of $\\mathbf{w}$ and b that maximize the likelihood of our training labels. <br>\n",
    "We assume that the classes $h_1, h_0$ have labels 1 and 0 respectively. Also, let $y_i = P(C_i = 1 | \\mathbf{x}_i, \\mathbf{w}, b) = \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b)$. It follows that $P(C_i = 0 | \\mathbf{x}_i, \\mathbf{w}, b) = 1 - y_i$. <br>\n",
    "So, the distribution for $C_i | \\mathbf{x}_i, \\mathbf{w}, b$ is a Bernoulli distribution:\n",
    "\n",
    "$$\n",
    "C_i | \\mathbf{x}_i, \\mathbf{w}, b \\sim \\text{Ber}(\\sigma(\\mathbf{w}^T \\mathbf{x}_i + b)) = \\text{Ber}(y_i)\n",
    "$$\n",
    "\n",
    "We can thus rewrite the likelihood using the Bernoulli formula for the density:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, b) = \\prod_{i=1}^{n} y_i^{c_i} (1 - y_i)^{(1 - c_i)}\n",
    "$$\n",
    "\n",
    "\n",
    "As always working with the log-likelihood is more practical, so we have\n",
    "\n",
    "$$\n",
    "l(\\mathbf{w}, b) = \\sum_{i=1}^{n} [c_i \\log y_i + (1 - c_i) \\log(1 - y_i)] = \\sum_{i=1}^{n} [c_i \\log \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) \\ + (1 - c_i) \\log \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) \\right)] \\tag{1 way}\n",
    "$$\n",
    "This is the first way to write the L.R. Objective function. There are other three ways. All of this formulas have a tag on the right which tells you they are all the ways to write the objective function. <br>\n",
    "Our goal is the maximization of $l$ with respect to $\\mathbf{w}$ and $b$.\n",
    "As briefly said before, we can do this but also follow other two approaches involving the minimization of the average cross-entropy or the minimization of the average Logistic Loss function. <br>\n",
    "The Average cross-entropy measures how good are the predictions made by te Recognizer, versus the actual labels present in the training data and it's just the negative of the likelihood:\n",
    "$$\n",
    "\\mathcal{J}(\\mathbf{w}, b) = - \\sum_{i=1}^{n} [c_i \\log y_i + (1 - c_i) \\log(1 - y_i)] = \\sum_{i=1}^{n} - [c_i \\log \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) \\ + (1 - c_i) \\log \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) \\right)] \\tag{2 way}\n",
    "$$\n",
    "Some numerical issues that may arise when explicitly computing sigmoids followed by natural logarithm, so we can rewrite the formula for the average cross entropy by excplicitly plugging in the sigmoid formula and observing that:\n",
    "$$\n",
    "\\log \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) = -\\log (1 + e^{-\\mathbf{w}^T \\mathbf{x}_i - b}) \\\\ \\\\\n",
    "\\log (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b)) = -\\log \\sigma(-\\mathbf{w}^T \\mathbf{x}_i - b) = -\\log (1 + e^{\\mathbf{w}^T \\mathbf{x}_i + b})\n",
    "$$\n",
    "Thus we can also write the average cross-entropy this way:\n",
    "$$\n",
    "\\mathcal{J}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[c_i \\log (1 + e^{-\\mathbf{w}^T \\mathbf{x}_i - b}) + (1 - c_i) \\log (1 + e^{\\mathbf{w}^T \\mathbf{x}_i + b})\\right] \\tag{3 way}\n",
    "$$\n",
    "\n",
    "The Logistic Loss function is just another way to see all of this, although from an algebric standpoint it's just equivalent to the average cross-entropy after some semplifications and a substitution. As a matter of facts, let:\n",
    "$$\n",
    "z_i = 2c_i - 1 \\implies\n",
    "\\begin{cases}\n",
    "    z_i = 2* 0 - 1 = -1 & \\text{if } c_i = 0 \\\\\n",
    "    z_i = 2 * 1 - 1 = 1  & \\text{if } c_i = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "So, considering just a single sample $\\mathbf{x}_i$, we can see that the binary cross entropy for predicted label $y_i$ and an actual label $c_i$ is:\n",
    "$$\n",
    "\\mathcal{H}(c_i, y_i) = - [c_i \\log \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) \\ + (1 - c_i) \\log \\left(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) \\right)] =\n",
    "\\begin{cases}\n",
    "    - \\log \\sigma (\\mathbf{w}^T \\mathbf{x}_i + b) & \\text{if } c_i = 1 \\iff z_i = 1 \\\\\n",
    "    - \\log \\sigma (- \\mathbf{w}^T \\mathbf{x}_i + b) & \\text{if } c_i = 0 \\iff z_i = -1\n",
    "\\end{cases} =\n",
    "- \\log \\sigma (z_i \\cdot (\\mathbf{w}^T \\mathbf{x}_i + b))\n",
    "$$\n",
    "Thus we can rewrite the average cross entropy across all samples as:\n",
    "$$\n",
    "\\mathcal{J}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i}^{n} \\mathcal{H}(c_i, y_i) = \\frac{1}{n} \\sum_{i}^{n} \\left[ - \\log \\sigma (z_i \\cdot (\\mathbf{w}^T \\mathbf{x}_i + b)) \\right]\n",
    "$$\n",
    "If we explicitly write the sigmoid formula and make some semplifications we get:\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i}^{n} \\log \\left(  1 + e^{-z_i \\cdot (\\mathbf{w}^T \\mathbf{x}_i + b)} \\right) \\tag{4 way}\n",
    "$$\n",
    "This is called **average Logistic Loss function**. It measures the empirical risk which is defined by the cost to pay due to missclassification of samples, or even due uncertainty despite the classification is correct.<br>\n",
    "\n",
    "The last thing to talk about is **regularization**. In fact, if the classes are completely **separable** (i.e. we don't have samples from one class mixed with other from other classes), the L.R. solution is **undefined**, because we can aribtrarily decrease the Logistic Loss by arbitrarily increasing either $\\|\\mathbf{w}\\|$ or b, so the score $\\mathbf{w}^T \\mathbf{x}_i + b$ can get extremely high. This means the optimal solution is obtained where $\\|\\mathbf{w}\\| \\to \\infty$. In this case, to avoid having an undefined solution (and a potential freeze and/or crash of our program), we can manually impose a **norm penalty** to the objective function, so, when looking for the optimal solution, we'll have to respect this contraint as well and won't be able to increase the norm of $\\mathbf{w}$ more and more towards $\\infty$. \n",
    "The regularization term (norm penalty term) is composed like this:\n",
    "$$\n",
    "\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2\n",
    "$$\n",
    "It's important to note that $\\lambda$ is a **hyperparameter** of the model and its optimal value\n",
    "may be found with **cross-validation**. In fact, $\\lambda$ can’t be obtained by trying to\n",
    "minimize the objective function with respect to $\\lambda$ , as we would obtain $\\lambda = 0$ which would remove\n",
    "the regularization term from the equation. <br>\n",
    "So, we can apply regularization and rewrite the three objective functions (for now we don't rewrite the log likelihood anymore because it's the only one which needs to be maximized, and in this case $\\lambda$ would have to have the opposite sign wrt all the other cases..so we just write the two ways of expressing the average cross entropy, and the Logistic Loss function):\n",
    "$$\n",
    "\\mathcal{J}(\\mathbf{w}, b) = \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 - \\frac{1}{n} \\sum_{i=1}^{n} [c_i \\log \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) + (1 - c_i) \\log(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b))] \\quad (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(\\mathbf{w}, b) = \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 + \\frac{1}{n} \\sum_{i=1}^{n} \\log (1 + e^{-z_i(\\mathbf{w}^T \\mathbf{x}_i + b)}), \\quad z_i = \\begin{cases} 1 & \\text{if } c_i = 1 \\\\ -1 & \\text{if } c_i = 0 \\end{cases} \\quad (i.e. \\ z_i = 2c_i - 1) \\quad (2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(\\mathbf{w}, b) = \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 + \\frac{1}{n} \\sum_{i=1}^{n} [c_i \\log (1 + e^{-\\mathbf{w}^T \\mathbf{x}_i - b}) + (1 - c_i) \\log (1 + e^{\\mathbf{w}^T \\mathbf{x}_i + b})] \\quad (3)\n",
    "$$\n",
    "\n",
    "So, to summarize, the model will minimize the objective function and provide the best paramaters $(\\mathbf{w}, b)$ that lead to this this minimization. After this, we have a score, which in this case is linear wrt the features: $s(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b$. The decision rule will be thus given by the hyperplane: $s(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b = 0$, and so we can predict labels $1$ for scores $s(\\mathbf{x}_i)> 0$, and labels $0$ otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d4033",
   "metadata": {},
   "source": [
    "**Let's implement the Logistic Regression using (2) as objective function.** <br>\n",
    "First, let's import the dataset and apply the train-test split: <br>\n",
    "We will represent labels with 1 (iris versicolor) and 0 (iris virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9225defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4, 66)\n",
      "Validation set shape: (4, 34)\n",
      "Training set labels: [1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0]\n",
      "Validation set labels: [1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "from train_validation_split import splitTrainingValidation\n",
    "from mean_covariance import vcol, vrow\n",
    "\n",
    "def load_iris_binary():\n",
    "    D, L = sklearn.datasets.load_iris()['data'].T, sklearn.datasets.load_iris()['target']\n",
    "    D = D[:, L != 0] # We remove setosa from D\n",
    "    L = L[L!=0] # We remove setosa from L\n",
    "    L[L==2] = 0 # We assign label 0 to virginica (was label 2)\n",
    "    return D, L\n",
    "\n",
    "\n",
    "D, L = load_iris_binary()\n",
    "(DTR, LTR), (DVAL, LVAL) = splitTrainingValidation(2/3, D, L)\n",
    "print(f\"Training set shape: {DTR.shape}\")\n",
    "print(f\"Validation set shape: {DVAL.shape}\")\n",
    "print(f\"Training set labels: {LTR}\")\n",
    "print(f\"Validation set labels: {LVAL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10654056",
   "metadata": {},
   "source": [
    "- Function logreg_obj should receive a single numpy array v with shape (D+1,), where D is the\n",
    "dimensionality of the feature space (e.g. D = 4 for IRIS). v should pack all model parameters, i.e.\n",
    "v = [w, b]. Inside the function you can then unpack the array e.g. w, b = v[0:-1], v[-1].\n",
    "- The function logreg_obj needs to access also DTR, LTR and $\\lambda$, which are required to compute\n",
    "the objective function. So, we embed the objective function and its optimization inside a function trainLogReg that receives both DTR, LTR and a value for l. The objective function is defined inside trainLogReg,\n",
    "thus it can access the variables DTR and LTR in the outer function scope (i.e., those passed to\n",
    "trainLogReg)\n",
    "- The computation of $\\log(1 + e^{-z_i(\\mathbf{w}^T \\mathbf{x}_i + b)})$ can lead to numerical issues when $z_i(\\mathbf{w}^T \\mathbf{x}_i + b)$ is large, since the sum will make the contribution of the exponential term disappear. We can avoid the issue by using the `numpy.logaddexp` function, which computes:\n",
    "    $$\n",
    "    \\text{numpy.logaddexp}(a, b) = \\log(e^a + e^b)\n",
    "    $$\n",
    "    In our example, we need to compute `numpy.logaddexp(0, -z_i(\\mathbf{w}^T \\mathbf{x}_i + b))`.\n",
    "- Broadcasting can significantly speed-up the computations. You can compute a vector of “scores” $\\mathbf{S}$\n",
    "\n",
    "    $$\n",
    "    \\mathbf{S} = [(\\mathbf{w}^T \\mathbf{x}_1 + b) \\ldots (\\mathbf{w}^T \\mathbf{x}_n + b)]\n",
    "    $$\n",
    "\n",
    "    using simple matrix-vector multiplication: `S = (vcol(w).T @ DTR + b).ravel()`. Remember to reshape the result to a 1-D array (`.ravel()`). You can then multiply each element of the result-vector by the corresponding label: `-ZTR * S`, where `ZTR = 2 * LTR - 1`. `numpy.logaddexp` supports broadcasting so you can compute in a single shot all terms\n",
    "\n",
    "    $$\n",
    "    \\log \\left(1 + e^{-z_i(\\mathbf{w}^T \\mathbf{x}_i + b)}\\right)\n",
    "    $$\n",
    "\n",
    "    with `numpy.logaddexp(0, -ZTR * S)`.\n",
    "\n",
    "- To speed-up computations, it's useful to provide also the gradient of the function. We can express in vector form the derivatives with respect to the components of $\\mathbf{w}$ and $b$ as (notice that there is no minus in the exponential term $z_i(\\mathbf{w}^T \\mathbf{x}_i + b)$):\n",
    "\n",
    "    $$\n",
    "    \\nabla_{\\mathbf{w}} J = \\left[\\frac{\\partial J}{\\partial w_j}\\right] = \\lambda \\mathbf{w} + \\frac{1}{n} \\sum_{i=1}^{n} \\frac{-z_i}{1 + e^{z_i(\\mathbf{w}^T \\mathbf{x}_i + b)}} \\mathbf{x}_i\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{-z_i}{1 + e^{z_i(\\mathbf{w}^T \\mathbf{x}_i + b)}}\n",
    "    $$\n",
    "\n",
    "    where $d$ is the dimensionality of the feature vectors (`DTR.shape[0]`). These terms can be efficiently computed by computing the vector $\\mathbf{G}$:\n",
    "\n",
    "    $$\n",
    "    \\mathbf{G} = \\left[\\frac{-z_1}{1 + e^{z_1(\\mathbf{w}^T \\mathbf{x}_1 + b)}} \\ldots \\frac{-z_n}{1 + e^{z_n(\\mathbf{w}^T \\mathbf{x}_n + b)}}\\right]\n",
    "    $$\n",
    "\n",
    "    so that\n",
    "\n",
    "    $$\n",
    "    \\nabla_{\\mathbf{w}} J = \\lambda \\mathbf{w} + \\frac{1}{n} \\sum_{i=1}^{n} G_i \\mathbf{x}_i\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^{n} G_i\n",
    "    $$\n",
    "\n",
    "    which can be computed, using broadcasting, from the vector of scores $\\mathbf{S}$ as `G = -ZTR / (1.0 + numpy.exp(ZTR * S))` (note: you may obtain overflow from the exponentiation due to well-classified samples, however you can safely ignore the issue since it will simply cause the corresponding $G_i$ becoming exactly 0 rather than almost 0, and this will not cause numerical issues). You can then compute all terms $\\mathbf{G} \\mathbf{x}_i$ with broadcasting: `(vrow(G) * DTR)`.\n",
    "\n",
    "    Remember that `scipy.optimize.fmin_l_bfgs_b` requires that you pack $\\nabla_{\\mathbf{w}} J$ and $\\frac{\\partial J}{\\partial b}$ in a single vector `vgrad = [$\\nabla_{\\mathbf{w}} J$, $\\frac{\\partial J}{\\partial b}$]`. You have to modify the function `logreg_obj` so that it returns both the objective and the gradient `vgrad` (and set `approx_grad = False` when calling the optimizer function).\n",
    "\n",
    "- $\\lambda$ is a hyper-parameter. As usual, we should employ a validation set to estimate good values of $\\lambda$. For this laboratory, we can simply try different values and see how this affects the performance.\n",
    "\n",
    "- The starting point does not significantly influence the result, since the objective function is convex (there may be slight differences, but should be very small). You can use as initial value an array of all zeros `x0 = numpy.zeros(DTR.shape[0] + 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdffe36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLogReg(DTR, LTR, l): \n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier using LTR as labels and DTR as data.\n",
    "    l is the regularization parameter (lambda).\n",
    "    \"\"\"\n",
    "\n",
    "    #LTR: ACTUAL labels -> they the c_i\n",
    "    #compute z_i = 2 * c_i - 1 -> ZTR = 2 * LTR - 1\n",
    "    #z_i is used in the Logistic Loss, not c_i\n",
    "    ZTR = 2 * LTR - 1\n",
    "\n",
    "\n",
    "    def logreg_obj(v):\n",
    "        \"\"\"\n",
    "        Compute the Objective function for logistic regression.\n",
    "        v is the vector of parameters in the form g. w, b = v[0:-1], v[-1]\n",
    "        Parameter:\n",
    "        - v: numpy array of shape (n_features + 1,)\n",
    "        Returns:\n",
    "        - f: float, the value of the objective function\n",
    "        \"\"\"\n",
    "        #extract w and b from v\n",
    "        w = v[:-1]  #weights\n",
    "        b = v[-1]   #bias   \n",
    "\n",
    "        #Now, the objective function is the Logistic Loss which is:\n",
    "        #f(w, b) = 0.5 * l * ||w||^2 + 1/n * sum_i=1^n log(1 + exp(-z_i * (w^T x_i + b)))\n",
    "        #so it's the sum of two terms:\n",
    "        #1. the regularization term: 0.5 * l * ||w||^2\n",
    "        #2. the average logistic loss term: 1/n * sum_i=1^n log(1 + exp(-z_i * (w^T x_i + b)))\n",
    "\n",
    "\n",
    "        #compute regularization term (= norm penality)\n",
    "        normPenalty = 0.5 * l * np.linalg.norm(w)**2\n",
    "\n",
    "        #to compute the term: log (1 + exp(-z_i * (w^T x_i + b)))\n",
    "        #we can exploit numpy broadcasting + logaddexp\n",
    "        #so first we build a vector of scores S = [(w^T x1 + b). . .(w^T xn + b)]\n",
    "        #then we reshape S to a 1-D array of shape (n_samples, 1)\n",
    "        S = (vcol(w).T @ DTR).ravel() + b\n",
    "\n",
    "        #then we exploit broadcasting to compute -z_i * (w^T x_i + b) -> in code it's -ZTR * S -> this term is te full exponent\n",
    "        exponent = -ZTR * S\n",
    "\n",
    "        #then we exploit logaddexp: since the log (1 + exp(-z_i * (w^T x_i + b))) can lead to numerical issues, use logaddexp\n",
    "        #logaddexp(a, b) = log(exp(a) + exp(b))\n",
    "        #logaddexp(0, exponent) = log(1 + exp(exponent)) -> this is all the second term of the objective function, we just need to compute the mean of it\n",
    "        logTerm = np.logaddexp(0, exponent)\n",
    "        avgLogTerms = logTerm.mean()\n",
    "\n",
    "        return normPenalty + avgLogTerms\n",
    "    \n",
    "    #so, the outer function has to invoke the scipy optimizer (fmin_l_bfgs_b) passing the function logreg_obj and the initial x0 which is a numpy array of all zeros and approx_grad = True\n",
    "    #in this version, we will not manually compute the gradient of the function, but we will use the approx_grad = True option of fmin_l_bfgs_b\n",
    "    #this will be slower, but we will not have to compute the gradient manually\n",
    "    xf = opt.fmin_l_bfgs_b(func = logreg_obj, x0 = np.zeros(DTR.shape[0]+1), approx_grad=True)\n",
    "    \n",
    "    #xf is a tuple with the first element being the minimum point of the function f, the second element being the value of the function f at the minimum point xf[0], and the third element being a dictionary with information about the optimization process\n",
    "    \n",
    "    #extract w_min, b_min\n",
    "    w_min = xf[0][:-1] #weights which minimize the objective function\n",
    "    b_min = xf[0][-1]  #bias which minimizes the objective function\n",
    "\n",
    "    #extract value of objective function in (w_min, b_min)\n",
    "    objMin = xf[1] \n",
    "    \n",
    "    return w_min, b_min, objMin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc073146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 0.001, w_min: [ 1.72973348  0.98291151 -4.54960055 -7.12479414], b_min: 20.952610898785906\n",
      "Value of the objective function at the minimum point: 0.11000090355257285\n",
      "lambda: 0.01, w_min: [ 0.55673154  0.35523499 -2.78054068 -2.76755649], b_min: 13.963100856019771\n",
      "Value of the objective function at the minimum point: 0.24296227288058903\n",
      "lambda: 0.1, w_min: [-0.19685817 -0.0212446  -1.10336574 -0.80719181], b_min: 8.175517115004444\n",
      "Value of the objective function at the minimum point: 0.4539406895961112\n",
      "lambda: 1.0, w_min: [-0.11040223 -0.02898697 -0.2478712  -0.14950474], b_min: 2.3109467688182845\n",
      "Value of the objective function at the minimum point: 0.6316436205357172\n"
     ]
    }
   ],
   "source": [
    "#retrieve the parameters which minimize the objective function\n",
    "#change lambda from 10^(-3) to 1.0, by incrementing it by a decimal order every time\n",
    "parameters_l = {} #key: lambda, value: (w_min, b_min)\n",
    "lambdas = [10**(-3), 10**(-2), 10**(-1), 1.0]\n",
    "for l in lambdas:\n",
    "    w_min, b_min, objMin = trainLogReg(DTR, LTR, l)\n",
    "    parameters_l[l] = (w_min, b_min)\n",
    "    print(f\"lambda: {l}, w_min: {w_min}, b_min: {b_min}\")\n",
    "    print(f\"Value of the objective function at the minimum point: {objMin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ddede2",
   "metadata": {},
   "source": [
    "A more computationally efficient way of solving this minimization problem is supplying to the scipy function for the L-BFGS algorithm a **manually computed gradient**, rather than making the function approximate it by itself. In fact, as we've already seen in the first part of the lab, this speeds up a lot the computations. \n",
    "The gradient of the objective function (2):\n",
    "$$\n",
    "\\mathcal{J}(\\mathbf{w}, b) = \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 + \\frac{1}{n} \\sum_{i=1}^{n} \\log (1 + e^{-z_i(\\mathbf{w}^T \\mathbf{x}_i + b)}), \\quad z_i = \\begin{cases} 1 & \\text{if } c_i = 1 \\\\ -1 & \\text{if } c_i = 0 \\end{cases} \\quad (i.e. \\ z_i = 2c_i - 1) \\quad (2)\n",
    "$$\n",
    "\n",
    "is this:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} J = \\left[\\frac{\\partial J}{\\partial w_1} \\cdots \\frac{\\partial J}{\\partial w_d}\\right] = \\lambda \\mathbf{w} + \\frac{1}{n} \\sum_{i=1}^{n} \\frac{-z_i}{1 + e^{z_i(\\mathbf{w}^T \\mathbf{x}_i + b)}} \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{-z_i}{1 + e^{z_i(\\mathbf{w}^T \\mathbf{x}_i + b)}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "So, we can modify the function `logreg_obj` to pack both the objective and the manually compute gradient inside a tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b95dddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLogReg(DTR, LTR, l, manual_grad=True): \n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier using LTR as labels and DTR as data.\n",
    "    l is the regularization parameter (lambda).\n",
    "    Params:\n",
    "    - DTR: numpy array of shape (n_features, n_samples)\n",
    "    - LTR: numpy array of shape (n_samples,)\n",
    "    - l: float, the regularization parameter\n",
    "    - manual_grad: boolean, if True, the gradient is computed manually, otherwise it is computed using approx_grad\n",
    "    \"\"\"\n",
    "\n",
    "    #LTR: ACTUAL labels -> they the c_i\n",
    "    #compute z_i = 2 * c_i - 1 -> ZTR = 2 * LTR - 1\n",
    "    #z_i is used in the Logistic Loss, not c_i\n",
    "    ZTR = 2 * LTR - 1\n",
    "\n",
    "\n",
    "    def logreg_obj(v):\n",
    "        \"\"\"\n",
    "        Compute the Objective function for logistic regression.\n",
    "        v is the vector of parameters in the form g. w, b = v[0:-1], v[-1]\n",
    "        Parameter:\n",
    "        - v: numpy array of shape (n_features + 1,)\n",
    "        Returns:\n",
    "        - f: float, the value of the objective function\n",
    "        \"\"\"\n",
    "        #extract w and b from v\n",
    "        w = v[:-1]  #weights\n",
    "        b = v[-1]   #bias   \n",
    "\n",
    "        #Now, the objective function is the Logistic Loss which is:\n",
    "        #f(w, b) = 0.5 * l * ||w||^2 + 1/n * sum_i=1^n log(1 + exp(-z_i * (w^T x_i + b)))\n",
    "        #so it's the sum of two terms:\n",
    "        #1. the regularization term: 0.5 * l * ||w||^2\n",
    "        #2. the average logistic loss term: 1/n * sum_i=1^n log(1 + exp(-z_i * (w^T x_i + b)))\n",
    "\n",
    "\n",
    "        #compute regularization term (= norm penality)\n",
    "        normPenalty = 0.5 * l * np.linalg.norm(w)**2\n",
    "\n",
    "        #to compute the term: log (1 + exp(-z_i * (w^T x_i + b)))\n",
    "        #we can exploit numpy broadcasting + logaddexp\n",
    "        #so first we build a vector of scores S = [(w^T x1 + b). . .(w^T xn + b)]\n",
    "        #then we reshape S to a 1-D array of shape (n_samples, 1)\n",
    "        S = (vcol(w).T @ DTR).ravel() + b\n",
    "\n",
    "        #then we exploit broadcasting to compute -z_i * (w^T x_i + b) -> in code it's -ZTR * S -> this term is te full exponent\n",
    "        exponent = -ZTR * S\n",
    "\n",
    "        #then we exploit logaddexp: since the log (1 + exp(-z_i * (w^T x_i + b))) can lead to numerical issues, use logaddexp\n",
    "        #logaddexp(a, b) = log(exp(a) + exp(b))\n",
    "        #logaddexp(0, exponent) = log(1 + exp(exponent)) -> this is all the second term of the objective function, we just need to compute the mean of it\n",
    "        logTerm = np.logaddexp(0, exponent)\n",
    "        avgLogTerms = logTerm.mean()\n",
    "\n",
    "\n",
    "        #if manual_grad is True, we compute the gradient manually\n",
    "        if (manual_grad):\n",
    "            #compute the vector G of the deerivatives of the log of the (1 + exp(-z_i * (w^T x_i + b)))\n",
    "            G = -ZTR / (1.0 + np.exp(ZTR * S))\n",
    "            #compute G_i * x_i with broadcasting\n",
    "            Gixi = vrow(G) * DTR\n",
    "            #compute derivatives of the objective function wrt to w: dobjF/dw\n",
    "            der_w = l * w.ravel() + Gixi.mean(axis=1) #mean over the n samples\n",
    "            #compute drivative of the objective function wrt to b: dobjF/db\n",
    "            der_b = G.mean() #mean over the n samples\n",
    "\n",
    "            #pack the two derivatives in a single array\n",
    "            #stack horizontally the array der_w and the scalar\n",
    "            #np.hstack is safer than np.array since it will not raise an error if the two arrays have different shapes\n",
    "            v_grad = np.hstack((der_w, der_b))\n",
    "\n",
    "            #return the value of the objective function and the gradient\n",
    "            return normPenalty + avgLogTerms, v_grad\n",
    "\n",
    "        #if manual_grad is False, we return only the value of the objective function\n",
    "        return normPenalty + avgLogTerms\n",
    "    \n",
    "    #so, the outer function has to invoke the scipy optimizer (fmin_l_bfgs_b) passing the function logreg_obj and the initial x0 which is a numpy array of all zeros and approx_grad = True\n",
    "    #in this version, we will not manually compute the gradient of the function, but we will use the approx_grad = True option of fmin_l_bfgs_b\n",
    "    #approx_grad = it depends on the value of manual_grad, if manual_grad is True, we will compute the gradient manually, otherwise we will use the approx_grad = True option of fmin_l_bfgs_b\n",
    "    xf = opt.fmin_l_bfgs_b(func = logreg_obj, x0 = np.zeros(DTR.shape[0]+1), approx_grad=not manual_grad)\n",
    "    \n",
    "    #xf is a tuple with the first element being the minimum point of the function f, the second element being the value of the function f at the minimum point xf[0], and the third element being a dictionary with information about the optimization process\n",
    "    \n",
    "    #extract w_min, b_min\n",
    "    w_min = xf[0][:-1] #weights which minimize the objective function\n",
    "    b_min = xf[0][-1]  #bias which minimizes the objective function\n",
    "\n",
    "    #extract value of objective function in (w_min, b_min)\n",
    "    objMin = xf[1] \n",
    "    \n",
    "    return w_min, b_min, objMin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6dcf96",
   "metadata": {},
   "source": [
    "Let's retieve the params with this new method which supplies also the manually compute gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fa51901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 0.001, w_min: [ 1.72973736  0.98290096 -4.54960598 -7.12478282], b_min: 20.952625079900397\n",
      "Value of the objective function at the minimum point: 0.11000090354987904\n",
      "lambda: 0.01, w_min: [ 0.55673227  0.35523454 -2.7805404  -2.76755686], b_min: 13.963096874829418\n",
      "Value of the objective function at the minimum point: 0.24296227288068295\n",
      "lambda: 0.1, w_min: [-0.19685795 -0.02124461 -1.10336579 -0.80719185], b_min: 8.175516101423055\n",
      "Value of the objective function at the minimum point: 0.4539406895957473\n",
      "lambda: 1.0, w_min: [-0.11040209 -0.0289869  -0.2478711  -0.14950473], b_min: 2.310945096712581\n",
      "Value of the objective function at the minimum point: 0.6316436205354083\n"
     ]
    }
   ],
   "source": [
    "#retrieve the parameters which minimize the objective function\n",
    "#change lambda from 10^(-3) to 1.0, by incrementing it by a decimal order every time\n",
    "parameters_l = {} #key: lambda, value: (w_min, b_min)\n",
    "lambdas = [10**(-3), 10**(-2), 10**(-1), 1.0]\n",
    "for l in lambdas:\n",
    "    w_min, b_min, objMin = trainLogReg(DTR, LTR, l, manual_grad=True)\n",
    "    parameters_l[l] = (w_min, b_min)\n",
    "    print(f\"lambda: {l}, w_min: {w_min}, b_min: {b_min}\")\n",
    "    print(f\"Value of the objective function at the minimum point: {objMin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c2434",
   "metadata": {},
   "source": [
    "Now, we've compute the model parameters $(\\mathbf{w}, b)$. <br>\n",
    "So, we just jave to compute the posterior log-probability, by computing for each **validation** (**not training!**) sample $\\mathbf{x}_t$:\n",
    "$$\n",
    "s(\\mathbf{x}_t) = \\mathbf{w}^T \\mathbf{x}_t + b\n",
    "$$\n",
    "using $(\\mathbf{w}, b)$ found before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ec0fa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 0.001, log posterior ratio: [ 7.34573105 -1.39291983  7.8487674  -3.59511308  7.51568146 10.3192656\n",
      "  5.64864932 -2.7628981  -5.93601189  0.05822314 -0.35935684 -7.9592753\n",
      " -7.70820105 -0.6649525   5.32463218  4.8301535  -5.01446268  5.29198042\n",
      "  5.46278923 -5.33168744 -4.21320866  8.17022076 -0.38296579 -6.28666092\n",
      " -2.77750163  3.5185371   6.79550277  0.71193003  4.88293213 -1.97790102\n",
      " -8.15024837 -6.56525144  5.70188711  4.92242477]\n",
      "lambda: 0.01, log posterior ratio: [ 3.98610179 -1.16283271  4.50320903 -1.92101139  4.08046413  5.5606327\n",
      "  3.13308834 -1.34636372 -2.75766427  0.0309689   0.05637192 -3.90249551\n",
      " -4.9201439  -0.18138284  2.59427587  2.45534757 -3.6194004   2.89055777\n",
      "  3.19322561 -2.4393109  -2.52744802  4.19528783  0.04099808 -3.2731614\n",
      " -1.64716429  1.57455467  3.63383503  0.13307529  3.09855156 -0.80246374\n",
      " -5.11842021 -2.63609133  2.89086948  2.76383762]\n",
      "lambda: 0.1, log posterior ratio: [ 1.70462274 -0.38947037  1.99003599 -0.90194033  1.82896247  2.32921767\n",
      "  1.42079177 -0.51968027 -0.93195135  0.22001354  0.19697037 -1.52070406\n",
      " -2.40670977  0.05151108  0.82764159  0.98228944 -2.13631628  1.30833073\n",
      "  1.57687335 -0.85673749 -1.04901069  1.72697519  0.18153347 -1.39651288\n",
      " -0.67169028  0.39410241  1.45238507  0.03787276  1.66129928 -0.16829401\n",
      " -2.58939273 -0.59017078  1.09956507  1.33226547]\n",
      "lambda: 1.0, log posterior ratio: [ 0.44624567 -0.03525829  0.5219396  -0.19315264  0.48907398  0.58923401\n",
      "  0.38986248 -0.08156834 -0.15446812  0.12114896  0.10792503 -0.31103285\n",
      " -0.58006042  0.06685486  0.19539125  0.26375326 -0.55676647  0.36217666\n",
      "  0.44542613 -0.14596405 -0.20797707  0.44377253  0.1026822  -0.30031365\n",
      " -0.11788722  0.09214049  0.37048871  0.04584899  0.48258605  0.01953031\n",
      " -0.63894833 -0.03350416  0.28218843  0.37901428]\n"
     ]
    }
   ],
   "source": [
    "#compute log posteriors ratios using DVAL samples\n",
    "scores_l = {} #key: lambda, value: (log posterior ratio for each sample in DVAL)\n",
    "\n",
    "for l in lambdas:\n",
    "    p = parameters_l[l]\n",
    "    w, b = p[0], p[1]\n",
    "    S = (vcol(w).T @ DVAL).ravel() + b\n",
    "    scores_l[l] = S\n",
    "    print(f\"lambda: {l}, log posterior ratio: {S}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f1eb6",
   "metadata": {},
   "source": [
    "Then, we know that $s(\\mathbf{x}_t) = \\mathbf{w}^T \\mathbf{x}_t + b = 0$ is the **decision surface** which separates the two classes. So, we can perform nclass assignments by thresholding the scores with 0 (i.e. $S[i] > 0 \\implies LP[i] = 1$, where $LP$ is the array of predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a50d16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for lambda 0.001: 8.823529411764707\n",
      "Error rate for lambda 0.01: 8.823529411764707\n",
      "Error rate for lambda 0.1: 11.76470588235294\n",
      "Error rate for lambda 1.0: 14.705882352941178\n"
     ]
    }
   ],
   "source": [
    "#Perform class assignments\n",
    "LP_l = {} #key: lambda, value: (log posterior ratio for each sample in DVAL)\n",
    "for l in lambdas:\n",
    "    score = scores_l[l]\n",
    "    LP = np.zeros(score.shape) #predicted labels array\n",
    "    LP[score > 0] = 1 #assign label 1 to samples with score > 0\n",
    "    LP[score < 0] = 0 #assign label 0 to samples with score < 0\n",
    "    LP_l[l] = LP #store the predicted labels for each lambda\n",
    "\n",
    "#compute error rates \n",
    "err_l = {} #key: lambda, value: error rate\n",
    "for l in lambdas:\n",
    "    LP = LP_l[l]\n",
    "    err = (LP != LVAL).sum() / float(LVAL.size) * 100\n",
    "    err_l[l] = err #store the error rate for each lambda\n",
    "    print(f\"Error rate for lambda {l}: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7798ecef",
   "metadata": {},
   "source": [
    "Now, let's make another step. As said before, at the beginning, the score provided by L.R. can be interpreted ad the log ratio between class Posteriors:\n",
    "$$\n",
    "s(\\mathbf{x}) = \\log \\frac{P(C = h1 \\mid \\mathbf{x}, \\mathbf{w}, b)}{P(C = h0 \\mid \\mathbf{x}, \\mathbf{w}, b)}\n",
    "$$\n",
    "Because of the fact that model parameters $(\\mathbf{w}, b)$ have been estimated by either minimizing the average cross entropy or Logistic Loss function, either maximizing the log-likelihood, the model will implicitly reflect the **empirical class priors** of the training set of the actual labels ($LTR$):\n",
    "$$\n",
    "P_{emp}(C = h_1) = \\frac{\\sum_{c_i=1}c_i}{K} = \\frac{\\sum_{i=0}^{K-1}c_i}{K} = \\frac{n_{h_1}}{K} \\\\\\\\\n",
    "P_{emp}(C = h_0) = 1 - P_{emp}(C = h_1) = \\frac{\\sum_{c_i=0}c_i}{K} = 1 - \\frac{n_{h_1}}{K} = \\frac{n_{h_0}}{K}\n",
    "$$\n",
    "The empirical prior log-odds from the training set is:\n",
    "$$\n",
    "\\log \\frac{P_{emp}(C = h_1)}{P_{emp}(C = h_0)} = \\log \\frac{n_{h_1} / K}{n_{h_0} / K} = \\frac{n_{h_1}}{n_{h_0}}\n",
    "$$\n",
    "So, the model learns to make predictions that align well with the distribution of classes observed in the training set. In the limit of a large training set, the model's learned bias term $b$ will be influenced by the log-odds of the empirical class priors. <br>\n",
    "So, it's evident that the model posterior probabilities are thus suited for applications\n",
    "whose effective prior is close to the empirical training set prior,\n",
    "but may provide **poor performance for different applications** when this is not the case. <br>\n",
    "The bias term, $b$, in the score will absorbe information about priors, so to solve this issue we can subtract the empirical prior log odds from the training set to recover a score which behaves like a log-likelihood ratio:\n",
    "$$\n",
    "s_{llr}(\\mathbf{x}) = s(\\mathbf{x}) - \\log \\frac{n_{h_1}}{n_{h_0}}\n",
    "$$\n",
    "This can be written also like this, expliciting the decision rule (which in this case is linear) and the training prior log odds:\n",
    "$$\n",
    "s_{llr}(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b - \\log \\frac{\\pi_{emp}^{DTR}}{1- \\pi_{emp}^{DTR}}\n",
    "$$\n",
    "Now  basically we can use $s_{llr}(\\mathbf{x})$ as a normal log-likelihood ratio and, to make a decision or obtain posterior probabilities for a specific application with a potentially different prior, we can combine this estimated LLR with the prior log-odds of that application. For example, if we have an application $(\\pi_T, 1, 1)$ with an effective prior $\\pi_T$, we can compute the posterior log-odds for the specific application:\n",
    "$$\n",
    "\\log \\frac{P_{app}(C = h1 \\mid \\mathbf{x}, \\mathbf{w}, b)}{P_{app}(C = h0 \\mid \\mathbf{x}, \\mathbf{w}, b)} = s_{llr}(\\mathbf{x}) + \\log \\frac{\\pi_T}{1 - \\pi_T} = \\mathbf{w}^T \\mathbf{x} + b - \\log \\frac{\\pi_{emp}^{DTR}}{1- \\pi_{emp}^{DTR}} + \\log \\frac{\\pi_T}{1 - \\pi_T}\n",
    "$$\n",
    "And then compute decisions by comparing $s_{llr}(\\mathbf{x})$ with the thredhold given by the application prior log odds:\n",
    "$$\n",
    "s_{llr}(\\mathbf{x}) \\gtrless \\log \\frac{\\pi_T}{1 - \\pi_T}\n",
    "$$\n",
    "This provides a decision rule that is adapted to the specific prior of the target application. The threshold $\\log \\frac{\\pi_T}{1 - \\pi_T}$\n",
    "represents the point where the posterior probability for class $h_1$ is equal to the application prior $\\pi_T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259e815",
   "metadata": {},
   "source": [
    "So, we compute empirical priors from the training set:\n",
    "$$\n",
    "P_{emp}(C = h_1) = \\frac{\\sum_{c_i=1}c_i}{K} = \\frac{\\sum_{i=0}^{K-1}c_i}{K} = \\frac{n_{h_1}}{K} \\\\\\\\\n",
    "P_{emp}(C = h_0) = 1 - P_{emp}(C = h_1) = \\frac{\\sum_{c_i=0}c_i}{K} = 1 - \\frac{n_{h_1}}{K} = \\frac{n_{h_0}}{K}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60d1e8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical prior for class 1: 51.52%\n",
      "Empirical prior for class 0: 48.48%\n",
      "Empirical prior log odds: 0.06062462181643484\n"
     ]
    }
   ],
   "source": [
    "#Empirical priors computation from DTR\n",
    "pi_emp_h1 = np.sum(LTR == 1) / LTR.size\n",
    "pi_emp_h0 = np.sum(LTR == 0) / LTR.size\n",
    "\n",
    "print(f\"Empirical prior for class 1: {round(pi_emp_h1 * 100, 2)}%\")\n",
    "print(f\"Empirical prior for class 0: {round(pi_emp_h0 * 100, 2)}%\")\n",
    "print(f\"Empirical prior log odds: {np.log(pi_emp_h1 / pi_emp_h0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e79b7da",
   "metadata": {},
   "source": [
    "We now can compute *llr-like* scores, $s_{llr}(\\mathbf{x})$, Beware, now we use the original scores, $s(\\mathbf{x})$, computed using the **validation** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c302da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 0.001, log likelihood ratio scores: [ 7.28510642e+00 -1.45354445e+00  7.78814278e+00 -3.65573770e+00\n",
      "  7.45505684e+00  1.02586410e+01  5.58802470e+00 -2.82352272e+00\n",
      " -5.99663652e+00 -2.40148315e-03 -4.19981459e-01 -8.01989992e+00\n",
      " -7.76882567e+00 -7.25577118e-01  5.26400756e+00  4.76952887e+00\n",
      " -5.07508730e+00  5.23135580e+00  5.40216461e+00 -5.39231206e+00\n",
      " -4.27383328e+00  8.10959614e+00 -4.43590411e-01 -6.34728554e+00\n",
      " -2.83812625e+00  3.45791248e+00  6.73487815e+00  6.51305410e-01\n",
      "  4.82230750e+00 -2.03852564e+00 -8.21087299e+00 -6.62587606e+00\n",
      "  5.64126248e+00  4.86180015e+00]\n",
      "lambda: 0.01, log likelihood ratio scores: [ 3.92547716e+00 -1.22345733e+00  4.44258441e+00 -1.98163602e+00\n",
      "  4.01983951e+00  5.50000808e+00  3.07246372e+00 -1.40698834e+00\n",
      " -2.81828890e+00 -2.96557203e-02 -4.25269930e-03 -3.96312013e+00\n",
      " -4.98076852e+00 -2.42007457e-01  2.53365125e+00  2.39472295e+00\n",
      " -3.68002502e+00  2.82993315e+00  3.13260099e+00 -2.49993552e+00\n",
      " -2.58807264e+00  4.13466321e+00 -1.96265425e-02 -3.33378602e+00\n",
      " -1.70778891e+00  1.51393005e+00  3.57321041e+00  7.24506642e-02\n",
      "  3.03792694e+00 -8.63088360e-01 -5.17904483e+00 -2.69671595e+00\n",
      "  2.83024485e+00  2.70321300e+00]\n",
      "lambda: 0.1, log likelihood ratio scores: [ 1.64399812 -0.450095    1.92941137 -0.96256495  1.76833785  2.26859305\n",
      "  1.36016715 -0.5803049  -0.99257597  0.15938892  0.13634575 -1.58132868\n",
      " -2.46733439 -0.00911354  0.76701697  0.92166482 -2.1969409   1.24770611\n",
      "  1.51624873 -0.91736211 -1.10963531  1.66635057  0.12090885 -1.4571375\n",
      " -0.7323149   0.33347779  1.39176045 -0.02275186  1.60067466 -0.22891863\n",
      " -2.65001735 -0.65079541  1.03894045  1.27164085]\n",
      "lambda: 1.0, log likelihood ratio scores: [ 0.38562104 -0.09588292  0.46131498 -0.25377726  0.42844936  0.52860939\n",
      "  0.32923786 -0.14219296 -0.21509274  0.06052434  0.04730041 -0.37165747\n",
      " -0.64068504  0.00623024  0.13476662  0.20312864 -0.61739109  0.30155204\n",
      "  0.38480151 -0.20658867 -0.26860169  0.38314791  0.04205758 -0.36093827\n",
      " -0.17851185  0.03151587  0.30986409 -0.01477563  0.42196143 -0.04109432\n",
      " -0.69957296 -0.09412878  0.22156381  0.31838966]\n"
     ]
    }
   ],
   "source": [
    "llr_like_scores_l = {} #key: lambda, value: (s_llr)\n",
    "for l in lambdas:\n",
    "    score = scores_l[l]\n",
    "    #subtract empirical prior log odds\n",
    "    s_llr = score - np.log(pi_emp_h1 / pi_emp_h0) #the same as s_llr = score - np.log(pi_emp_h1 / (1- pi_emp_h1))\n",
    "    llr_like_scores_l[l] = s_llr #store the log likelihood ratio scores for each lambda\n",
    "    print(f\"lambda: {l}, log likelihood ratio scores: {s_llr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0ceb0",
   "metadata": {},
   "source": [
    "Now we can compute the $\\text{DCF}$ and $\\text{min DCF}$ (i.e. the **normalized** Bayes Risk and normalized min Bayes Risk) by comparing the $s_{llr}(\\mathbf{x})$ to the application dependent threshold. <br>\n",
    "In this case, we abitrarily set $\\pi_T = 0.5$ for application prior (meaning in our application the two classes are perfectly balanced). <br>\n",
    "Based on what we obtain, we can actually say if our system is *good / useful* or *harmful /useless*:\n",
    "- if $DCF(\\pi_T, 1, 1) > 1 \\Rightarrow$ our system is **harmful**\n",
    "- if $DCF(\\pi_T, 1, 1) \\lt 1 \\Rightarrow$ our system is **good**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import function to compute DDCF, minDCF\n",
    "from decisionModelsEvaluation import computeEmpiricalBayesRisk_Normalized, computeMinEmpiricalBayesRisk_Normalized\n",
    "\n",
    "#computeEmpiricalBayesRisk_Normalized(llrs, LVAL, PriorTrue, Cfn, Cfp)\n",
    "#computeMinEmpiricalBayesRisk_Normalized(scores, LVAL, PriorTrue, Cfn, Cfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e8b6ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcf for lambda 0.001: 0.1181\n",
      "min_dcf for lambda 0.001: 0.0625\n",
      "dcf for lambda 0.01: 0.1181\n",
      "min_dcf for lambda 0.01: 0.0556\n",
      "dcf for lambda 0.1: 0.1111\n",
      "min_dcf for lambda 0.1: 0.0556\n",
      "dcf for lambda 1.0: 0.1667\n",
      "min_dcf for lambda 1.0: 0.1111\n"
     ]
    }
   ],
   "source": [
    "#Compute dcf, min_dcf\n",
    "dcf_l = {} #key: lambda, value: (dcf)\n",
    "min_dcf_l = {} #key: lambda, value: (min_dcf)\n",
    "\n",
    "for l in lambdas:\n",
    "\n",
    "    dcf = computeEmpiricalBayesRisk_Normalized(llr_like_scores_l[l], LVAL, 0.5, 1.0, 1.0)\n",
    "    min_dcf = computeMinEmpiricalBayesRisk_Normalized(llr_like_scores_l[l], LVAL, 0.5, 1.0, 1.0)\n",
    "    dcf_l[l] = dcf #store the dcf for each lambda\n",
    "    min_dcf_l[l] = min_dcf #store the min_dcf for each lambda\n",
    "\n",
    "    print(f\"dcf for lambda {l}: {dcf:.4f}\")\n",
    "    print(f\"min_dcf for lambda {l}: {min_dcf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02b9a4",
   "metadata": {},
   "source": [
    "We can redo all the steps in one unique function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a1bed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bd929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitLogReg(DTR, LTR, lambdas):\n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier using LTR as labels and DTR as data.\n",
    "    lambdas is a list of regularization parameters (lambda).\n",
    "    Parameters:\n",
    "    - DTR: numpy array of shape (n_features, n_samples), training data\n",
    "    - LTR: numpy array of shape (n_samples,), training labels\n",
    "    - lambdas: list of regularization parameters (lambda)\n",
    "    \"\"\"\n",
    "\n",
    "    #STEP 1: RETRIEVE MODEL PARAMS (w, b)\n",
    "    #retrieve the parameters which minimize the objective function\n",
    "    #change lambda from 10^(-3) to 1.0, by incrementing it by a decimal order every time\n",
    "    parameters_l = {} #key: lambda, value: (w_min, b_min)\n",
    "    lambdas = [10**(-3), 10**(-2), 10**(-1), 1.0]\n",
    "    for l in lambdas:\n",
    "        w_min, b_min, objMin = trainLogReg(DTR, LTR, l, manual_grad=True)\n",
    "        parameters_l[l] = (w_min, b_min)\n",
    "\n",
    "\n",
    "    #STEP 2: LOG POSTERIORS\n",
    "    #compute log posteriors ratios using DVAL samples\n",
    "    scores_l = {} #key: lambda, value: (log posterior ratio for each sample in DVAL)\n",
    "    for l in lambdas:\n",
    "        p = parameters_l[l]\n",
    "        w, b = p[0], p[1]\n",
    "        S = (vcol(w).T @ DVAL).ravel() + b\n",
    "        scores_l[l] = S\n",
    "\n",
    "\n",
    "    #STEP3: DECISION RULE -> PERFORM CLASS ASSIGNMENTS\n",
    "    #Perform class assignments\n",
    "    LP_l = {} #key: lambda, value: (log posterior ratio for each sample in DVAL)\n",
    "    for l in lambdas:\n",
    "        score = scores_l[l]\n",
    "        LP = np.zeros(score.shape) #predicted labels array\n",
    "        LP[score > 0] = 1 #assign label 1 to samples with score > 0\n",
    "        LP[score < 0] = 0 #assign label 0 to samples with score < 0\n",
    "        LP_l[l] = LP #store the predicted labels for each lambda\n",
    "\n",
    "    #compute error rates \n",
    "    err_l = {} #key: lambda, value: error rate\n",
    "    for l in lambdas:\n",
    "        LP = LP_l[l]\n",
    "        err = (LP != LVAL).sum() / float(LVAL.size) * 100\n",
    "        err_l[l] = err #store the error rate for each lambda\n",
    "\n",
    "\n",
    "\n",
    "    #STEP4: COMPUTE LLR LIKE SCORES BY SUBTRCACTING EMPIRICAL PRIOR LOG ODDS FROM THE SCORES\n",
    "    #Empirical priors computation from DTR\n",
    "    pi_emp_h1 = np.sum(LTR == 1) / LTR.size\n",
    "    pi_emp_h0 = np.sum(LTR == 0) / LTR.size\n",
    "    llr_like_scores_l = {} #key: lambda, value: (s_llr)\n",
    "    for l in lambdas:\n",
    "        score = scores_l[l]\n",
    "        #subtract empirical prior log odds\n",
    "        s_llr = score - np.log(pi_emp_h1 / pi_emp_h0) #the same as s_llr = score - np.log(pi_emp_h1 / (1- pi_emp_h1))\n",
    "        llr_like_scores_l[l] = s_llr #store the log likelihood ratio scores for each lambda\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #STEP5: COMPUTE DDCF, MIN DCF\n",
    "    #Compute dcf, min_dcf\n",
    "    dcf_l = {} #key: lambda, value: (dcf)\n",
    "    min_dcf_l = {} #key: lambda, value: (min_dcf)\n",
    "    for l in lambdas:\n",
    "        dcf = computeEmpiricalBayesRisk_Normalized(llr_like_scores_l[l], LVAL, 0.5, 1.0, 1.0)\n",
    "        min_dcf = computeMinEmpiricalBayesRisk_Normalized(llr_like_scores_l[l], LVAL, 0.5, 1.0, 1.0)\n",
    "        dcf_l[l] = dcf #store the dcf for each lambda\n",
    "        min_dcf_l[l] = min_dcf #store the min_dcf for each lambda\n",
    "\n",
    "\n",
    "\n",
    "    #STEP6: TABLE\n",
    "    # Create the table using matplotlib\n",
    "    lambdas_table = sorted(lambdas)\n",
    "    error_rates = [err_l[l] for l in lambdas_table]\n",
    "    min_dcfs = [min_dcf_l[l] for l in lambdas_table]\n",
    "    dcfs = [dcf_l[l] for l in lambdas_table]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Hide axes\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # Create table data\n",
    "    table_data = [\n",
    "        [f\"{l:.3f}\", f\"{err:.1f}%\", f\"{min_dcf:.4f}\", f\"{dcf:.4f}\"]\n",
    "        for l, err, min_dcf, dcf in zip(lambdas_table, error_rates, min_dcfs, dcfs)\n",
    "    ]\n",
    "\n",
    "    # Create the table\n",
    "    table = ax.table(cellText=table_data,\n",
    "                     colLabels=[\"$\\lambda$\", \"Error rate\", \"minDCF ($\\pi_T = 0.5$)\", \"actDCF ($\\pi_T = 0.5$)\"],\n",
    "                     loc='center')\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "\n",
    "    plt.title(\"Logistic Regression Performance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4131a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGGCAYAAADmRxfNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZhElEQVR4nO3de3zP9eP///tr5zmMjdkMm8PCDGPT1pxGyUjkUOkgE4nIsQ/v1Hsh56QUIiXlTQcq3pTEW0gMNZbTIt5jhZFT3gxje/z+8Nvr62Ubm+eG2u16ubwu9no+H8/H8/F4ejyfr9f99Xo+ny+bMcYIAAAAACxwut0NAAAAAPDXR7AAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAHBHaNGihVq0aFFo9VWtWlU9evQotPog2Ww2jRo16nY3o0hdvnxZw4cPV5UqVeTk5KSOHTve7iYBwF8GwQKAgw8//FA2m00//fTT7W7KDW3cuFGjRo3S6dOni3Q9VatWlc1msz9KliypyMhIzZs3r0jXiytGjRrlsP1LlCihOnXq6J///KfOnDlTqOv64IMPNHnyZD388MP66KOPNGTIkEKtHwD+zlxudwMAQJJWrlxZ4GU2btyo0aNHq0ePHipbtqzDvD179sjJqfA+O2nQoIFeeOEFSdKRI0f0/vvvKy4uThcvXlTv3r0LbT13svPnz8vF5fa9bMycOVOlSpXS2bNntXLlSo0bN07fffedNmzYIJvNVijr+O6771SpUiW9+eabhVIfABQnBAsAdwQ3N7dCrc/d3b1Q66tUqZK6detmf96jRw9Vr15db7755i0PFufOnVPJkiVv6TolycPD45av82oPP/ywypcvL0nq27evunTpoi+//FKbNm1SdHT0TddrjNGFCxfk6empY8eO5QipVmRlZSkjI+O2bzsAuBU4FQrATdm2bZvatm0rLy8vlSpVSvfdd582bdqUo9z27dsVExMjT09PVa5cWWPHjtXcuXNls9l04MABe7ncrrGYNm2aQkNDVaJECXl7e6tRo0b6+OOPJV05PWbYsGGSpGrVqtlPk8muM7drLE6fPq0hQ4aoatWqcnd3V+XKldW9e3cdP368wP339fVV7dq1tX//fofpWVlZmjp1qkJDQ+Xh4SE/Pz/16dNHp06dylFu1KhRCggIUIkSJdSyZUvt3r07R7uzT01bt26d+vXrpwoVKqhy5cr2+d98842aNWumkiVLqnTp0mrXrp127drlsK60tDQ9/fTTqly5stzd3VWxYkU99NBDDtv/p59+UmxsrMqXLy9PT09Vq1ZNPXv2dKgnt2ss8jMOsvuwYcMGDR06VL6+vipZsqQ6deqkP/74I7+bPId7771XkpSSkiIp/9u+atWqevDBB/Xtt9+qUaNG8vT01LvvviubzaY1a9Zo165d9vG0du1aSVfC3AsvvKAqVarI3d1dtWrV0uuvvy5jTI5t9Pzzz2vBggUKDQ2Vu7u7VqxYYd8GP/zwgwYOHChfX1+VLVtWffr0UUZGhk6fPq3u3bvL29tb3t7eGj58eI66X3/9dTVu3FjlypWTp6enIiIi9Pnnn+fYLtltWLJkierWrSt3d3eFhoZqxYoVOcoeOnRIvXr1UkBAgNzd3VWtWjU999xzysjIsJc5ffq0Bg8ebO97cHCwJk2apKysrIL/pwH4W+MbCwAFtmvXLjVr1kxeXl4aPny4XF1d9e6776pFixZat26doqKiJF1509KyZUvZbDaNGDFCJUuW1Pvvv5+vbxPee+89DRw4UA8//LAGDRqkCxcuaPv27dq8ebOeeOIJde7cWXv37tUnn3yiN9980/5Jtq+vb671nT17Vs2aNVNycrJ69uyp8PBwHT9+XEuXLtXvv/9uXz6/Ll++rN9//13e3t4O0/v06aMPP/xQTz/9tAYOHKiUlBRNnz5d27Zt04YNG+Tq6ipJGjFihF577TW1b99esbGx+vnnnxUbG6sLFy7kur5+/frJ19dXr7zyis6dOydJ+te//qW4uDjFxsZq0qRJSk9P18yZM9W0aVNt27ZNVatWlSR16dJFu3bt0oABA1S1alUdO3ZMq1atUmpqqv1569at5evrqxdffFFly5bVgQMH9OWXX153G+R3HGQbMGCAvL29NXLkSB04cEBTp07V888/r88++6xA2z5bdqgrV65cgba9dOVUuccff1x9+vRR7969VblyZf3rX//SuHHjdPbsWU2YMEGSFBISImOMOnTooDVr1qhXr15q0KCBvv32Ww0bNkyHDh3KcdrUd999p4ULF+r5559X+fLlVbVqVSUlJdm3gb+/v0aPHq1NmzZp9uzZKlu2rDZu3KjAwECNHz9ey5cv1+TJk1W3bl11797dXu9bb72lDh066Mknn1RGRoY+/fRTPfLII/rqq6/Url07hzb88MMP+vLLL9WvXz+VLl1ab7/9trp06aLU1FT79jp8+LAiIyN1+vRpPfvss6pdu7YOHTqkzz//XOnp6XJzc1N6erpiYmJ06NAh9enTR4GBgdq4caNGjBihI0eOaOrUqTf1fwfgb8oAwFXmzp1rJJkff/wxzzIdO3Y0bm5uZv/+/fZphw8fNqVLlzbNmze3TxswYICx2Wxm27Zt9mknTpwwPj4+RpJJSUmxT4+JiTExMTH25w899JAJDQ29blsnT56co55sQUFBJi4uzv78lVdeMZLMl19+maNsVlbWddcTFBRkWrdubf744w/zxx9/mB07dpinnnrKSDL9+/e3l1u/fr2RZBYsWOCw/IoVKxymp6WlGRcXF9OxY0eHcqNGjTKSHNqd/f/RtGlTc/nyZfv0//3vf6Zs2bKmd+/eDnWkpaWZMmXK2KefOnXKSDKTJ0/Os3+LFy++4f+5McZIMiNHjrQ/z+84yO5Dq1atHLb1kCFDjLOzszl9+vR11zty5EgjyezZs8f88ccfJiUlxbz77rvG3d3d+Pn5mXPnzuV72xtz5f9TklmxYkWOdcXExOQYd0uWLDGSzNixYx2mP/zww8Zms5l9+/Y5bCMnJyeza9cuh7LZ2yA2NtZhG0RHRxubzWb69u1rn3b58mVTuXJlh/3BGGPS09MdnmdkZJi6deuae++912G6JOPm5ubQrp9//tlIMtOmTbNP6969u3Fycsr1/z27jWPGjDElS5Y0e/fudZj/4osvGmdnZ5OamppjWQDFF6dCASiQzMxMrVy5Uh07dlT16tXt0ytWrKgnnnhCP/zwg/1OPStWrFB0dLQaNGhgL+fj46Mnn3zyhuspW7asfv/9d/3444+F0u4vvvhCYWFh6tSpU455+bnwd+XKlfL19ZWvr6/q1aunf/3rX3r66ac1efJke5lFixapTJkyuv/++3X8+HH7IyIiQqVKldKaNWskSatXr9bly5fVr18/h3UMGDAgz/X37t1bzs7O9uerVq3S6dOn9fjjjzusy9nZWVFRUfZ1eXp6ys3NTWvXrs1xSlC27GsKvvrqK126dOmG20Iq2DjI9uyzzzps62bNmikzM1MHDx7M1zpr1aolX19fVatWTX369FFwcLC+/vprlShRIt/bPlu1atUUGxubr/UuX75czs7OGjhwoMP0F154QcYYffPNNw7TY2JiVKdOnVzr6tWrl8M2iIqKkjFGvXr1sk9zdnZWo0aN9N///tdhWU9PT/vfp06d0p9//qlmzZpp69atOdbTqlUr1ahRw/68fv368vLysteZlZWlJUuWqH379mrUqFGO5bPbuGjRIjVr1kze3t4O27VVq1bKzMzU999/n2s/ARRPnAoFoED++OMPpaenq1atWjnmhYSEKCsrS7/99ptCQ0N18ODBXC+qDQ4OvuF6/vGPf+g///mPIiMjFRwcrNatW+uJJ55QkyZNbqrd+/fvV5cuXW5qWenKG8CxY8cqMzNTO3fu1NixY3Xq1CmHi85//fVX/fnnn6pQoUKudRw7dkyS7G+kr90OPj4+OU6tylatWjWH57/++quk/3edwbW8vLwkXbmIfdKkSXrhhRfk5+ene+65Rw8++KC6d+8uf39/SVfeCHfp0kWjR4/Wm2++qRYtWqhjx4564okn8jxtrSDjIFtgYKBDuey+5hV4rvXFF1/Iy8tLrq6uqly5ssMb5/xu+2zXbs/rOXjwoAICAlS6dGmH6SEhIfb5+a372m1QpkwZSVKVKlVyTL92u3z11VcaO3askpKSdPHiRfv03ILxteuRrmzv7Dr/+OMPnTlzRnXr1s2zrdKV7bp9+/Y8TzG8drsCKN4IFgDuSCEhIdqzZ4+++uorrVixQl988YXeeecdvfLKKxo9evQtb0/58uXVqlUrSVJsbKxq166tBx98UG+99ZaGDh0q6cqnwBUqVNCCBQtyrSOvN2f5cfWn1dnrkq5cZ5EdEK529W1hBw8erPbt22vJkiX69ttvFR8frwkTJui7775Tw4YNZbPZ9Pnnn2vTpk1atmyZvv32W/Xs2VNTpkzRpk2bVKpUqZtu99Wu/sblauaai5Tz0rx58zyvhSnotr92exam69Wd1zbIbfrV22X9+vXq0KGDmjdvrnfeeUcVK1aUq6ur5s6da7+hQX7Wk99tnS0rK0v333+/hg8fnuv8mjVrFqg+AH9vBAsABeLr66sSJUpoz549Oeb98ssvcnJysn/6GhQUpH379uUol9u03JQsWVJdu3ZV165dlZGRoc6dO2vcuHEaMWKEPDw8CvTbBTVq1NDOnTvzXf5G2rVrp5iYGI0fP159+vRRyZIlVaNGDf3nP/9RkyZNrvvmMigoSNKV7XD1p9snTpzI96f32Z/WV6hQwR54blT+hRde0AsvvKBff/1VDRo00JQpUzR//nx7mXvuuUf33HOPxo0bp48//lhPPvmkPv30Uz3zzDM56ivIOLgV8rvtb0ZQUJD+85//6H//+5/Dtxa//PKLfX5R++KLL+Th4aFvv/3W4VukuXPn3lR9vr6+8vLyuuE+UaNGDZ09ezZfYwwAuMYCQIE4OzurdevW+ve//+1wu9KjR4/q448/VtOmTe2n4cTGxiohIcF+RxxJOnnyZJ6fKl/txIkTDs/d3NxUp04dGWPs1wFk/5ZDfn55u0uXLvr555+1ePHiHPMK+ilutn/84x86ceKE3nvvPUnSo48+qszMTI0ZMyZH2cuXL9vbed9998nFxUUzZ850KDN9+vR8rzs2NlZeXl4aP358rtdFZN/GNT09PcedpmrUqKHSpUvbT6c5depUjm2QfV3M1afcXK0g4+BWyO+2vxkPPPCAMjMzc/z/vPnmm7LZbGrbtu1N151fzs7OstlsyszMtE87cOCAlixZclP1OTk5qWPHjlq2bJl++umnHPOzx8Ojjz6qhIQEffvttznKnD59WpcvX76p9QP4e+IbCwC5+uCDD3K97/2gQYM0duxYrVq1Sk2bNlW/fv3k4uKid999VxcvXtRrr71mLzt8+HDNnz9f999/vwYMGGC/3WxgYKBOnjx53W8cWrduLX9/fzVp0kR+fn5KTk7W9OnT1a5dO/unxhEREZKkl19+WY899phcXV3Vvn37XH88btiwYfr888/1yCOPqGfPnoqIiNDJkye1dOlSzZo1S2FhYQXeRm3btlXdunX1xhtvqH///oqJiVGfPn00YcIEJSUlqXXr1nJ1ddWvv/6qRYsW6a233tLDDz8sPz8/DRo0SFOmTFGHDh3Upk0b/fzzz/rmm29Uvnz5fH0T4+XlpZkzZ+qpp55SeHi4HnvsMfn6+io1NVVff/21mjRpounTp2vv3r2677779Oijj6pOnTpycXHR4sWLdfToUT322GOSpI8++kjvvPOOOnXqpBo1auh///uf3nvvPXl5eemBBx7Isw35HQe3Qn63/c1o3769WrZsqZdfflkHDhxQWFiYVq5cqX//+98aPHiww7UeRaVdu3Z644031KZNGz3xxBM6duyYZsyYoeDgYG3fvv2m6hw/frxWrlypmJgYPfvsswoJCdGRI0e0aNEi/fDDDypbtqyGDRumpUuX6sEHH1SPHj0UERGhc+fOaceOHfr888914MCBAt+qGcDf2G27HxWAO1L2bTHzevz222/GGGO2bt1qYmNjTalSpUyJEiVMy5YtzcaNG3PUt23bNtOsWTPj7u5uKleubCZMmGDefvttI8mkpaXZy117u9l3333XNG/e3JQrV864u7ubGjVqmGHDhpk///zTof4xY8aYSpUqGScnJ4dbz157u1ljrtzq9vnnnzeVKlUybm5upnLlyiYuLs4cP378utskKCjItGvXLtd5H374oZFk5s6da582e/ZsExERYTw9PU3p0qVNvXr1zPDhw83hw4ftZS5fvmzi4+ONv7+/8fT0NPfee69JTk425cqVc7j16I1u/7tmzRoTGxtrypQpYzw8PEyNGjVMjx49zE8//WSMMeb48eOmf//+pnbt2qZkyZKmTJkyJioqyixcuNBex9atW83jjz9uAgMDjbu7u6lQoYJ58MEH7XVk0zW3m81e9kbjIK8+rFmzxkgya9asybVv2bJvN/vHH39ct5wx+dv21/v/zO12s8Zcub3vkCFDTEBAgHF1dTV33XWXmTx5co5bFeuaWxBny2sb5NW3uLg4U7JkSYdpc+bMMXfddZdxd3c3tWvXNnPnzrUvn5825LZPHDx40HTv3t34+voad3d3U716ddO/f39z8eJFh76PGDHCBAcHGzc3N1O+fHnTuHFj8/rrr5uMjIwc6wFQfNmMuclzAADgJg0ePFjvvvuuzp49m+dFpsXR6dOn5e3trbFjx+rll1++3c0BAKBAuMYCQJE6f/68w/MTJ07oX//6l5o2bVqsQ8W120WS/VeMW7RocWsbAwBAIeAaCwBFKjo6Wi1atFBISIiOHj2qOXPm6MyZM4qPj7/dTbutPvvsM3344Yd64IEHVKpUKf3www/65JNP1Lp165v+rQ4AAG4nggWAIvXAAw/o888/1+zZs2Wz2RQeHq45c+aoefPmt7tpt1X9+vXl4uKi1157TWfOnLFf0D127Njb3TQAAG4K11gAAAAAsIxrLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGCZS34Lpqam6vjx40XZFuCOdvHiRbm7u9/uZgC3BeMfxR37AIqz8uXLKzAw8Ibl8hUsUlNTFRISovT0dMsNA/6qnJ2dlZmZebubAdwWjH8Ud+wDKM5KlCih5OTkG4aLfAWL48ePKz09XfPnz1dISEihNBD4K1m+fLni4+PZB1AsMf5R3LEPoDhLTk5Wt27ddPz48cIJFtlCQkIUHh5uqXHAX1FycrIk9gEUT4x/FHfsA0D+cPE2AAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAoFlq0aKHBgwff7mYU2IkTJ1ShQgUdOHDgdjelyD322GOaMmXK7W4G7hDFZez/ncY9weIv4MUXX5S7u7ueeOKJ290UAPjL+vLLLzVmzJgCLdOjRw/ZbDbZbDa5urrKz89P999/vz744ANlZWU5lE1LS9OAAQNUvXp1ubu7q0qVKmrfvr1Wr16da31XP/bt25dnG8aNG6eHHnpIVatWLVDbi9qMGTNUtWpVeXh4KCoqSlu2bLlu+VGjRuXod+3atR3K/POf/9S4ceP0559/FmXTcZXcAndBxr1047F/M+NeujPHPuP++ggWfwEjRozQlClT9Mknn9xwJwSKQl4vCm3atLndTbtpNptNS5Ysud3NwC3k4+Oj0qVLF3i5Nm3a6MiRIzpw4IC++eYbtWzZUoMGDdKDDz6oy5cvS5IOHDigiIgIfffdd5o8ebJ27NihFStWqGXLlurfv3+u9V39qFatWq7rTk9P15w5c9SrV6+Cd7gIffbZZxo6dKhGjhyprVu3KiwsTLGxsTp27Nh1lwsNDXXo9w8//OAwv27duqpRo4bmz59flM1HPuRn3Ev5H/sFGffSnTn2Gfc3RrD4CyhTpox69eolJycn7dix43Y3B8VUbi8Kn3zySZ7lL126lGNaRkbGTa07v8tlZmbm+mka/n5atGihAQMGaPDgwfL29pafn5/ee+89nTt3Tk8//bRKly6t4OBgffPNNw7LXP3JbIsWLTRw4EANHz5cPj4+8vf316hRo3Ksy93dXf7+/qpUqZLCw8P10ksv6d///re++eYbffjhh5Kkfv36yWazacuWLerSpYtq1qyp0NBQDR06VJs2bcq1vqsfzs7OufZz+fLlcnd31z333OMwPSQkJNewb7PZNH369JvbqAXwxhtvqHfv3nr66adVp04dzZo1SyVKlNAHH3xw3eVcXFwc+l2+fPkcZdq3b69PP/20qJr+t7NixQo1bdpUZcuWVbly5fTggw9q//799vlZWVl67bXXFBwcLHd3dwUGBmrcuHGSrnxotG7dOr311lv28ZN92lF+xr2U/7FfkHEv5T72Gfd3PoLFX8Tly5dVokQJ7dy583Y3BcVUbi8K3t7e9vk2m00zZ85Uhw4dVLJkSY0bN06jRo1SgwYN9P7776tatWry8PCQJKWmpuqhhx5SqVKl5OXlpUcffVRHjx6115XXctf68MMPVbZsWS1dulR16tSRu7u7UlNT9eOPP+r+++9X+fLlVaZMGcXExGjr1q325bK/Vu/UqZNsNpvD1+z//ve/FR4eLg8PD1WvXl2jR492+HQOd46PPvpI5cuX15YtWzRgwAA999xzeuSRR9S4cWNt3bpVrVu31lNPPaX09PTr1lGyZElt3rxZr732ml599VWtWrXqhuu+9957FRYWpi+//FInT57UihUr1L9/f5UsWTJH2bJly950H9evX6+IiIgc07/44gtJ0urVq+2fKjs5OWnRokXq3bt3vuoeP368SpUqdd1HampqjuUyMjKUmJioVq1a2ac5OTmpVatWSkhIuO46f/31VwUEBKh69ep68sknc60/MjJSW7Zs0cWLF/PVj+Lu3LlzGjp0qH766SetXr1aTk5O6tSpk/1DlhEjRmjixImKj4/X7t279fHHH8vPz0+S9NZbbyk6Olq9e/e2f2BUpUqVPNd19biXdMvHfmGMe+nmxj7jPn8K9AN5uH3++c9/6uzZswQL3NFGjRqliRMnaurUqXJxcdEHH3ygffv26YsvvtCXX34pZ2dnZWVl2UPFunXrdPnyZfXv319du3bV2rVr7XVdu1xe0tPTNWnSJL3//vsqV66cKlSooP/+97+Ki4vTtGnTZIzRlClT9MADD+jXX39V6dKl9eOPP6pChQqaO3eu2rRpY69//fr16t69u95++201a9ZM+/fv17PPPitJGjlyZJFuOxRcWFiY/vnPf0r6f2+eypcvb3+D8corr2jmzJnavn17jk/8s9WvX9/+f3vXXXdp+vTpWr16te6///4brr927dravn279u3bJ2NMjvOm8/LVV1+pVKlS9udt27bVokWLci178OBBBQQE5Jh+9OhRubi4qEmTJnJ3d1diYqKysrLUrFkzzZ07V7NmzVJmZqb27NmjOnXqSJJ69+7tcGpK37599eijj163rbmt+/jx48rMzLS/Oc3m5+enX375Jc+6oqKi9OGHH6pWrVo6cuSIRo8erWbNmmnnzp0Op6gFBAQoIyNDaWlpCgoKum77IHXp0sXh+QcffCBfX1/t3r1bQUFBeuuttzR9+nTFxcVJkmrUqKGmTZtKunJGhJubm0qUKCF/f/98rS973Esq0NgvyLiXch/71xv37u7umjVrVpGNfcZ9/hAs/gISExM1a9YstWvXjmCB2+baFwVJeumll/TSSy/Znz/xxBN6+umnHcpkZGRo3rx58vX1lSStWrVKO3bsUEpKiv2TsXnz5ik0NFQ//vij7r777lyXy8ulS5f0zjvvKCwszD7t3nvvdSgze/ZslS1bVuvWrdODDz5or7Ns2bIOL6ajR4/Wiy++aH8Brl69usaMGaPhw4cTLO5A9evXt//t7OyscuXKqV69evZp2W8Arnf+89V1SFLFihVveL50NmOMbDabjDEFabZatmypmTNn2p/n9klvtvPnz+f6jd2OHTtUs2ZNubu7S5J+/vlnVahQQX5+furbt6/69u2r7du3q3fv3tq8eXOudfv4+MjHx6dAbbeibdu29r/r16+vqKgoBQUFaeHChQ7n0Xt6ekrSdb9pwv/z66+/6pVXXtHmzZt1/Phx+zcVqampSk9P18WLF3XfffcV2vqyx3323/lVkHEv5T72rzfuJd2RY7+4jXuCxR0uKytLffr00fPPP6+oqCh169ZNly5dkqur6+1uGoqZa18UJOU4MDdq1CjHckFBQQ7hIDk5WVWqVHH4ur1OnToqW7askpOT7cHi2uXy4ubmluPN4dGjR/XPf/5Ta9eu1bFjx5SZman09PRcv36+2s8//6wNGzbYzz+Wrly3ceHCBaWnp6tEiRI3bA9unWuPg9l3sLn6uaTrXneTWx35vU4nOTlZ1apV01133SWbzXbdTy2vVrJkSQUHB+erbPny5XXq1Kkc07dv3+4Qon7++WeH55K0a9cuhYaG5ln3+PHjNX78+Ouuf/fu3QoMDMzRJmdnZ4fTF6Ur+11+P/WWrgT7mjVr5rgpycmTJyUpX/s/rpybHxQUpPfee08BAQHKyspS3bp1lZGRYX+zWpiyx72kAo39gox7Kfexn59xLxXN2Gfc5w/XWNzhpk2bpuPHj+vVV19VvXr1dOnSpXy/eAGFKftF4erHtcEit0+gbvSp1PXWlx+enp72N5DZ4uLilJSUpLfeeksbN25UUlKSypUrd8OLwM+ePavRo0crKSnJ/tixY4d+/fXXPK/zQPH03XffaceOHerSpYt8fHwUGxurGTNm6Ny5cznKnj59+qbX07BhQ+3evTvH9O3btzsE6p9//jlHwN65c+d131z17dvXYazn9sjtVCg3NzdFREQ43EY3KytLq1evVnR0dL77dvbsWe3fv18VK1bM0e7KlSvneoErHJ04cUJ79uzRP//5T913330KCQlxeDN+1113ydPT0+H/6lpubm7KzMzM1/quHveSbvnYz8+4l4pm7DPu84dgcQc7dOiQ4uPjNWPGDJUsWVJ33XWX3N3dOR0Kf2khISH67bff9Ntvv9mn7d69W6dPn7afD2vVhg0bNHDgQD3wwAMKDQ2Vu7u7jh8/7lDG1dU1x4tpeHi49uzZkyNABQcHy8mJw2VxdfHiRaWlpenQoUPaunWrxo8fr4ceekgPPvigunfvLunKve0zMzMVGRmpL774Qr/++quSk5P19ttvF+hNx7ViY2O1a9cuhzeLWVlZ2rVrl8Mbqv379+e41/+uXbtUt27dPOv28fHJdaxf/XBxyf3EhqFDh+q9997TRx99pOTkZD333HP2O3JJ0vTp03OcfvN///d/WrdunQ4cOKCNGzeqU6dOcnZ21uOPP+5Qbv369WrdunW+tk9x5+3trXLlymn27Nnat2+fvvvuOw0dOtQ+38PDQ//4xz80fPhwzZs3T/v379emTZs0Z84ce5mqVatq8+bNOnDggMOpVPkZ99KtG/v5HfdS0Y39G417KefYL27jnlOh7mADBw5U27Zt1a5dO0lXblcWEhJCsMBtkf0iczUXF5cCf7rSqlUr1atXT08++aSmTp2qy5cvq1+/foqJicn1VKqbcdddd+lf//qXGjVqpDNnzmjYsGE5TgmoWrWqVq9ebb8I0NvbW6+88ooefPBBBQYG6uGHH5aTk5N+/vln7dy5U2PHji2UtuGvZ8WKFapYsaJcXFzk7e2tsLAwvf3224qLi7MHzurVq2vr1q0aN26cXnjhBR05ckS+vr6KiIjIcQphQdSrV0/h4eFauHCh+vTpI+nKm6n09HSHN1j16tXTyJEjFRERoSZNmki68ae2VnTt2lV//PGHXnnlFaWlpalBgwZasWKF/Vz348ePO9zyVJJ+//13Pf744zpx4oR8fX3VtGlTbdq0yeHUjwsXLmjJkiVasWJFkbT778bJyUmffvqpBg4cqLp166pWrVp6++231aJFC3uZ+Ph4ubi46JVXXtHhw4dVsWJF9e3b1z7///7v/xQXF6c6dero/PnzSklJkZS/cS/durGf33EvFd3Yv9G4l3KO/WI37k0+JCYmGkkmMTExP8VRCJYtW2bKli1rjhw54jD9qaeeMh06dLhNrSq+5s+fX6z3gbi4OCMpx6NWrVr2MpLM4sWLHZYbOXKkCQsLy1HfwYMHTYcOHUzJkiVN6dKlzSOPPGLS0tJuuNy15s6da8qUKZNj+tatW02jRo2Mh4eHueuuu8yiRYtMUFCQefPNN+1lli5daoKDg42Li4sJCgqyT1+xYoVp3Lix8fT0NF5eXiYyMtLMnj37hm35Oyvu4/92++qrr0xISIjJzMzM9zLp6enGx8enCFtVNN555x1z//333+5m5MA+cHsUl7F/p477bAXJAXxjcYd68MEHc71gb968ebehNSjuPvzwQ4cfRMqNyeXuIKNGjcr1B8cCAwP173//O8+68lruWj169FCPHj1yTG/YsKF+/PFHh2kPP/yww/P27durffv2OZaNjY1VbGzsDdcN3Crt2rXTr7/+qkOHDl33NwaulpycnO/b395JXF1dNW3atNvdDNwhisvY/zuNe4IFAAB3uKt/MTw/wsPDtWHDhqJpTBF65plnbncTcIcpDmP/7zTuuRoRAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYVqDfsVi+fLmSk5OLqi3AHSv7ntjsAyiOGP8o7tgHUJylpKTku6zN5PZzuddISEhQs2bNlJmZaalhwF+Zk5OTsrKybnczgNuC8Y/ijn0AxZmzs7PWr1+v6Ojo65bL1zcW7u7uyszM1Pz58xUSElIoDQT+SpYvX674+Hj2ARRLjH8Ud+wDKM6Sk5PVrVs3ubu737BsgU6FCgkJUXh4+E03DPiryv7qm30AxRHjH8Ud+wCQP1y8DQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQrNjBkzVLVqVXl4eCgqKkpbtmy5bvlFixapdu3a8vDwUL169bR8+XKH+cYYvfLKK6pYsaI8PT3VqlUr/frrrw5lxo0bp8aNG6tEiRIqW7ZsYXcJuCmZmZmKj49XtWrV5OnpqRo1amjMmDG60c8GLViwQGFhYSpRooQqVqyonj176sSJE/b5q1atUs2aNeXl5aWnnnpKGRkZ9nl//vmnatasqYMHDxZZv1B8FfbxXbpyp6UOHTqoTJkyKlmypO6++26lpqZKkk6ePKkBAwaoVq1a8vT0VGBgoAYOHKg///zToQ6bzZbj8emnnxZexwEVbPzv2rVLXbp0UdWqVWWz2TR16tQcZb7//nu1b99eAQEBstlsWrJkSY4yZ8+e1fPPP6/KlSvL09NTderU0axZsxzKzJ49Wy1atJCXl5dsNptOnz5tsafWESxQKD777DMNHTpUI0eO1NatWxUWFqbY2FgdO3Ys1/IbN27U448/rl69emnbtm3q2LGjOnbsqJ07d9rLvPbaa3r77bc1a9Ysbd68WSVLllRsbKwuXLhgL5ORkaFHHnlEzz33XJH3EcivSZMmaebMmZo+fbqSk5M1adIkvfbaa5o2bVqey2zYsEHdu3dXr169tGvXLi1atEhbtmxR7969JUlZWVl64okn1LdvXyUkJOinn37S7Nmz7cu/+OKL6tu3r4KCgoq8fyheiuL4vn//fjVt2lS1a9fW2rVrtX37dsXHx8vDw0OSdPjwYR0+fFivv/66du7cqQ8//FArVqxQr169cqxv7ty5OnLkiP3RsWPHItkOKJ4KOv7T09NVvXp1TZw4Uf7+/rmWOXfunMLCwjRjxow81zt06FCtWLFC8+fPV3JysgYPHqznn39eS5cudVhXmzZt9NJLL1nrZGEy+ZCYmGgkmcTExPwURzEUGRlp+vfvb3+emZlpAgICzIQJE3It/+ijj5p27do5TIuKijJ9+vQxxhiTlZVl/P39zeTJk+3zT58+bdzd3c0nn3ySo765c+eaMmXKFEJPcjd//nz2AeRbu3btTM+ePR2mde7c2Tz55JN5LjN58mRTvXp1h2lvv/22qVSpkjHGmKNHjxpJ5vz588YYY4YPH2769etnjDFmw4YNJiIiwly+fLkwu2HH+C/eCvv4bowxXbt2Nd26dStQOxYuXGjc3NzMpUuX7NMkmcWLFxeonpvBPlB8FXT8Xy0oKMi8+eab1y2T1xgODQ01r776qsO08PBw8/LLL+cou2bNGiPJnDp16oZtuhkFyQF8YwHLMjIylJiYqFatWtmnOTk5qVWrVkpISMh1mYSEBIfykhQbG2svn5KSorS0NIcyZcqUUVRUVJ51AneKxo0ba/Xq1dq7d68k6eeff9YPP/ygtm3b5rlMdHS0fvvtNy1fvlzGGB09elSff/65HnjgAUmSr6+vKlasqJUrVyo9PV3r169X/fr1denSJT333HN699135ezsfEv6h+KjKI7vWVlZ+vrrr1WzZk3FxsaqQoUKioqKyvV0kKv9+eef8vLykouL42/79u/fX+XLl1dkZKQ++OCDG55yCOTXzYz/wtK4cWMtXbpUhw4dkjFGa9as0d69e9W6desiXa9VBAtYdvz4cWVmZsrPz89hup+fn9LS0nJdJi0t7brls/8tSJ3AneLFF1/UY489ptq1a8vV1VUNGzbU4MGD9eSTT+a5TJMmTbRgwQJ17dpVbm5u8vf3V5kyZexfldtsNi1cuFBjxoxRaGioGjZsqJ49e2rixIlq2bKlPDw81KRJE9WqVUvTp0+/VV3F31xRHN+PHTums2fPauLEiWrTpo1WrlypTp06qXPnzlq3bl2e7RgzZoyeffZZh+mvvvqqFi5cqFWrVqlLly7q16/fdU85BAriZsZ/YZk2bZrq1KmjypUry83NTW3atNGMGTPUvHnzIl2vVS43LgIAKIiFCxdqwYIF+vjjjxUaGqqkpCQNHjxYAQEBiouLy3WZ3bt3a9CgQXrllVcUGxurI0eOaNiwYerbt6/mzJkjSWratKl+/PFH+zJ79+7VvHnztG3bNjVv3lyDBg1S27ZtVbduXTVv3lz169e/Jf0FCiIrK0uS9NBDD2nIkCGSpAYNGmjjxo2aNWuWYmJiHMqfOXNG7dq1U506dTRq1CiHefHx8fa/GzZsqHPnzmny5MkaOHBg0XYCKGLTpk3Tpk2btHTpUgUFBen7779X//79FRAQkOMbwTsJwQKWlS9fXs7Ozjp69KjD9KNHj+Z54ZK/v/91y2f/e/ToUVWsWNGhTIMGDQqx9UDhGzZsmP1bC0mqV6+eDh48qAkTJuQZLCZMmKAmTZpo2LBhkqT69eurZMmSatasmcaOHeuwH2Tr06ePpkyZoqysLG3btk2PPPKISpQooZiYGK1bt45gAcuK4vhevnx5ubi4qE6dOg5lQkJC9MMPPzhM+9///qc2bdqodOnSWrx4sVxdXa/b3qioKI0ZM0YXL16Uu7t7vvoI5OVmxn9hOH/+vF566SUtXrxY7dq1k3TlNSEpKUmvv/76HR0sOBUKlrm5uSkiIkKrV6+2T8vKytLq1asVHR2d6zLR0dEO5aUrt9LMLl+tWjX5+/s7lDlz5ow2b96cZ53AnSI9PV1OTo6HV2dnZ/sntQVZRlKu54zPmTNHPj4+6tChgzIzMyVJly5dsv+bPQ2woiiO725ubrr77ru1Z88ehzJ79+51uKvZmTNn1Lp1a7m5uWnp0qX2O0ZdT1JSkry9vQkVKBQ3M/4Lw6VLl3Tp0qUCv47cCfjGAoVi6NChiouLU6NGjRQZGampU6fq3LlzevrppyVJ3bt3V6VKlTRhwgRJ0qBBgxQTE6MpU6aoXbt2+vTTTx1un2mz2TR48GCNHTtWd911l6pVq6b4+HgFBAQ43EowNTVVJ0+eVGpqqjIzM5WUlCRJCg4OVqlSpW7pNgCytW/fXuPGjVNgYKBCQ0O1bds2vfHGG+rZs6e9zIgRI3To0CHNmzfPvkzv3r01c+ZM+6lQgwcPVmRkpAICAhzqP3bsmMaOHasNGzZIkry9vRUSEqKpU6eqdevWWr16tV5++eVb12H8rRX28V268q1e165d1bx5c7Vs2VIrVqzQsmXLtHbtWkn/L1Skp6dr/vz5OnPmjM6cOSPpyo0MnJ2dtWzZMh09elT33HOPPDw8tGrVKo0fP17/93//d2s3EP7WCjr+MzIytHv3bvvfhw4dUlJSkkqVKqXg4GBJV36jYt++ffZ1pKSkKCkpST4+PgoMDJSXl5diYmI0bNgweXp6KigoSOvWrdO8efP0xhtv2JdLS0tTWlqava4dO3aodOnSCgwMlI+Pzy3ZPjkU9m2mUHxNmzbNBAYGGjc3NxMZGWk2bdpknxcTE2Pi4uIcyi9cuNDUrFnTuLm5mdDQUPP11187zM/KyjLx8fHGz8/PuLu7m/vuu8/s2bPHoUxcXJyRlOOxZs2aQu0btxpEQZw5c8YMGjTIBAYGGg8PD1O9enXz8ssvm4sXL9rLxMXFmZiYGIfl3n77bVOnTh3j6elpKlasaJ588knz+++/56j/scceM9OmTXOYtnnzZlO7dm3j4+NjRo8eXaj9YfyjsI/vxhgzZ84cExwcbDw8PExYWJhZsmSJfV727TNze6SkpBhjjPnmm29MgwYNTKlSpUzJkiVNWFiYmTVrlsnMzCz0/rMPFG8FGf8pKSm5jturj/d5je+r6zly5Ijp0aOHCQgIMB4eHqZWrVpmypQpJisry15m5MiRudYzd+7cQu1/QXKAzZgb35dt69atioiIUGJiosLDwwsl0AB/JQsWLFC3bt3YB1AsMf5R3LEPoDgrSA7gGgsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZS4FKbx8+XIlJycXVVuAO9aGDRsksQ+geGL8o7hjH0BxlpKSku+y+frl7YSEBDVr1kyZmZmWGgb8lTk5OSkrK+t2NwO4LRj/KO7YB1CcOTs7a/369YqOjr5uuXx9Y+Hu7q7MzEzNnz9fISEhhdJA4K9k+fLlio+PZx9AscT4R3HHPoDiLDk5Wd26dZO7u/sNyxboVKiQkBCFh4ffdMOAv6rsr77ZB1AcMf5R3LEPAPnDxdsAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1ig0MyYMUNVq1aVh4eHoqKitGXLluuWX7RokWrXri0PDw/Vq1dPy5cvd5j/5ZdfqnXr1ipXrpxsNpuSkpJy1HHhwgX1799f5cqVU6lSpdSlSxcdPXq0MLsFFFhmZqbi4+NVrVo1eXp6qkaNGhozZoxu9LNBCxYsUFhYmEqUKKGKFSuqZ8+eOnHihH3+qlWrVLNmTXl5eempp55SRkaGfd6ff/6pmjVr6uDBg0XWLxRfhX1879Gjh2w2m8OjTZs2DmWqVq2ao8zEiRMdyhhj9Prrr6tmzZpyd3dXpUqVNG7cuMLpNPD/K8j437Vrl7p06WIfv1OnTs1R5vvvv1f79u0VEBAgm82mJUuW5Chz9uxZPf/886pcubI8PT1Vp04dzZo1y6HM7Nmz1aJFC3l5eclms+n06dMWe2odwQKF4rPPPtPQoUM1cuRIbd26VWFhYYqNjdWxY8dyLb9x40Y9/vjj6tWrl7Zt26aOHTuqY8eO2rlzp73MuXPn1LRpU02aNCnP9Q4ZMkTLli3TokWLtG7dOh0+fFidO3cu9P4BBTFp0iTNnDlT06dPV3JysiZNmqTXXntN06ZNy3OZDRs2qHv37urVq5d27dqlRYsWacuWLerdu7ckKSsrS0888YT69u2rhIQE/fTTT5o9e7Z9+RdffFF9+/ZVUFBQkfcPxUtRHN8lqU2bNjpy5Ij98cknn+So69VXX3UoM2DAAIf5gwYN0vvvv6/XX39dv/zyi5YuXarIyMjC6zyKvYKO//T0dFWvXl0TJ06Uv79/rmXOnTunsLAwzZgxI8/1Dh06VCtWrND8+fOVnJyswYMH6/nnn9fSpUsd1tWmTRu99NJL1jpZmEw+JCYmGkkmMTExP8VRDEVGRpr+/fvbn2dmZpqAgAAzYcKEXMs/+uijpl27dg7ToqKiTJ8+fXKUTUlJMZLMtm3bHKafPn3auLq6mkWLFtmnJScnG0kmISHBQm9ymj9/PvsA8q1du3amZ8+eDtM6d+5snnzyyTyXmTx5sqlevbrDtLfffttUqlTJGGPM0aNHjSRz/vx5Y4wxw4cPN/369TPGGLNhwwYTERFhLl++XJjdsGP8F29FcXyPi4szDz300HXXGxQUZN5888085+/evdu4uLiYX3755cadsIh9oPgq6Pi/2o3GsDHGSDKLFy/OMT00NNS8+uqrDtPCw8PNyy+/nKPsmjVrjCRz6tSpG7bpZhQkB/CNBSzLyMhQYmKiWrVqZZ/m5OSkVq1aKSEhIddlEhISHMpLUmxsbJ7lc5OYmKhLly451FO7dm0FBgYWqB6gsDVu3FirV6/W3r17JUk///yzfvjhB7Vt2zbPZaKjo/Xbb79p+fLlMsbo6NGj+vzzz/XAAw9Iknx9fVWxYkWtXLlS6enpWr9+verXr69Lly7pueee07vvvitnZ+db0j8UH0V5fF+7dq0qVKigWrVq6bnnnnM47S/bxIkTVa5cOTVs2FCTJ0/W5cuX7fOWLVum6tWr66uvvlK1atVUtWpVPfPMMzp58qSVLgN2NzP+C0vjxo21dOlSHTp0SMYYrVmzRnv37lXr1q2LdL1WESxg2fHjx5WZmSk/Pz+H6X5+fkpLS8t1mbS0tAKVz6sONzc3lS1b1lI9QGF78cUX9dhjj6l27dpydXVVw4YNNXjwYD355JN5LtOkSRMtWLBAXbt2lZubm/z9/VWmTBn7V+U2m00LFy7UmDFjFBoaqoYNG6pnz56aOHGiWrZsKQ8PDzVp0kS1atXS9OnTb1VX8TdXVMf3Nm3aaN68eVq9erUmTZqkdevWqW3btsrMzLSXGThwoD799FOtWbNGffr00fjx4zV8+HD7/P/+9786ePCgFi1apHnz5unDDz9UYmKiHn744cLoOnBT47+wTJs2TXXq1FHlypXl5uamNm3aaMaMGWrevHmRrtcql9vdAAD4u1m4cKEWLFigjz/+WKGhoUpKStLgwYMVEBCguLi4XJfZvXu3Bg0apFdeeUWxsbE6cuSIhg0bpr59+2rOnDmSpKZNm+rHH3+0L7N3717NmzdP27ZtU/PmzTVo0CC1bdtWdevWVfPmzVW/fv1b0l+goB577DH73/Xq1VP9+vVVo0YNrV27Vvfdd5+kK+eYZ6tfv77c3NzUp08fTZgwQe7u7srKytLFixc1b9481axZU5I0Z84cRUREaM+ePapVq9at7RRQiKZNm6ZNmzZp6dKlCgoK0vfff6/+/fsrICAgxzeCdxKCBSwrX768nJ2dc9yN6ejRo3leuOTv71+g8nnVkZGRodOnTzt8a1HQeoDCNmzYMPu3FtKVN04HDx7UhAkT8gwWEyZMUJMmTTRs2DBJV95IlSxZUs2aNdPYsWNVsWLFHMv06dNHU6ZMUVZWlrZt26ZHHnlEJUqUUExMjNatW0ewgGW36vhevXp1lS9fXvv27bMHi2tFRUXp8uXLOnDggGrVqqWKFSvKxcXFHiokKSQkRJKUmppKsIBlNzP+C8P58+f10ksvafHixWrXrp2kK68JSUlJev311+/oYMGpULDMzc1NERERWr16tX1aVlaWVq9erejo6FyXiY6OdigvXbmVZl7lcxMRESFXV1eHevbs2aPU1NQC1QMUtvT0dDk5OR5enZ2dlZWVVeBlJOV6m9o5c+bIx8dHHTp0sJ8+cunSJfu/V59SAtysW3V8//3333XixIlcA3S2pKQkOTk5qUKFCpKunD54+fJl7d+/314m+7om7o6GwnAz478wXLp0SZcuXSrw68idgG8sUCiGDh2quLg4NWrUSJGRkZo6darOnTunp59+WpLUvXt3VapUSRMmTJB05RaBMTExmjJlitq1a6dPP/00x+0zT548qdTUVB0+fFjSldAgXfk0LPv88169emno0KHy8fGRl5eXBgwYoOjoaN1zzz23eAsA/0/79u01btw4BQYGKjQ0VNu2bdMbb7yhnj172suMGDFChw4d0rx58+zL9O7dWzNnzrSfCjV48GBFRkYqICDAof5jx45p7Nix2rBhgyTJ29tbISEhmjp1qlq3bq3Vq1fr5ZdfvnUdxt9aYR/fz549q9GjR6tLly7y9/fX/v37NXz4cAUHBys2NlbSlQvAN2/erJYtW6p06dJKSEjQkCFD1K1bN3l7e0uSWrVqpfDwcPXs2VNTp05VVlaW+vfvr/vvv9/hWwzAioKO/4yMDO3evdv+96FDh5SUlKRSpUopODhY0pV9YN++ffZ1pKSkKCkpST4+PgoMDJSXl5diYmI0bNgweXp6KigoSOvWrdO8efP0xhtv2JdLS0tTWlqava4dO3aodOnSCgwMlI+Pzy3ZPjkU9m2mUHxNmzbNBAYGGjc3NxMZGWk2bdpknxcTE2Pi4uIcyi9cuNDUrFnTuLm5mdDQUPP11187zJ87d66RlOMxcuRIe5nz58+bfv36GW9vb1OiRAnTqVMnc+TIkULvG7caREGcOXPGDBo0yAQGBhoPDw9TvXp18/LLL5uLFy/ay8TFxZmYmBiH5d5++21Tp04d4+npaSpWrGiefPJJ8/vvv+eo/7HHHjPTpk1zmLZ582ZTu3Zt4+PjY0aPHl2o/WH8ozCP7+np6aZ169bG19fXuLq6mqCgINO7d2+TlpZmL5OYmGiioqJMmTJljIeHhwkJCTHjx483Fy5ccFjPoUOHTOfOnU2pUqWMn5+f6dGjhzlx4kSh9599oHgryPjPvkX+tY+rj/fZt4e99nF1PUeOHDE9evQwAQEBxsPDw9SqVctMmTLFZGVl2cuMHDky13rmzp1bqP0vSA6wGXODn4KVtHXrVkVERCgxMVHh4eGFGGuAv4YFCxaoW7du7AMolhj/KO7YB1CcFSQHcI0FAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKXghRevny5kpOTi6otwB1rw4YNktgHUDwx/lHcsQ+gOEtJScl32Xz98nZCQoKaNWumzMxMSw0D/sqcnJyUlZV1u5sB3BaMfxR37AMozpydnbV+/XpFR0dft1y+vrFwd3dXZmam5s+fr5CQkEJpIPBXsnz5csXHx7MPoFhi/KO4Yx9AcZacnKxu3brJ3d39hmULdCpUSEiIwsPDb7phwF9V9lff7AMojhj/KO7YB4D84eJtAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsUGhmzJihqlWrysPDQ1FRUdqyZUueZXft2qUuXbqoatWqstlsmjp16k3VeeHCBfXv31/lypVTqVKl1KVLFx09erQwuwU4+P7779W+fXsFBATIZrNpyZIlDvO//PJLtW7dWuXKlZPNZlNSUlK+6p06dapq1aolT09PValSRUOGDNGFCxfs8xcsWKAqVarI29tbQ4cOdVj2wIEDqlmzps6cOWO1e0CuCnJ8l6RFixapdu3a8vDwUL169bR8+XKH+T169JDNZnN4tGnTxqFM9uvD1Y+JEyc6lDHG6PXXX1fNmjXl7u6uSpUqady4cYXTaeD/V9jvb270OiLl77Vk9uzZatGihby8vGSz2XT69Omb72QhIVigUHz22WcaOnSoRo4cqa1btyosLEyxsbE6duxYruXT09NVvXp1TZw4Uf7+/jdd55AhQ7Rs2TItWrRI69at0+HDh9W5c+ci6SMgSefOnVNYWJhmzJiR5/ymTZtq0qRJ+a7z448/1osvvqiRI0cqOTlZc+bM0WeffaaXXnpJknT8+HE988wzev3117Vy5UrNnz9fX331lX35fv36aeLEifLy8rLWOSAXBT2+b9y4UY8//rh69eqlbdu2qWPHjurYsaN27tzpUK5NmzY6cuSI/fHJJ5/kqOvVV191KDNgwACH+YMGDdL777+v119/Xb/88ouWLl2qyMjIwus8ir2ieH9zo9eR7DI3ei1JT09XmzZt7K8VdwSTD4mJiUaSSUxMzE9xFEORkZGmf//+9ueZmZkmICDATJgw4YbLBgUFmTfffLPAdZ4+fdq4urqaRYsW2cskJycbSSYhIcFCb3KaP38++wBykGQWL16c67yUlBQjyWzbtu2G9fTv39/ce++9DtOGDh1qmjRpYowxZvPmzcbPz88+79FHHzWvvfaaMcaYjz/+2HTo0OHmOpBPjP/iraDH90cffdS0a9fOYVpUVJTp06eP/XlcXJx56KGHrrvevF4bsu3evdu4uLiYX3755cadsIh9oPgqivc3V7ve64gx+XstWbNmjZFkTp06dcM23YyC5AC+sYBlGRkZSkxMVKtWrezTnJyc1KpVKyUkJBRZnYmJibp06ZJDmdq1ayswMPCm1wvcDo0bN1ZiYqL96/X//ve/Wr58uR544AFJ0l133aX09HRt27ZNJ0+e1I8//qj69evr1KlTio+P1/Tp029n8/E3djPH94SEBIfykhQbG5uj/Nq1a1WhQgXVqlVLzz33nE6cOJGjrokTJ6pcuXJq2LChJk+erMuXL9vnLVu2TNWrV9dXX32latWqqWrVqnrmmWd08uRJK10G7Iri/c3fXYF+eRvIzfHjx5WZmSk/Pz+H6X5+fvrll1+KrM60tDS5ubmpbNmyOcqkpaXd1HqB2+GJJ57Q8ePH1bRpUxljdPnyZfXt29f+9ba3t7c++ugjde/eXefPn1f37t0VGxurXr166fnnn1dKSoo6dOigS5cuadSoUXr44Ydvc4/wd3Ezx/e0tLRcy199XG7Tpo06d+6satWqaf/+/XrppZfUtm1bJSQkyNnZWZI0cOBAhYeHy8fHRxs3btSIESN05MgRvfHGG5KuBPCDBw9q0aJFmjdvnjIzMzVkyBA9/PDD+u677wpzM6CYKor3N393BAsAuM3Wrl2r8ePH65133lFUVJT27dunQYMGacyYMYqPj5ckderUSZ06dbIvs27dOm3fvl3Tpk1TcHCwPvnkE/n7+ysyMlLNmzdXhQoVbld3gBt67LHH7H/Xq1dP9evXV40aNbR27Vrdd999kuRwk4L69evLzc1Nffr00YQJE+Tu7q6srCxdvHhR8+bNU82aNSVJc+bMUUREhPbs2aNatWrd2k4B4OJtWFe+fHk5OzvnuBvT0aNH87xwqTDq9Pf3V0ZGRo67IFhZL3A7xMfH66mnntIzzzyjevXqqVOnTho/frwmTJigrKysHOUvXryofv366d1339W+fft0+fJlxcTEqFatWqpZs6Y2b958G3qBv6ObOb77+/sX+PWgevXqKl++vPbt25dnmaioKF2+fFkHDhyQJFWsWFEuLi72UCFJISEhkqTU1NTr9gvIj6J4f/N3R7CAZW5uboqIiNDq1avt07KysrR69WpFR0cXWZ0RERFydXV1KLNnzx6lpqbe9HqB2yE9PV1OTo6H4+zTQYwxOcqPHTtWbdq0UXh4uDIzMx3OO7906ZIyMzOLtsEoNm7m+B4dHe1QXpJWrVp13ePy77//rhMnTqhixYp5lklKSpKTk5P927gmTZro8uXL2r9/v73M3r17JUlBQUE37hxwA0Xx/ubvjlOhUCiGDh2quLg4NWrUSJGRkZo6darOnTunp59+WpLUvXt3VapUSRMmTJB05YKo3bt32/8+dOiQkpKSVKpUKQUHB+erzjJlyqhXr14aOnSofHx85OXlpQEDBig6Olr33HPPbdgKKA7Onj3r8KlqSkqKkpKS5OPjo8DAQJ08eVKpqak6fPiwpCthV7ryKW72J1zX7g/t27fXG2+8oYYNG9pPhYqPj1f79u3tASPb7t279dlnn2nbtm2SrtywwMnJSXPmzJG/v79++eUX3X333UW+HVB8FPT4PmjQIMXExGjKlClq166dPv30U/3000+aPXu2pCv70OjRo9WlSxf5+/tr//79Gj58uIKDgxUbGyvpygXgmzdvVsuWLVW6dGklJCRoyJAh6tatm7y9vSVJrVq1Unh4uHr27KmpU6cqKytL/fv31/333+/wLQZgRVG8v7nR64ikfL2WpKWlKS0tzV7Xjh07VLp0aQUGBsrHx6eoN03uCvs2Uyi+pk2bZgIDA42bm5uJjIw0mzZtss+LiYkxcXFx9ufZt0+79hETE5PvOo0x5vz586Zfv37G29vblChRwnTq1MkcOXKk0PvGrQaRLfu2ftc+ssf33Llzc50/cuRIex3X7g+XLl0yo0aNMjVq1DAeHh6mSpUqpl+/fjluHZiVlWWaNGlili1b5jB92bJlJjAw0Pj5+Zn33nuv0PvM+EdBju/GGLNw4UJTs2ZN4+bmZkJDQ83XX39tn5eenm5at25tfH19jaurqwkKCjK9e/c2aWlp9jKJiYkmKirKlClTxnh4eJiQkBAzfvx4c+HCBYf1HDp0yHTu3NmUKlXK+Pn5mR49epgTJ04Uev/ZB4q3wn5/c6PXEWPy91oycuTIXMvMnTu3UPtfkBxgMyaX79mvsXXrVkVERCgxMVHh4eFWcgzwl7RgwQJ169aNfQDFEuMfxR37AIqzguQArrEAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWOZSkMLLly9XcnJyUbUFuGNt2LBBEvsAiifGP4o79gEUZykpKfkum69f3k5ISFCzZs2UmZlpqWHAX5mTk5OysrJudzOA24Lxj+KOfQDFmbOzs9avX6/o6OjrlsvXNxbu7u7KzMzU/PnzFRISUigNBP5Kli9frvj4ePYBFEuMfxR37AMozpKTk9WtWze5u7vfsGyBToUKCQlReHj4TTcM+KvK/uqbfQDFEeMfxR37AJA/XLwNAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFrDs+++/V/v27RUQECCbzaYlS5bccJm1a9cqPDxc7u7uCg4O1ocffpijzIwZM1S1alV5eHgoKipKW7ZscZh/4cIF9e/fX+XKlVOpUqXUpUsXHT16tJB6BeSuIOO9b9++stlsmjp16nXrrFq1qmw2W45H//797WWGDh0qHx8fValSRQsWLHBYftGiRWrfvr2VbgF5utGx+Gq7du1Sly5d7GM6t7Gfn33oyy+/VOvWrVWuXDnZbDYlJSXlKDN79my1aNFCXl5estlsOn369M13EriOwt4HJOnQoUPq1q2bypUrJ09PT9WrV08//fSTQ5nk5GR16NBBZcqUUcmSJXX33XcrNTVVknTgwIFcXzdsNpsWLVpUaH0vKIIFLDt37pzCwsI0Y8aMfJVPSUlRu3bt1LJlSyUlJWnw4MF65pln9O2339rLfPbZZxo6dKhGjhyprVu3KiwsTLGxsTp27Ji9zJAhQ7Rs2TItWrRI69at0+HDh9W5c+dC7x9wtfyO98WLF2vTpk0KCAi4YZ0//vijjhw5Yn+sWrVKkvTII49IkpYtW6aPP/5YK1eu1GuvvaZnnnlGx48flyT9+eefevnll/O9/wEFkZ9j8dXS09NVvXp1TZw4Uf7+/rmWyc8+dO7cOTVt2lSTJk3Ks0x6erratGmjl156qWCdAgqgKPaBU6dOqUmTJnJ1ddU333yj3bt3a8qUKfL29raX2b9/v5o2baratWtr7dq12r59u+Lj4+Xh4SFJqlKlisPrxpEjRzR69GiVKlVKbdu2LfwNkV8mHxITE40kk5iYmJ/iKMYkmcWLF1+3zPDhw01oaKjDtK5du5rY2Fj788jISNO/f3/788zMTBMQEGAmTJhgjDHm9OnTxtXV1SxatMheJjk52UgyCQkJhdATR/Pnz2cfQA55jffff//dVKpUyezcudMEBQWZN998s0D1Dho0yNSoUcNkZWUZY4yZNGmS6dq1q31+hQoVzJYtW4wxxjz77LPmjTfeuOk+5Afjv/i60bH4evIz9m/0mpGSkmIkmW3btuVZZs2aNUaSOXXq1A3bdLPYB4qvotgH/vGPf5imTZted9muXbuabt26FaitDRo0MD179izQMvlRkBzANxa45RISEtSqVSuHabGxsUpISJAkZWRkKDEx0aGMk5OTWrVqZS+TmJioS5cuOZSpXbu2AgMD7WWA2yErK0tPPfWUhg0bptDQ0AIvn5GRofnz56tnz56y2WySpLCwMP300086deqUEhMTdf78eQUHB+uHH37Q1q1bNXDgwMLuBpCvYzHwd1ZU+8DSpUvVqFEjPfLII6pQoYIaNmyo9957zz4/KytLX3/9tWrWrKnY2FhVqFBBUVFR1z31NjExUUlJSerVq9dNt6swECxwy6WlpcnPz89hmp+fn86cOaPz58/r+PHjyszMzLVMWlqavQ43NzeVLVs2zzLA7TBp0iS5uLjc9Jv9JUuW6PTp0+rRo4d9WmxsrLp166a7775bPXr00EcffaSSJUvqueee06xZszRz5kzVqlVLTZo00a5duwqpJyju8nMsBv7Oimof+O9//6uZM2fqrrvu0rfffqvnnntOAwcO1EcffSRJOnbsmM6ePauJEyeqTZs2WrlypTp16qTOnTtr3bp1udY5Z84chYSEqHHjxjfdrsLgclvXDgB/I4mJiXrrrbe0detW+7cNBTVnzhy1bds2x7UZo0aN0qhRo+zPR48erVatWsnV1VVjx47Vjh079NVXX6l79+5KTEy00g0AQBHKyspSo0aNNH78eElSw4YNtXPnTs2aNUtxcXHKysqSJD300EMaMmSIJKlBgwbauHGjZs2apZiYGIf6zp8/r48//ljx8fG3tiO54BsL3HL+/v457t509OhReXl5ydPTU+XLl5ezs3OuZbIvhPL391dGRkaOu4BcXQa41davX69jx44pMDBQLi4ucnFx0cGDB/XCCy+oatWqN1z+4MGD+s9//qNnnnnmuuV++eUXzZ8/X2PGjNHatWvVvHlz+fr66tFHH9XWrVv1v//9r5B6hOIsP8di4O+sqPaBihUrqk6dOg7TQkJC7Hd8Kl++vFxcXK5b5mqff/650tPT1b1795tuU2EhWOCWi46O1urVqx2mrVq1StHR0ZIkNzc3RUREOJTJysrS6tWr7WUiIiLk6urqUGbPnj1KTU21lwFutaeeekrbt29XUlKS/REQEKBhw4Y53PUsL3PnzlWFChXUrl27PMsYY9SnTx+98cYbKlWqlDIzM3Xp0iVJsv+bmZlZOB1CsZafYzHwd1ZU+0CTJk20Z88eh2l79+5VUFCQfb133333dctcbc6cOerQoYN8fX1vuk2FhVOhYNnZs2e1b98++/OUlBQlJSXJx8dHgYGBGjFihA4dOqR58+ZJunJv/+nTp2v48OHq2bOnvvvuOy1cuFBff/21vY6hQ4cqLi5OjRo1UmRkpKZOnapz587p6aefliSVKVNGvXr1st/b38vLSwMGDFB0dLTuueeeW7sBUKzcaLyXK1fOobyrq6v8/f1Vq1Yt+7T77rtPnTp10vPPP2+flpWVpblz5youLk4uLnkfmt9//335+vraf7eiSZMmGjVqlDZt2qRvvvlGderUyXHtEXCzbnQs7t69uypVqqQJEyZIunKx6+7du+1/Hzp0SElJSSpVqpSCg4Ml3XgfkqSTJ08qNTVVhw8fliT7Gyx/f3/7J8VpaWlKS0uz17Vjxw6VLl1agYGB8vHxKepNg2KiKPaBIUOGqHHjxho/frweffRRbdmyRbNnz9bs2bPt6x02bJi6du2q5s2bq2XLllqxYoWWLVumtWvXOrRv3759+v7777V8+fJbsDXyobBvM4XiJ/tWf9c+4uLijDHGxMXFmZiYmBzLNGjQwLi5uZnq1aubuXPn5qh32rRpJjAw0Li5uZnIyEizadMmh/nnz583/fr1M97e3qZEiRKmU6dO5siRI0XSR241iGw3Gu/Xyu12g0FBQWbkyJEO07799lsjyezZsyfPdaelpZmgoCBz6NAhh+mjR482Pj4+pnbt2mbz5s03063rYvwXb9c7FsfExDiM/ezbw177uPo1ID/70Ny5c3Mtc/V+M3LkyFzL5PZ6YhX7QPFW2PuAMcYsW7bM1K1b17i7u5vatWub2bNn51jvnDlzTHBwsPHw8DBhYWFmyZIlOcqMGDHCVKlSxWRmZhZaf69VkBxgM8aYG4WPrVu3KiIiQomJiQoPD7cQY4C/pgULFqhbt27sAyiWGP8o7tgHUJwVJAdwjQUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMpeCFE5OTi6qdgB3tJSUFEnsAyieGP8o7tgHUJwVZNzn65e3U1NTFRISovT0dEsNA/7KnJ2dlZmZebubAdwWjH8Ud+wDKM5KlCih5ORkBQYGXrdcvoKFdCVcHD9+vFAaB/wVXbx4Ue7u7re7GcBtwfhHccc+gOKsfPnyNwwVUgGCBQAAAADkhYu3AQAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABg2f8HT3yg7GgVOmIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TRY THE FINAL FUNCTION\n",
    "lambdas_param = [10**(-3), 10**(-2), 10**(-1), 1.0]\n",
    "fitLogReg(DTR, LTR, lambdas_param)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
